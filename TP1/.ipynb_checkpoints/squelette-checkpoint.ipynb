{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## École Polytechnique de Montréal\n",
    "## Département Génie Informatique et Génie Logiciel\n",
    "\n",
    "## INF8460 – Traitement automatique de la langue naturelle - TP1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectifs d'apprentissage: \n",
    "\n",
    "•\tSavoir accéder à un corpus, le nettoyer et effectuer divers pré-traitements sur les données\n",
    "•\tSavoir effectuer une classification automatique des textes pour l’analyse de sentiments\n",
    "•\tEvaluer l’impact des pré-traitements sur les résultats obtenus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Équipe et contributions \n",
    "Veuillez indiquer la contribution effective de chaque membre de l'équipe en pourcentage et en indiquant les modules ou questions sur lesquelles chaque membre a travaillé\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cedric Sadeu: x% (détail)\n",
    "\n",
    "Mamoudou Sacko: x% (détail)\n",
    "\n",
    "Oumayma Messoussi: x% (détail)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Librairies externes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\oumay\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\oumay\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\oumay\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from typing import List, Literal, Tuple\n",
    "from IPython.display import display\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "from nltk import tokenize\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Valeurs globales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_path = \"data\"\n",
    "output_path = \"output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path: str) -> Tuple[List[str], List[bool], List[Literal[\"M\", \"W\"]]]:\n",
    "    data = pd.read_csv(path)\n",
    "    inputs = data[\"response_text\"].tolist()\n",
    "    labels = (data[\"sentiment\"] == \"Positive\").tolist()\n",
    "    gender = data[\"op_gender\"].tolist()\n",
    "    return inputs, labels, gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data line 0: \n",
      "\t- response_text: i don't think any one there has ebola bob latta you should be back in washington actually getting something done there on the house floor.\n",
      "\t- sentiment: False\n",
      "\t- gender: M\n"
     ]
    }
   ],
   "source": [
    "train_data = read_data(os.path.join(data_path, \"train.csv\"))\n",
    "test_data = read_data(os.path.join(data_path, \"test.csv\"))\n",
    "\n",
    "train_data = ([text.lower() for text in train_data[0]], train_data[1], train_data[2])\n",
    "test_data = ([text.lower() for text in test_data[0]], test_data[1], test_data[2])\n",
    "\n",
    "print(\"train data line 0: \\n\\t- response_text: \" + str(train_data[0][0]) \n",
    "      + \"\\n\\t- sentiment: \" + str(train_data[1][0]) + \"\\n\\t- gender: \" + str(train_data[2][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pré-traitement et Exploration des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Lecture et prétraitement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Dans cette section, vous devez compléter la fonction preprocess_corpus qui doit être appelée sur les fichiers train.csv et test.csv. La fonction preprocess_corpus appellera les différentes fonctions créées ci-dessous. Les différents fichiers de sortie doivent se retrouver dans le répertoire output.  Chacune des sous-questions suivantes devraient être une ou plusieurs fonctions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_csv(sentences, corpus_name):\n",
    "    dt = pd.DataFrame(sentences, columns =['sentences'])\n",
    "    dt.to_csv(corpus_name + '.csv')\n",
    "\n",
    "def write_corpus_to_csv(corpus, corpus_name):\n",
    "    sentences = []\n",
    "    for doc in corpus:\n",
    "        sentences.extend(doc[1])\n",
    "    write_to_csv(sentences, corpus_name)\n",
    "\n",
    "def process_list_corpus_tup(func, corpus_tup_list):\n",
    "    process_corpus = []\n",
    "    for doc in corpus_tup_list:\n",
    "        result = (doc[0], func(doc[1]), doc[2])\n",
    "        process_corpus.append(result)\n",
    "    return process_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 1) Segmentez chaque corpus en phrases, et stockez-les dans un fichier `nomcorpus`_phrases.csv (une phrase par ligne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def make_sentence(line):\n",
    "    sentences = tokenize.sent_tokenize(line)\n",
    "    return [sentence for sentence in sentences if re.findall(r\"[\\w]+\", sentence)]\n",
    "\n",
    "\n",
    "def corpus_to_sentences(data):  \n",
    "    #check data is not empty and lists inside data have the same length\n",
    "    if (not data) or [len(element) for element in data if len(element) != len(data[0])]:\n",
    "        raise ValueError(\"Data is not valid.\")\n",
    "    vocabulary = []\n",
    "    for i, item in enumerate(data[0]):\n",
    "        document = (data[2][i], make_sentence(item), data[1][i])\n",
    "        vocabulary.append(document)\n",
    "    \n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([\"i don't think any one there has ebola bob latta you should be back in washington actually getting something done there on the house floor.\", \";-)...anything other than jeans and t-shirts are superfluous, by the way. 'update your wardrobe'...pfft.\", 'meh, i could only get to 8. need to work up.', 'a bill consisting of a single sentence. very well done, sir.', 'so far, so good. thx !'], [False, False, False, True, True], ['M', 'M', 'M', 'M', 'W'])\n",
      "[('M', [\"i don't think any one there has ebola bob latta you should be back in washington actually getting something done there on the house floor.\"], False), ('M', [';-)...anything other than jeans and t-shirts are superfluous, by the way.', \"'update your wardrobe'...pfft.\"], False), ('M', ['meh, i could only get to 8. need to work up.'], False), ('M', ['a bill consisting of a single sentence.', 'very well done, sir.'], True), ('W', ['so far, so good.', 'thx !'], True)]\n"
     ]
    }
   ],
   "source": [
    "print((train_data[0][0:5], train_data[1][0:5], train_data[2][0:5]))\n",
    "print(corpus_to_sentences((train_data[0][0:5], train_data[1][0:5], train_data[2][0:5])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 2) Normalisez chaque corpus au moyen d’expressions régulières en annotant les négations avec _Neg L’annotation de la négation doit ajouter un suffixe _NEG à chaque mot qui apparait entre une négation et un signe de ponctuation qui identifie une clause. Exemple : \n",
    "No one enjoys it.  no one_NEG enjoys_NEG it_NEG .\n",
    "I don’t think I will enjoy it, but I might.  i don’t think_NEG i_NEG will_NEG enjoy_NEG it_NEG, but i might."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def normalise_doc(doc):\n",
    "    normalised_doc = []\n",
    "    for sentence in doc:\n",
    "        normalised = re.sub(r\"\"\"(n't|\\b(?:never|no|nothing|nowhere|noone|none|not|\n",
    "                havent|hasnt|hadnt|cant|couldnt|shouldnt|\n",
    "                wont|wouldnt|dont|doesnt|didnt|isnt|arent|aint))\\b[\\w\\s]+[^\\w\\s]\"\"\", lambda match: re.sub(r'(\\s+)(\\w+)', r'\\1\\2_NEG', match.group(0)), sentence, flags=re.IGNORECASE)\n",
    "        normalised_doc.append(normalised)\n",
    "    return normalised_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"i don't think_NEG any_NEG one_NEG there_NEG has_NEG ebola_NEG bob_NEG latta_NEG you_NEG should_NEG be_NEG back_NEG in_NEG washington_NEG actually_NEG getting_NEG something_NEG done_NEG there_NEG on_NEG the_NEG house_NEG floor_NEG.\",\n",
       " \";-)...anything other than jeans and t-shirts are superfluous, by the way. 'update your wardrobe'...pfft.\",\n",
       " 'meh, i could only get to 8. need to work up.',\n",
       " 'a bill consisting of a single sentence. very well done, sir.',\n",
       " 'so far, so good. thx !',\n",
       " 'my buddy jeff johnson was your prop master on that.',\n",
       " 'she had me at everlasting youth.',\n",
       " 'congratulations to you for a well deserved recognition!',\n",
       " 'baffoon, idiot, dumb. the intelegent conversation continues.......',\n",
       " \"they don't deserve_NEG that_NEG honour_NEG(stupid hollywood movie business people)\",\n",
       " 'yawn! is this honestly news?',\n",
       " 'same to you brotha! get at it :)',\n",
       " 'would good to know how the age of all of these things have been measured.',\n",
       " 'the perfect society is shaped in the form of a pyramid, the old at the top and the young at the bottom supporting the old. if we kill off the young that will take care of us with thier s.s. taxes, we will have no support_NEG. this is the reason social security is failing.',\n",
       " 'so genuine. so true. so good and loving. thank you.',\n",
       " 'i am so totally going to try and make this one. thanks for the reminder.',\n",
       " 'was on holiday in nz a year ago, amazing place',\n",
       " 'yeah, i recognized you as a mod. ^(funny that were having 2 convos in 2 different places in the same thread.)',\n",
       " 'damn straight. now lets get our main lead and the good female director!',\n",
       " \"god bless the children and their family's amen\",\n",
       " 'my grandfather had a garden and one of my greatest memories of childhood is having fresh carrots. he would pull them straight from the soil wash it off and i would sit and enjoy. i still garden to this day.',\n",
       " 'thanks for the 4th of july wishes and for all you do for texas!',\n",
       " 'did u jerk off when u got back to ur car or did u manage to wait until u got home?',\n",
       " 'i would like to volunteer to help in your campaign headquarters',\n",
       " 'i twittered this. i found it very excellent and (insightful). i only could watch half of it, because i was relating kind of how my life is now. i hope to watch the other half or hear from you soon. \"is there a black and a white area?\" :)']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalise_doc(train_data[0][0:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 3) Segmentez chaque phrase en mots (tokenisation) et stockez-les dans un fichier `nomcorpus`_mots.csv. (Une phrase par ligne, chaque token séparé par un espace, il n’est pas nécessaire de stocker la phrase non segmentée ici) ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def tokenize_sentence(sentence):\n",
    "    return regexp_tokenize(sentence, \"[\\w']+\")\n",
    "\n",
    "def tokenize_doc(doc):\n",
    "    results = []\n",
    "    for sentence in doc:\n",
    "        tokens = tokenize_sentence(sentence)\n",
    "        results.append(\" \".join(tokens))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"i don't think any one there has ebola bob latta you should be back in washington actually getting something done there on the house floor\",\n",
       " \"anything other than jeans and t shirts are superfluous by the way 'update your wardrobe' pfft\"]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_doc(train_data[0][0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 4) Lemmatisez les mots et stockez les lemmes dans un fichier `nomcorpus`_lemmes.csv (une phrase par ligne, les lemmes séparés par un espace) ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def lemmatisation_sentence(wordList):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    word_tuples = nltk.pos_tag(wordList)\n",
    "    results = []\n",
    "    for word, tag in word_tuples:\n",
    "        if tag[0].lower() in ['a', 'r', 'n', 'v']:\n",
    "            word = word.lower()\n",
    "            if '_neg' in word:\n",
    "                word = word.replace('_neg', '')\n",
    "                lemme = lemmatizer.lemmatize(word, tag[0].lower())\n",
    "                lemme = lemme + '_NEG'\n",
    "                results.append(lemme)\n",
    "            else:\n",
    "                results.append(lemmatizer.lemmatize(word, tag[0].lower()))\n",
    "        else:\n",
    "            results.append(word)\n",
    "    return results\n",
    "\n",
    "def lemmatize_doc(doc):\n",
    "    results = []\n",
    "    for line in doc:\n",
    "        lemmas = lemmatisation_sentence(re.split(r'\\s', line))\n",
    "        results.append(\" \".join(lemmas))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"i don't think_NEG any_NEG one_NEG there_NEG has_NEG ebola_NEG bob_NEG latta_NEG you_NEG should_NEG be_NEG back_NEG in_NEG washington_NEG actually_NEG getting_NEG something_NEG done_NEG there_NEG on_NEG the_NEG house_NEG floor_NEG.\", \";-)...anything other than jeans and t-shirts are superfluous, by the way. 'update your wardrobe'...pfft.\", 'meh, i could only get to 8. need to work up.', 'a bill consisting of a single sentence. very well done, sir.', 'so far, so good. thx !']\n",
      "[\"i don't think_NEG any_NEG one_NEG there_NEG ha_NEG ebola_NEG bob_NEG latta_NEG you_NEG should_NEG be_NEG back_NEG in_NEG washington_NEG actually_NEG getting_NEG something_NEG done_NEG there_NEG on_NEG the_NEG house_NEG floor_NEG\", \"anything other than jean and t shirt be superfluous by the way 'update your wardrobe' pfft\", 'meh i could only get to 8 need to work up', 'a bill consist of a single sentence very well do sir', 'so far so good thx']\n"
     ]
    }
   ],
   "source": [
    "print(normalise_doc(train_data[0][0:5]))\n",
    "print(lemmatize_doc(tokenize_doc(normalise_doc(train_data[0][0:5]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 5) Retrouvez la racine des mots (stemming) en utilisant nltk.PorterStemmer(). Stockez-les dans un fichier `nomcorpus`_stems.csv (une phrase par ligne, les racines séparées par une espace) ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def stemming_sentence(wordList):\n",
    "    stemmer = PorterStemmer()\n",
    "    result = []\n",
    "    for word in wordList:\n",
    "        word = word.lower()\n",
    "        if '_neg' in word:\n",
    "            word = word.replace('_neg', '')\n",
    "            stem = stemmer.stem(word)\n",
    "            stem = stem + '_NEG'\n",
    "            result.append(stem)\n",
    "        else:\n",
    "            result.append(stemmer.stem(word))\n",
    "    return result\n",
    "\n",
    "def stems_doc(doc):\n",
    "    results = []\n",
    "    for line in doc:\n",
    "        stems = stemming_sentence(re.split(r'\\s', line))\n",
    "        results.append(\" \".join(stems))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"i don't think_NEG any_NEG one_NEG there_NEG has_NEG ebola_NEG bob_NEG latta_NEG you_NEG should_NEG be_NEG back_NEG in_NEG washington_NEG actually_NEG getting_NEG something_NEG done_NEG there_NEG on_NEG the_NEG house_NEG floor_NEG.\", \";-)...anything other than jeans and t-shirts are superfluous, by the way. 'update your wardrobe'...pfft.\", 'meh, i could only get to 8. need to work up.', 'a bill consisting of a single sentence. very well done, sir.', 'so far, so good. thx !']\n",
      "[\"i don't think_NEG ani_NEG one_NEG there_NEG ha_NEG ebola_NEG bob_NEG latta_NEG you_NEG should_NEG be_NEG back_NEG in_NEG washington_NEG actual_NEG get_NEG someth_NEG done_NEG there_NEG on_NEG the_NEG hous_NEG floor_NEG\", \"anyth other than jean and t shirt are superflu by the way 'updat your wardrobe' pfft\", 'meh i could onli get to 8 need to work up', 'a bill consist of a singl sentenc veri well done sir', 'so far so good thx']\n"
     ]
    }
   ],
   "source": [
    "print(normalise_doc(train_data[0][0:5]))\n",
    "print(stems_doc(tokenize_doc(normalise_doc(train_data[0][0:5]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 6) Ecrivez une fonction qui supprime les mots outils (stopwords) du corpus. Vous devez utiliser la liste de stopwords de NLTK ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def remove_stopwords_wordList(wordList):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [word for word in wordList if word.replace(\"_NEG\", \"\").isalnum() and word.replace(\"_NEG\", \"\") not in stop_words] \n",
    "\n",
    "def remove_stopwords_doc(doc):\n",
    "    results = []\n",
    "    for line in doc:\n",
    "        clean_line = remove_stopwords_wordList(re.split(r'\\s', line))\n",
    "        results.append(\" \".join(clean_line))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['think_NEG one_NEG ebola_NEG bob_NEG latta_NEG back_NEG washington_NEG actually_NEG getting_NEG something_NEG done_NEG house_NEG floor_NEG',\n",
       " 'anything jeans shirts superfluous way pfft',\n",
       " 'meh could get 8 need work',\n",
       " 'bill consisting single sentence well done sir',\n",
       " 'far good thx']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopwords_doc(tokenize_doc(normalise_doc(train_data[0][0:5])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 7) Écrivez une fonction preprocess_corpus(corpus) qui prend un corpus brut stocké dans un fichier.csv, effectue les étapes précédentes, puis stocke le résultat de ces différentes opérations dans un fichier corpus _norm.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def preprocess_corpus(input_file: str, output_file: str) -> None:\n",
    "    matches  = re.findall(r\"\\w+_norm.csv$\", output_file)\n",
    "    if matches:\n",
    "        output_file = matches[0].split(\"_norm.csv\")[0]\n",
    "    output_file += \"/train\" if \"train\" in input_file else \"/test\"\n",
    "    \n",
    "    train_data = read_data(input_file)\n",
    "    \n",
    "    corpus = corpus_to_sentences(train_data)\n",
    "    write_corpus_to_csv(corpus, output_file + '_phrases')\n",
    "\n",
    "    normalised_corpus = process_list_corpus_tup(normalise_doc, corpus)\n",
    "    write_corpus_to_csv(normalised_corpus, output_file + '_normalised')\n",
    "\n",
    "    #tokenized_corpus = process_dict_corpus(tokenize_doc, normalised_corpus)\n",
    "    tokenized_corpus = process_list_corpus_tup(tokenize_doc, corpus)\n",
    "    write_corpus_to_csv(tokenized_corpus, output_file + '_mots')\n",
    "    #print(tokenized_corpus)\n",
    "\n",
    "#     lemmatized_corpus = process_list_corpus_tup(lemmatize_doc, tokenized_corpus)\n",
    "#     write_corpus_to_csv(lemmatized_corpus, output_file + '_lemmes')\n",
    "#     #print(lemmatized_corpus)\n",
    "\n",
    "    stemmed_corpus = process_list_corpus_tup(stems_doc, tokenized_corpus)\n",
    "    write_corpus_to_csv(stemmed_corpus, output_file + '_stems')\n",
    "    #print(stemmed_corpus)\n",
    "\n",
    "    removed_stopwords_corpus = process_list_corpus_tup(remove_stopwords_doc, stemmed_corpus)\n",
    "    write_corpus_to_csv(removed_stopwords_corpus, output_file + '_norm')\n",
    "    #print(removed_stopwords_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "preprocess_corpus(os.path.join(data_path, \"train.csv\"), os.path.join(output_path))\n",
    "\n",
    "preprocess_corpus(os.path.join(data_path, \"test.csv\"), os.path.join(output_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Exploration des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Complétez les fonctions retournant les informations suivantes (une fonction par information, chaque fonction prenant en argument un corpus composé d'une liste de phrases segmentées en tokens(tokenization)) ou une liste de genres et une liste de sentiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic corpus for quick testing\n",
    "corpus = [ (\"M\", [\"I am home_NEG\", \"You are late_NEG\", \"He is fine\"], \"N\" ),\n",
    "           (\"W\", [\"let's go home_NEG\", \"I'm happy\"], \"P\" ),\n",
    "           (\"M\", [\"You are alone\"], \"N\" ) ]\n",
    "\n",
    "# Helper function\n",
    "def get_distinct_number(data: List[object]) -> int:\n",
    "\n",
    "    unique_list = []\n",
    "    for sentence in data: \n",
    "        for word in sentence:\n",
    "            if word not in unique_list: \n",
    "                unique_list.append(word) \n",
    "\n",
    "    return len(unique_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### a. Le nombre total de tokens (mots non distincts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def total_tokens(corpus: List[object]) -> int:\n",
    "    total = 0\n",
    "    for doc in corpus:\n",
    "        for sentence in doc[1]:\n",
    "            total += len(sentence.split())\n",
    "    return total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### b. Le nombre total de types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def total_types(corpus: List[object]) -> int:\n",
    "    types = []\n",
    "    for doc in corpus:\n",
    "        for sentence in doc[1]:\n",
    "            for word in sentence.split():\n",
    "                types.append(word)\n",
    "    return len(set(types))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### c. Le nombre total de phrases avec négation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def total_neg(corpus: List[object]) -> int:\n",
    "    total = 0\n",
    "    for doc in corpus:\n",
    "        for sentence in doc[1]:\n",
    "            if \"_NEG\" in sentence:\n",
    "                total += 1\n",
    "    return total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### d. Le ratio token/type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def TTR(corpus: List[object]) -> int:\n",
    "    return total_tokens(corpus) / total_types(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### e. Le nombre total de lemmes distincts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lem_count(corpus: List[object]) -> int: # corpus already lemmatized\n",
    "    return get_distinct_number(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### f. Le nombre total de racines (stems) distinctes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_stem_count(corpus: List[object]) -> int: # corpus already stemmed\n",
    "    return get_distinct_number(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### g. Le nombre total de documents (par classe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_documents_count(corpus: List[object]) -> object:\n",
    "    result = {}\n",
    "    for doc in corpus: \n",
    "        sentiment = doc[2]\n",
    "        if sentiment in result:\n",
    "            result[sentiment] += 1\n",
    "        else:\n",
    "            result.setdefault(sentiment, 1)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### h. Le nombre total de phrases (par classe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_sentences_count(corpus: List[object]) -> object:\n",
    "    result = {}\n",
    "    for doc in corpus: \n",
    "        sentiment = doc[2]\n",
    "        sentences = doc[1]\n",
    "        if sentiment in result:\n",
    "            result[sentiment] += len(sentences)\n",
    "        else:\n",
    "            result.setdefault(sentiment, len(sentences))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### i. Le nombre total de phrases avec négation (par classe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_sentences_neg_count(corpus: List[object]) -> object:\n",
    "    result = {}\n",
    "    for doc in corpus: \n",
    "        sentiment = doc[2]\n",
    "        sentences_neg = [sentence for sentence in doc[1] if sentence.find(\"_NEG\") >= 0]\n",
    "        if sentiment in result:\n",
    "            result[sentiment] += len(sentences_neg)\n",
    "        else:\n",
    "            result.setdefault(sentiment, len(sentences_neg))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### j. Le pourcentage de réponses positives par genre de la personne à qui cette réponse est faite (op_gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_positive_answers(corpus: List[object]) -> object:\n",
    "    result = {\"M\": 0, \"W\": 0}\n",
    "    for doc in corpus: \n",
    "        sentiment = doc[2]\n",
    "        genre = doc[0]\n",
    "        if sentiment:\n",
    "            if genre in result:\n",
    "                result[genre] += 1\n",
    "            else:\n",
    "                result.setdefault(genre, 1)\n",
    "\n",
    "    response = {}\n",
    "    response[\"M\"] = str((result[\"M\"] / (result[\"M\"]+result[\"W\"])) * 100) + \"%\"  # result[\"M\"]+result[\"W\"] ?\n",
    "    response[\"W\"] = str((result[\"W\"] / (result[\"M\"]+result[\"W\"])) * 100) + \"%\"\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 2) Écrivez la fonction explore(corpus, sentiments, genders) qui calcule et affiche toutes ces informations, précédées d'une légende reprenant l’énoncé de chaque question (a,b, ….j)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def explore(corpus: List[object]) -> None:\n",
    "    normalised_corpus = process_list_corpus_tup(normalise_doc, corpus)\n",
    "    tokenized_corpus = process_list_corpus_tup(tokenize_doc, corpus)\n",
    "    lemmatized_corpus = process_list_corpus_tup(lemmatize_doc, tokenized_corpus)\n",
    "    stemmed_corpus = process_list_corpus_tup(stems_doc, tokenized_corpus)\n",
    "    \n",
    "    print(\"a. Le nombre total de tokens (mots non distincts)\")\n",
    "    display(total_tokens(corpus))\n",
    "    print(\"\\nb. Le nombre total de types\")\n",
    "    display(total_types(corpus))\n",
    "    print(\"\\nc. Le nombre total de phrases avec négation\")\n",
    "    display(total_neg(normalised_corpus))\n",
    "    print(\"\\nd. Le ratio token/type\")\n",
    "    display(TTR(corpus))\n",
    "    print(\"\\ne. Le nombre total de lemmes distincts\")\n",
    "    display(get_lem_count(lemmatized_corpus))\n",
    "    print(\"\\nf. Le nombre total de racines (stems) distinctes\")\n",
    "    display(get_stem_count(stemmed_corpus))\n",
    "    print(\"\\ng. Le nombre total de documents (par classe)\")\n",
    "    display(get_documents_count(corpus))\n",
    "    print(\"\\nh. Le nombre total de phrases (par classe)\")\n",
    "    display(get_sentences_count(corpus))\n",
    "    print(\"\\ni. Le nombre total de phrases avec négation (par classe)\")\n",
    "    display(get_sentences_neg_count(normalised_corpus))\n",
    "    print(\"\\nj. Le pourcentage de réponses positives par genre de la personne à qui cette réponse est faite (op_gender)\")\n",
    "    display(get_positive_answers(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a. Le nombre total de tokens (mots non distincts)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "154680"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "b. Le nombre total de types\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "26769"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "c. Le nombre total de phrases avec négation\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1895"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "d. Le ratio token/type\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5.778325675221338"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "e. Le nombre total de lemmes distincts\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8685"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "f. Le nombre total de racines (stems) distinctes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8680"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "g. Le nombre total de documents (par classe)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{False: 2285, True: 6756}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "h. Le nombre total de phrases (par classe)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{False: 4302, True: 13109}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "i. Le nombre total de phrases avec négation (par classe)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{False: 890, True: 1005}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "j. Le pourcentage de réponses positives par genre de la personne à qui cette réponse est faite (op_gender)\n",
      "{'M': 3198, 'W': 3558}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'M': '47.335701598579035%', 'W': '52.664298401420965%'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data = read_data(os.path.join(data_path, \"train.csv\"))\n",
    "corpus = corpus_to_sentences(train_data)\n",
    "\n",
    "explore(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 3) Calculer une table de fréquence (lemme, rang (le mot le plus fréquent a le rang 1 etc.) ; fréquence (le nombre de fois où il a été vu dans le corpus).  Seuls les N mots les plus fréquents du vocabulaire (N est un paramètre) doivent être gardés. Vous devez stocker les 1000 premières lignes de cette table dans un fichier nommé table_freq.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(doc, vocabulary_dict):\n",
    "    #vocabulary_dict = {}\n",
    "    for line in doc:\n",
    "        words_list = re.split(r'\\s', line)\n",
    "        for word in words_list:\n",
    "            if word in vocabulary_dict:\n",
    "                vocabulary_dict[word] += 1\n",
    "            else:\n",
    "                vocabulary_dict.setdefault(word, 1)\n",
    "    return vocabulary_dict\n",
    "\n",
    "def count_words_corpus(corpus):\n",
    "    vocabulary_dict = {}\n",
    "    for doc in corpus:\n",
    "        vocabulary_dict = count_words(doc[1], vocabulary_dict) \n",
    "    return vocabulary_dict\n",
    "\n",
    "\n",
    "def frequence_table_corpus(tokenized_corpus, N):\n",
    "    dict_word = count_words_corpus(tokenized_corpus)\n",
    "    result = sorted(dict_word.items(), key=lambda x: x[1], reverse=True)\n",
    "    if len(result) > N:\n",
    "        result = result[0:N]\n",
    "    return result\n",
    "\n",
    "def write_frequence_to_csv(tup_list):\n",
    "    result = list(zip(*tup_list))\n",
    "    rang = range(1, len(result[1])+1)\n",
    "    dict_result = {\"word\" : result[0], \"frequence\" : result[1], \"rang\":  rang}\n",
    "    dt = pd.DataFrame(dict_result)\n",
    "    dt.to_csv('table_freq.csv')\n",
    "    \n",
    "def display_freq_table(tup_list):\n",
    "    result = list(zip(*tup_list))\n",
    "    rang = range(1, len(result[1])+1)\n",
    "    dict_result = {\"word\" : result[0], \"frequence\" : result[1], \"rang\":  rang}\n",
    "    dt = pd.DataFrame(dict_result)\n",
    "    display(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>frequence</th>\n",
       "      <th>rang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>5043</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>to</td>\n",
       "      <td>4256</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I</td>\n",
       "      <td>3398</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>you</td>\n",
       "      <td>3349</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>and</td>\n",
       "      <td>3253</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>lead</td>\n",
       "      <td>17</td>\n",
       "      <td>996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>Amen</td>\n",
       "      <td>17</td>\n",
       "      <td>997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>attention</td>\n",
       "      <td>17</td>\n",
       "      <td>998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>politicians</td>\n",
       "      <td>17</td>\n",
       "      <td>999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>More</td>\n",
       "      <td>17</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            word  frequence  rang\n",
       "0            the       5043     1\n",
       "1             to       4256     2\n",
       "2              I       3398     3\n",
       "3            you       3349     4\n",
       "4            and       3253     5\n",
       "..           ...        ...   ...\n",
       "995         lead         17   996\n",
       "996         Amen         17   997\n",
       "997    attention         17   998\n",
       "998  politicians         17   999\n",
       "999         More         17  1000\n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data = read_data(os.path.join(data_path, \"train.csv\"))\n",
    "corpus = corpus_to_sentences(train_data)\n",
    "tokenized_corpus = process_list_corpus_tup(tokenize_doc, corpus)\n",
    "\n",
    "result = frequence_table_corpus(tokenized_corpus, 1000)\n",
    "display_freq_table(result)\n",
    "\n",
    "write_frequence_to_csv(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 2. Classification automatique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### a) Classification  automatique avec un modèle sac de mots (unigrammes), Naive Bayes et la régression logistique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "En utilisant la librairie scikitLearn et l’algorithme Multinomial Naive Bayes et Logistic Regression, effectuez la classification des textes avec un modèle sac de mots unigramme pondéré avec TF-IDF.  Vous devez entrainer chaque modèle sur l’ensemble d’entrainement et le construire à partir de votre fichier corpus_train.csv. \n",
    "\n",
    "Construisez et sauvegardez votre modèle sac de mots avec les données d’entrainement en testant les pré-traitements suivants (séparément et en combinaison): tokenisation, lemmatisation, stemming, normalisation des négations, et suppression des mots outils. Vous ne devez garder que la combinaison d’opérations qui vous donne les meilleures performances sur le corpus de test. Indiquez dans un commentaire les pré-traitements qui vous amènent à votre meilleure performance (voir la section 3 – évaluation). Il est possible que la combinaison optimale ne soit pas la même selon que vous utilisiez la régression logistique ou Naive Bayes. On s’attend à avoir deux modèles optimaux, un pour Naive Bayes, et un avec régression logistique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combinations_maker(process_list):\n",
    "    tuple_list = []\n",
    "    for i, _ in enumerate(process_list):\n",
    "        if i == 2:\n",
    "            break\n",
    "        tuple_list.extend(list(combinations(process_list, i+1)))\n",
    "    return [item for item in tuple_list if item != (lemmatize_doc, stems_doc) and item !=  (stems_doc, lemmatize_doc)]\n",
    "\n",
    "\n",
    "def combinations_process_corpus(process_list, tokenized_corpus):\n",
    "    result = []\n",
    "    comb_process_list = combinations_maker(process_list)\n",
    "    for comb in comb_process_list:\n",
    "        corpus = tokenized_corpus\n",
    "        for func in comb:\n",
    "            corpus = process_list_corpus_tup(func, corpus)\n",
    "        result.append((corpus, tuple(i.__name__ for i in comb)))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sac de mots + TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer #CountVectorizer, TfidfVectorizer\n",
    "\n",
    "X_train, y_train, X_test, y_test, X_train_tfidf = [], [], [], [], []\n",
    "process_list = [lemmatize_doc, stems_doc, remove_stopwords_doc]\n",
    "\n",
    "train_data = read_data(os.path.join(data_path, \"train.csv\"))\n",
    "corpus = corpus_to_sentences(train_data)\n",
    "normalised_corpus = process_list_corpus_tup(normalise_doc, corpus)\n",
    "tokenized_corpus = process_list_corpus_tup(tokenize_doc, corpus)\n",
    "\n",
    "combinations_result = combinations_process_corpus(process_list, tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('M', [\"I don't think any one there have ebola bob latta You should be back in washington actually get something do there on the house floor\"], False)\n"
     ]
    }
   ],
   "source": [
    "# for i, pre_process_set in enumerate(combinations_result):\n",
    "    \n",
    "#     X_train[i], y_train[i] = processed_train_data[0:2], processed_train_data[2]\n",
    "#     X_test[i], y_test[i] = processed_test_data[0:2], processed_test_data[2] \n",
    "\n",
    "#     tfidf_vectorizer = TfidfVectorizer()\n",
    "#     X_train_tfidf[i] = tfidf_vectorizer.fit_transform(X_train[i]).toarray()\n",
    "#     X_test_tfidf[i] = tfidf_vectorizer.transform(X_test[i])\n",
    "#     print(X_train_tfidf[i].shape)\n",
    "\n",
    "print(combinations_result[0][0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "y_pred = []\n",
    "for i in len(combinations_result):\n",
    "    model = MultinomialNB()\n",
    "    model.fit(X_train_tfidf[i], y_train[i])\n",
    "\n",
    "    y_pred[i] = classifier.predict(X_test_tfidf[i])\n",
    "    # accuracy: np.mean(y_pred[i] == y_test[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Régression Logistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "    \n",
    "y_pred = []\n",
    "for i in len(combinations_result):\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train_tfidf[i], y_train[i])\n",
    "\n",
    "    y_pred[i] = classifier.predict(X_test_tfidf[i])\n",
    "    # accuracy: np.mean(y_pred[i] == y_test[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save/load a model\n",
    "\n",
    "with open('text_classifier', 'wb') as picklefile:\n",
    "    pickle.dump(classifier, picklefile)\n",
    "    \n",
    "with open('text_classifier', 'rb') as training_model:\n",
    "    model = pickle.load(training_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "###  b) Autre représentation pour l’analyse de sentiments et classification automatique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "On vous propose maintenant d’utiliser une nouvelle représentation de chaque document à classifier.\n",
    "Vous devez créer à partir de votre corpus la table suivante :\n",
    "\n",
    "| Vocabulaire | Freq-positive | Freq-négative |\n",
    "|-------------|---------------|---------------|\n",
    "| happy | 10 | 1 |\n",
    "| ... | ... | ... |\n",
    "\n",
    "Où :\n",
    "\n",
    "• Vocabulaire représente tous les types (mots uniques) de votre corpus d’entrainement\n",
    "\n",
    "• Freq-positive : représente la somme des fréquences du mot dans tous les documents de la classe positive\n",
    "\n",
    "• Freq-négative : représente la somme des fréquences du mot dans tous les documents de la classe négative\n",
    "\n",
    "Notez qu’en Python, vous pouvez créer un dictionnaire associant à tout (mot, classe) une fréquence.\n",
    "Ensuite il vous suffit de représenter chaque document par un vecteur à 3 dimensions dont le premier élément représente un biais (initialisé à 1), le deuxième élément représente la somme des fréquences positives (freq-pos) de tous les mots uniques (types) du document et enfin le troisième élément représente la somme des fréquences négative (freq-neg) de tous les mots uniques du document. \n",
    "\n",
    "En utilisant cette représentation ainsi que les pré-traitements suggérés, trouvez le meilleur modèle possible en testant la régression logistique et Naive Bayes. Vous ne devez fournir que le code de votre meilleur modèle dans votre notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 0, 1),\n",
       " ('am', 0, 1),\n",
       " ('home_NEG', 1, 1),\n",
       " ('You', 0, 2),\n",
       " ('are', 0, 2),\n",
       " ('late_NEG', 0, 1),\n",
       " ('He', 0, 1),\n",
       " ('is', 0, 1),\n",
       " ('fine', 0, 1),\n",
       " (\"let's\", 1, 0),\n",
       " ('go', 1, 0),\n",
       " (\"I'm\", 1, 0),\n",
       " ('happy', 1, 0),\n",
       " ('alone', 0, 1)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[(1, 1, 11), (2, 5, 1), (3, 0, 5)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def calculFrequences(corpus: List[object]) -> object:\n",
    "    dic = {}\n",
    "    for doc in corpus: \n",
    "        sentiment = doc[2]\n",
    "        for sentence in doc[1]: \n",
    "            for word in sentence.split():\n",
    "                if word in dic:\n",
    "                    dic[word][sentiment] += 1\n",
    "                else:\n",
    "                    dic_sent = {'P': 0, 'N': 0}\n",
    "                    dic_sent[sentiment]  = 1\n",
    "                    dic.setdefault(word, dic_sent)\n",
    "    return dic\n",
    "\n",
    "def representData1(freq: object) -> object:\n",
    "    result = []\n",
    "    for word in freq:\n",
    "        result.append((word, freq[word][\"P\"], freq[word][\"N\"]))\n",
    "        \n",
    "    return result\n",
    "\n",
    "def representData2(freq: object, corpus: List[object]) -> object:\n",
    "    result = []\n",
    "    doc_index = 0\n",
    "    for doc in corpus: \n",
    "        pos_freq = 0\n",
    "        neg_freg = 0\n",
    "        doc_index += 1\n",
    "        for sentence in doc[1]: \n",
    "            for word in sentence.split():\n",
    "                pos_freq += freq[word][\"P\"]\n",
    "                neg_freg += freq[word][\"N\"]\n",
    "        result.append((doc_index, pos_freq, neg_freg))\n",
    "\n",
    "    return result\n",
    "\n",
    "freq = calculFrequences(corpus)\n",
    "display(representData1(freq))\n",
    "display(representData2(freq, corpus))\n",
    "\n",
    "# from sklearn.feature_extraction.text import TfidfTransformer #CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# X_train, y_train, X_test, y_test, X_train_tfidf = [], [], [], [], []\n",
    "# process_list = [lemmatize_doc, stems_doc, remove_stopwords_doc]\n",
    "\n",
    "# train_data = read_data(os.path.join(data_path, \"train.csv\"))\n",
    "# corpus = corpus_to_sentences(train_data)\n",
    "# normalised_corpus = process_list_corpus_tup(normalise_doc, corpus)\n",
    "# tokenized_corpus = process_list_corpus_tup(tokenize_doc, corpus)\n",
    "\n",
    "# combinations_result = combinations_process_corpus(process_list, tokenized_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "y_pred = []\n",
    "for i in len(combinations_result):\n",
    "    model = MultinomialNB()\n",
    "    model.fit(X_train_tfidf[i], y_train[i])\n",
    "\n",
    "    y_pred[i] = classifier.predict(X_test_tfidf[i])\n",
    "    # accuracy: np.mean(y_pred[i] == y_test[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Régression Logistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "    \n",
    "y_pred = []\n",
    "for i in len(combinations_result):\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train_tfidf[i], y_train[i])\n",
    "\n",
    "    y_pred[i] = classifier.predict(X_test_tfidf[i])\n",
    "    # accuracy: np.mean(y_pred[i] == y_test[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 3. Évaluation et discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### a) Pour déterminer la performance de vos modèles, vous devez tester vos modèles de classification sur l’ensemble de test et générer vos résultats pour chaque modèle dans une table avec les métriques suivantes : Accuracy et pour chaque classe, la précision, le rappel et le F1 score. On doit voir cette table générée dans votre notebook avec la liste de vos modèles de la section 2 et leurs performances respectives. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "for i in len(combinations):\n",
    "    print(classification_report(y_test[i], y_pred[i], target_names=[\"P\", \"N\"]))\n",
    "    print(confusion_matrix(y_test[i], y_pred[i]))\n",
    "    print(accuracy_score(y_test[i], y_pred[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### b) Générez un graphique qui représente la performance moyenne (mean accuracy – 10 Fold cross-validation) de vos différents modèles par tranches de 500 textes sur l’ensemble d’entrainement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### c) Que se passe-t-il lorsque le paramètre de régularisation de la régression logisque (C) est augmenté ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analyse et discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) En considérant les deux types de représentations, répondez aux question suivantes en reportant la question dans le notebook et en inscrivant votre réponse:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Quel est l’impact de l’annotation de la négation ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) La suppression des stopwords est-elle une bonne idée pour l’analyse de sentiments ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d) Le stemming et/ou la lemmatisation sont-ils souhaitables dans le cadre de l’analyse de sentiments ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Contribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complétez la section en haut du notebook indiquant la contribution de chaque membre de l’équipe en indiquant ce qui a été effectué par chaque membre et le pourcentage d’effort du membre dans le TP. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
