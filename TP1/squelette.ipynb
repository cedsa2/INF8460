{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## École Polytechnique de Montréal\n",
    "## Département Génie Informatique et Génie Logiciel\n",
    "\n",
    "## INF8460 – Traitement automatique de la langue naturelle - TP1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectifs d'apprentissage: \n",
    "\n",
    "•\tSavoir accéder à un corpus, le nettoyer et effectuer divers pré-traitements sur les données\n",
    "•\tSavoir effectuer une classification automatique des textes pour l’analyse de sentiments\n",
    "•\tEvaluer l’impact des pré-traitements sur les résultats obtenus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Équipe et contributions \n",
    "Veuillez indiquer la contribution effective de chaque membre de l'équipe en pourcentage et en indiquant les modules ou questions sur lesquelles chaque membre a travaillé\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cedric Sadeu: x% (détail)\n",
    "\n",
    "Mamoudou Sacko: x% (détail)\n",
    "\n",
    "Oumayma Messoussi: x% (détail)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Librairies externes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "hidden": true,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     C:\\Users\\mamoudou\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     C:\\Users\\mamoudou\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\mamoudou\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from typing import List, Tuple\n",
    "from IPython.display import display\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "from nltk import tokenize\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import regexp_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Valeurs globales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_path = \"data\"\n",
    "output_path = \"output\"\n",
    "\n",
    "corpus = [ (\"M\", [\"I am home NEG\", \"You are late NEG\", \"He is fine\"], \"N\" ),\n",
    "           (\"W\", [\"let's go home NEG\", \"Im happy\"], \"P\" ),\n",
    "           (\"M\", [\"You are alone\"], \"N\", ), \n",
    "           (\"M\", [\"Life is good\"], \"P\", ) ]  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path: str) -> Tuple[List[str], List[bool], List[Literal[\"M\", \"W\"]]]:\n",
    "    data = pd.read_csv(path)\n",
    "    inputs = data[\"response_text\"].tolist()\n",
    "    labels = (data[\"sentiment\"] == \"Positive\").tolist()\n",
    "    gender = data[\"op_gender\"].tolist()\n",
    "    return inputs, labels, gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_data = read_data(os.path.join(data_path, \"train.csv\"))\n",
    "test_data = read_data(os.path.join(data_path, \"test.csv\"))\n",
    "\n",
    "train_data = ([text.lower() for text in train_data[0]], train_data[1], train_data[2])\n",
    "test_data = ([text.lower() for text in test_data[0]], test_data[1], test_data[2])\n",
    "\n",
    "print(\"train data line 0: \\n\\t- response_text: \" + str(train_data[0][0]) \n",
    "      + \"\\n\\t- sentiment: \" + str([0]) + \"\\n\\t- gender: \" + str(train_data[2][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pré-traitement et Exploration des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Lecture et prétraitement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Dans cette section, vous devez compléter la fonction preprocess_corpus qui doit être appelée sur les fichiers train.csv et test.csv. La fonction preprocess_corpus appellera les différentes fonctions créées ci-dessous. Les différents fichiers de sortie doivent se retrouver dans le répertoire output.  Chacune des sous-questions suivantes devraient être une ou plusieurs fonctions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_csv(sentences, corpus_name):\n",
    "    dt = pd.DataFrame(sentences, columns =['sentences'])\n",
    "    dt.to_csv(corpus_name + '.csv')\n",
    "\n",
    "def write_corpus_to_csv(corpus, corpus_name):\n",
    "    sentences = []\n",
    "    for doc in corpus:\n",
    "        sentences.extend(doc[1])\n",
    "    write_to_csv(sentences, corpus_name)\n",
    "\n",
    "def process_list_corpus_tup(func, corpus_tup_list):\n",
    "    process_corpus = []\n",
    "    for doc in corpus_tup_list:\n",
    "        result = (doc[0], func(doc[1]), doc[2])\n",
    "        process_corpus.append(result)\n",
    "    return process_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 1) Segmentez chaque corpus en phrases, et stockez-les dans un fichier `nomcorpus`_phrases.csv (une phrase par ligne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def make_sentence(line):\n",
    "    sentences = tokenize.sent_tokenize(line)\n",
    "    return [sentence for sentence in sentences if re.findall(r\"[\\w]+\", sentence)]\n",
    "\n",
    "\n",
    "def corpus_to_sentences(data):  \n",
    "    #check data is not empty and lists inside data have the same length\n",
    "    if (not data) or [len(element) for element in data if len(element) != len(data[0])]:\n",
    "        raise ValueError(\"Data is not valid.\")\n",
    "    vocabulary = []\n",
    "    for i, item in enumerate(data[0]):\n",
    "        document = (data[2][i], make_sentence(item), data[1][i])\n",
    "        vocabulary.append(document)\n",
    "    \n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_to_sentences((train_data[0][0:2], train_data[1][0:2], train_data[2][0:2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 2) Normalisez chaque corpus au moyen d’expressions régulières en annotant les négations avec _Neg L’annotation de la négation doit ajouter un suffixe _NEG à chaque mot qui apparait entre une négation et un signe de ponctuation qui identifie une clause. Exemple : \n",
    "No one enjoys it.  no one_NEG enjoys_NEG it_NEG .\n",
    "I don’t think I will enjoy it, but I might.  i don’t think_NEG i_NEG will_NEG enjoy_NEG it_NEG, but i might."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def normalise_doc(doc):\n",
    "    normalised_doc = []\n",
    "    for sentence in doc:\n",
    "        #normalised = re.sub(r'\\b(?:not|never|no)\\b[\\w\\s]+[^\\w\\s]', lambda match: re.sub(r'(\\s+)(\\w+)', r'\\1\\2_NEG', match.group(0)), sentence, flags=re.IGNORECASE)\n",
    "        normalised = re.sub(r\"\"\"(n't|\\b(?:never|no|nothing|nowhere|noone|none|not|\n",
    "                havent|hasnt|hadnt|cant|couldnt|shouldnt|\n",
    "                wont|wouldnt|dont|doesnt|didnt|isnt|arent|aint))\\b[\\w\\s]+[^\\w\\s]\"\"\", lambda match: re.sub(r'(\\s+)(\\w+)', r'\\1\\2_NEG', match.group(0)), sentence, flags=re.IGNORECASE)\n",
    "        normalised_doc.append(normalised)\n",
    "    return normalised_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalise_doc(train_data[0][0:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 3) Segmentez chaque phrase en mots (tokenisation) et stockez-les dans un fichier `nomcorpus`_mots.csv. (Une phrase par ligne, chaque token séparé par un espace, il n’est pas nécessaire de stocker la phrase non segmentée ici) ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def tokenize_sentence(sentence):\n",
    "    return regexp_tokenize(sentence.lower(), \"[\\w']+\")\n",
    "\n",
    "def tokenize_doc(doc):\n",
    "    results = []\n",
    "    for sentence in doc:\n",
    "        tokens = tokenize_sentence(sentence)\n",
    "        #print(tokens)\n",
    "        results.append(\" \".join(tokens))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_doc(train_data[0][0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 4) Lemmatisez les mots et stockez les lemmes dans un fichier `nomcorpus`_lemmes.csv (une phrase par ligne, les lemmes séparés par un espace) ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def lemmatisation_sentence(wordList):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    word_tuples = nltk.pos_tag(wordList)\n",
    "    results = []\n",
    "    for word, tag in word_tuples:\n",
    "        if tag[0].lower() in ['a', 'r', 'n', 'v']:\n",
    "            results.append(lemmatizer.lemmatize(word, tag[0].lower()))\n",
    "        else:\n",
    "            results.append(word)\n",
    "    return results\n",
    "\n",
    "def lemmatize_doc(doc):\n",
    "    results = []\n",
    "    for line in doc:\n",
    "        lemmas = lemmatisation_sentence(re.split(r'\\s', line))\n",
    "        results.append(\" \".join(lemmas))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatize_doc(tokenize_doc(train_data[0][0:2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 5) Retrouvez la racine des mots (stemming) en utilisant nltk.PorterStemmer(). Stockez-les dans un fichier `nomcorpus`_stems.csv (une phrase par ligne, les racines séparées par une espace) ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def stemming_sentence(wordList):\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(word) for word in wordList]\n",
    "\n",
    "def stems_doc(doc):\n",
    "    results = []\n",
    "    for line in doc:\n",
    "        stems = stemming_sentence(re.split(r'\\s', line))\n",
    "        results.append(\" \".join(stems))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stems_doc(tokenize_doc(train_data[0][0:2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 6) Ecrivez une fonction qui supprime les mots outils (stopwords) du corpus. Vous devez utiliser la liste de stopwords de NLTK ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def remove_stopwords_wordList(wordList):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [word for word in wordList if word not in stop_words]\n",
    "\n",
    "def remove_stopwords_doc(doc):\n",
    "    results = []\n",
    "    for line in doc:\n",
    "        clean_line = remove_stopwords_wordList(re.split(r'\\s', line))\n",
    "        results.append(\" \".join(clean_line))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_stopwords_doc(tokenize_doc(train_data[0][0:2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 7) Écrivez une fonction preprocess_corpus(corpus) qui prend un corpus brut stocké dans un fichier.csv, effectue les étapes précédentes, puis stocke le résultat de ces différentes opérations dans un fichier corpus _norm.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def preprocess_corpus(input_file: str, output_file: str) -> None:\n",
    "    matches  = re.findall(r\"\\w+_norm.csv$\", output_file)\n",
    "    if matches:\n",
    "        output_file = matches[0].split(\"_norm.csv\")[0]\n",
    "    output_file += \"/train\" if \"train\" in input_file else \"/test\"\n",
    "    \n",
    "    train_data = read_data(input_file)\n",
    "    \n",
    "    corpus = corpus_to_sentences(train_data)\n",
    "    write_corpus_to_csv(corpus, output_file + '_phrases')\n",
    "\n",
    "    normalised_corpus = process_list_corpus_tup(normalise_doc, corpus)\n",
    "    write_corpus_to_csv(normalised_corpus, output_file + '_normalised')\n",
    "\n",
    "    #tokenized_corpus = process_dict_corpus(tokenize_doc, normalised_corpus)\n",
    "    tokenized_corpus = process_list_corpus_tup(tokenize_doc, corpus)\n",
    "    write_corpus_to_csv(tokenized_corpus, output_file + '_mots')\n",
    "    #print(tokenized_corpus)\n",
    "\n",
    "    lemmatized_corpus = process_list_corpus_tup(lemmatize_doc, tokenized_corpus)\n",
    "    write_corpus_to_csv(lemmatized_corpus, output_file + '_lemmes')\n",
    "    #print(lemmatized_corpus)\n",
    "\n",
    "    stemmed_corpus = process_list_corpus_tup(stems_doc, lemmatized_corpus)\n",
    "    write_corpus_to_csv(stemmed_corpus, output_file + '_stems')\n",
    "    #print(stemmed_corpus)\n",
    "\n",
    "    removed_stopwords_corpus = process_list_corpus_tup(remove_stopwords_doc, stemmed_corpus)\n",
    "    write_corpus_to_csv(removed_stopwords_corpus, output_file + '_norm')\n",
    "    #print(removed_stopwords_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true,
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "'total sit would harm part illeg unclear', 'stop amnesti'], False), ('M', [\"it' b c thi world much corpor control red tape ordinari peopl sacrific keep thi rare freedom inform express\"], False), ('M', ['excel talk', 'thi talk explain lot cognit bia like recenc effect optim bia anchor peak end rule etc'], True), ('W', ['introduc vaccin bill strip constitut right citizen wish everyon happi juli 4th', 'juli 4th repres freedom mandat thi bill break 1st 4th 5th 10th amend', 'go back read constitut'], False), ('M', ['pay tax serv countri dure time war make smart make'], False), ('M', ['hi good good humor wise person'], True), ('W', ['say member polit parti whose leader make concert effort restrict vote right everi state control'], False), ('W', ['hop get hamilton counti soon', 'let know'], True), ('W', ['seem contort realiti fit preconceiv notion'], False), ('M', ['thank donald listen us veteran appreci thought legisl'], True), ('M', ['thank rep schweikert appreci staff effort voter justifi hold respons action two senat', \"mccain elect 2016 folk let' get rid\", 'schweikert one good guy'], True), ('W', ['hardli wait see play'], True), ('W', ['good luck hope best polona'], True), ('W', ['thank stick senat sander work hard keep integr', 'bernieorbust'], True), ('W', ['propos cut neg impact new health center huntington park', 'need advoc cut behalf safeti net'], False), ('W', ['guy work hard', 'basebal footbal player make million', 'tenni player underr', 'glad hear good time dure season'], True), ('W', [\"actual hungri get happi becaus mean it' time eat\"], True), ('M', ['dont think germani arent awesom'], True), ('W', ['ay worri thank follow'], True), ('W', ['thank talk realli inspir'], True), ('W', ['slowli discov thi saw talk', 'give detail mean discov say work', 'thank'], True), ('M', ['vote plaid shirt yoho'], True), ('M', ['love think lack qualiti make origin truli origin'], True), ('M', ['live three fork post dome bill', 'appreci mode travel dure champagn', 'best way get close folk want repres', 'know pray'], True), ('W', ['ya steal us', 'bear nwt yellowknif'], False), ('W', ['ive realli look forward thi part rewatch', 'realli love gundam unicorn', 'think agre marida haman integr'], True), ('M', ['congratul ole neighbor commun famili'], True), ('M', ['trust music extens life', 'music trust quit simpli wither away', 'thi may veri well one signific statement life', 'thank thi awaken messag'], True), ('M', [\"well everybodi keep quot marx' thingi religion opium peopl person prefer napoleon' quip religion keep poor murder rich\"], False), ('W', ['si claro que bote porti tengo apollart ati'], True), ('W', ['word live remain day'], True), ('M', ['thi guy get togeth maxi creat simcel'], True), ('W', ['brene brown undoubtedli burst pride lewinski', 'thi bring tear eye feel hope', 'monica rock world'], True), ('M', ['everyth extrem outdat want upgrad one thing youd upgrad 4 make compat'], True), ('W', ['dont make inappropri comment', 'actual think everyon reddit adult'], False), ('M', ['u let evil capitalist exploit republican pig onc accept check say noth get self import photo op', 'clearli latter', 'joke'], False), ('W', ['problem go away'], False), ('W', ['ah shit', 'think first one sophi turner wrong whi label way', 'read littl slower'], False), ('W', ['thi woman never ceas amaz'], True), ('M', ['least fox news bubbl know feed crap', 'thi truli frighten'], False), ('W', ['absolut love courag determin come project', \"it' simpl yet profound statement make photo\", 'sinc last time see underground sewer beauti'], True), ('W', ['lower tax need special gov', 'program'], False), ('M', ['thank follow', 'blog german'], False), ('W', ['thi amaz', 'think therapist could tell thi', 'feel know', 'thank'], True), ('W', ['also quit anoth lose friday night sit dorm room hour away listen brent musburg bobbi knight tell good mizzou look 2 1 2 hour'], True), ('M', [\"thank part thi year' teach child save\", 'tcts2016'], True), ('M', ['veri inspir talk'], True), ('W', ['oh kiss good bye republican nutjob get offic'], False), ('W', [\"use ron paul' plan get rid wast excess start congress\"], False), ('M', ['despic treason'], False), ('M', ['mark trump tri get book deal', 'thi smart mark danger smart', 'money import mark'], True), ('M', ['onli peopl would read comment c take sincer'], False), ('W', ['health first madison', \"there' alway next one\"], True), ('W', ['cute puppi alisha'], True), ('M', ['tariff tariff tariff', 'screw free trade want play fair'], False), ('W', ['lisa bu discuss read open mind thi wonder talk addit thi say thi enjoy explain even shatter dream book chang', 'book absolut help life'], True), ('W', ['bigger fish fri thi week'], False), ('W', ['thank', 'amiga comunidad latina'], True), ('W', [\"that' i'm wrong\", 'enough mostli nake pictur'], True), ('W', ['domest violenc', 'well return isi fighter', \"that' free ticket domest violenc\", 'get elect'], False), ('W', [\"you'r welcom\", 'feel love'], True), ('W', [\"actual think govern take enough people' land\", 'enough alreadi'], False), ('W', ['pleas someth gun control mental health'], False), ('M', ['thank', 'nice bench'], True), ('M', ['steve whi love hi music'], True), ('W', ['prop deserv', 'thx follow', 'appreci'], True), ('W', ['muhammad ali truli greatest time', 'feel confid better place contribut make societi insid outsid ring', 'miss champ'], True), ('W', ['guy abov liter say wasnt perfect game niether whi peopl alway toxic game commun sheesh'], False), ('W', [\"you'r welcom\", \"haha use hate i'v grow love\"], True), ('M', ['still answer import question', 'pleas let thi go away answer call stand whi call'], False), ('M', ['sorri brother', 'wish speedi complet recoveri'], True), ('W', [\"think best sentenc i'v ever hear\"], True), ('W', ['hey daniella big fan'], True), ('W', ['put thi subsequ one http www ted com talk brene_brown_listening_to_sham html facebook send 100 peopl email', 'grate brene brown talk'], True), ('W', ['templ grandin school choic', 'school choic benefit aspergian', \"let' chang face educ\", 'need kid success becaus good societi', 'http www aspergerssyndromepar com school choic html'], True), ('W', ['act peac pleas vote war syria god bless'], True), ('W', ['thi horribl', \"think i'll stick lexington line least cave yet\"], False), ('W', ['brilliant talk make think mani diseas elimin yet', 'million peopl still die', 'show thing complex dig surfac'], True), ('W', ['kid', 'ted sponsor allianz see yet anoth talk financi servic logo financi servic compani display along experiment data', 'bummer', 'ted allow content becom corrupt'], False), ('W', ['wonder use technich busi daili life', 'could tell idea exampl'], True), ('M', ['oh aught go like lead balloon righti'], False), ('W', ['six peopl whose interest republican look'], False), ('W', ['lousi hypocrit statist liar', '300 000 steal us straight face tell us ever see walk match talk fire guess empti crimin fool fire', 'get nation capit', '', 'leav'], False), ('M', ['aw thank', 'good luck journey', 'look forward see take'], True), ('M', ['welcom sunshin state'], True), ('W', ['one guy leav snack dure half time show'], True), ('W', ['thank much', 'whole famili veri glad vote'], True), ('W', ['realli help'], True), ('M', ['thi messag make smile find run mate keen start barbel stuff hope easter hol start'], True), ('W', ['way score deer', 'tine antler point'], True), ('M', [\"i'll\", 'look forward'], True), ('M', ['good day', 'get come home', 'radiat treatment stay mom dure week babi weekend fill hug kiss hubbi oldest son', '4 21 go'], True), ('M', [\"yeah gotta understand hillari clinton way abov law fbi well guess they'r allow bend rule certain peopl find thi extrem hard believ happen it' disgust\"], False), ('W', ['nice', 'realli wonder'], True), ('W', ['golden glove clutch bat outstand'], True), ('W', ['read convict condit bodyweight part faq', 'doesnt requir weight faint sever minim risk get hurt'], True), ('W', ['thumb care ordinari peopl'], True), ('M', ['like becaus recogn import human histori univers', 'veri common', 'use listen repres veri veri veri veri veri small part univers histori'], True), ('W', ['timeless present'], True), ('M', ['least tri'], True), ('W', [\"therapist actual email link thi along he' abl help see thi brilliant\", 'noth well'], True), ('M', ['wow guy asshol', 'guess commun game'], False), ('M', ['thank follow back good luck goal'], True), ('M', ['someth understand'], False), ('W', ['know somebodi would get'], True), ('W', ['deborah present amazingli move messag', 'stun invis insidi yet deep impact war person husband wife group peopl', 'messag truli reach across chasm lend hand ear simpl compel'], True), ('M', ['problem', 'thank follow back'], True), ('M', ['patch', 'thi sound transform web'], True), ('W', ['hey watch comedian fox nearli entertain'], False), ('W', ['saw ur speech hous floor', 'thank tell like', 'dont allow radic gop tea put us revers'], True), ('M', ['love idea focu anim adapt way live'], True), ('M', [\"wish best luck you'r gonna great\"], True), ('W', ['thi tri peopl coma veget state', 'peopl sometim understand say way respond', 'thi could give way commun'], True), ('W', ['sadli much swan tight', ''], True), ('W', ['wish get play probabl qualifi higher ani american alreadi'], True), ('M', ['damag ski aim away obstacl'], False), ('M', ['becaus lead marvel hero super geniu figur themselv', 'peter parker toni stark hank pym bruce banner reed richard charl xavier hi guy chair'], True), ('M', ['amaz game never saw end like befor far rememb', 'unbeliev go buckey way'], True), ('W', ['wonderful awesom person love'], True), ('M', ['go isaac', 'excit read'], True), ('M', ['ye get fact dutch'], True), ('W', ['thank', \"it' long hard road happi get stronger\"], True), ('W', ['banker continu steal needi peopl bare surviv shamelessli outrag fee'], False), ('W', [\"it' equal work problem\"], False), ('M', ['alway look good', 'watch thi one'], True), ('W', ['notic', 'congrat', 'certainli deserv'], True), ('M', ['spend much time grind mark gun high rate fire auto rifl big crucibl', 'disappoint thing end shard come'], False), ('W', ['foam roll becom best thing sore leg'], True), ('M', ['wow awesom gratz'], True), ('M', ['obama miscreant', 'di douch bag say'], False), ('W', ['three word neuro linguist program', \"start intiti stage strong gestur there' littl jumbl word one infanc unform primit longer valid\"], False), ('M', ['thank much', 'sure tri make happi', 'haha post play report veri good say'], True), ('W', ['congrat great even hot dress lol'], True), ('W', ['je vou ador mari', 'le tenni n est plu le mme san vou'], True), ('M', ['guess send us', 'navys com say tell send us thi messag hooyah', 'need solid leadership dc', 'zink vote', 'ns com 5 sir smile'], True), ('W', ['bravo mari', \"joie de t'entendr\"], True), ('W', ['thank follow back', 'could follow'], True), ('W', ['post thi 6 work day', 'constitu either still sleep get readi work school', 'town hall actual particip ask question'], False), ('W', ['saw post forum want stop say good luck', 'still week summer left us teacher enjoy'], True), ('M', ['http www cnn com 2012 09 25 opinion kahn troster anti islam hate ad index html hpt hp_t3'], False), ('W', ['sign teddi bear get aiko nakamura japan dure japan open', 'haha'], True), ('W', [\"funni thing you'v never vote increas educ fund constantli vote reduc educ fund\", \"give i'll gladli bite whi mr picicci smile\"], False), ('M', ['craig never look well', '3'], True), ('W', [\"i'v enjoy mani prop think alreadi friend\"], True), ('W', ['fantast inspir video'], True), ('M', ['thank', 'alreadi say well thank'], True), ('W', ['thank veri much brilliant speech', 'thank share', \"i'm go use public speak class univers student\", 'definit huge teach potenti'], True), ('W', ['choos nau asu entir becaus cost', 'asu turn profit institut focu earn', 'shame happen thi countri'], False), ('W', ['hahahahahahahaha', 'serious', 'ouch'], True), ('W', [\"mississippi can't vote c pray\"], True), ('W', [\"ever start serv peopl entir state i'll congratul\", \"howev i'm sure cater self serv interest\"], False), ('M', ['awesom', 'man'], True), ('W', ['insight help', 'thank'], True), ('M', ['best director ever'], True), ('M', ['explain whi guy continu let obama break law', 'lawless abound c check balanc', 'arrest'], False), ('W', ['bravo mr mr floyd thomur wish could give medal hi love bride support year', 'thank servic'], True), ('M', ['twice much light', 'veri interest way close speech'], True), ('W', ['thank', 'blush'], True), ('W', ['blunt hi ilk add abort languag prevent thi bill move forward', 'know'], False), ('W', ['thank well'], True), ('M', ['wonder softwar use construct thi present', 'hi present slide realli impress', 'like use thi softwar present well'], True), ('M', ['awwwwwwww noooooooo', 'snatefinch', 'singl tear'], True), ('M', ['jesu christ', \"you'r kill\", 'onli dream come true'], False), ('M', ['alway pro'], True), ('W', ['nice meet'], True), ('M', ['thi trueli amaz job blind share drive feel use imposs', 'wish best thi work'], True), ('W', ['bravo hard work thi import topic'], True), ('W', ['project manag commun import skill', 'cours deal human right messag secur issu care', 'thi good brief'], True), ('W', ['whi zimmerman still talk news', 'omg'], False), ('W', ['great ruler thing would better wors leav', 'action caus death destruct everywher', 'hate', 'fact'], False), ('M', ['yah someth name give away get fit'], True), ('M', ['watch think thi one thing everybodi watch', 'becaus watch know becaus id watch would think opposit'], True), ('M', ['bore find enrich inform orderli'], True), ('W', ['whi beauti louisa chirico'], True), ('M', ['support hr38', \"thi chanc let' get\"], True), ('M', ['love watch', 'earth pleasant listen', 'dress veri well tonight', 'pay ani attent becaus jealou'], True), ('M', ['onli citizen vote thing even idea'], False), ('W', ['veri interest talk veri engag speaker', 'make want get happi get move social', 'thank'], True), ('W', [\"i'm watch either becaus without elizabeth may right\"], False), ('M', [\"doubt you'r go us proud\"], True), ('W', ['jim 716b save program reduc cost reduct benefit', 'like denni say research multipl sourc'], False), ('M', [\"i'd rather wait read whole book realli hope it' big one\"], True), ('M', ['even though explain method calcul digit couldnt underatand', 'anyway amaz'], True), ('M', ['thank man sure gonna start get seriou'], True), ('M', [\"thank man wish could hit number you'r get workout good luck well\"], True), ('W', ['proud u alisha'], True), ('W', ['hardli wait back tour aggress style game'], True), ('M', ['thank follow back', 'easi make friend connect peopl fito', 'thank also encourag wish'], True), ('M', ['watch kid', 'old'], True), ('M', ['chri say right talk damn eric first tear ted great project onli 5 day leav sleep post'], True), ('W', ['cheer rock island madison', \"i'm stay holiday inn husband build run\"], True), ('M', ['common sens longer common', 'hi stori guy judg creat work releas sentenc later serv five year jail becaus prosecutor appeal horribl', 'live societi wisdom judgement seem lose', 'thi good talk watch'], True), ('M', ['freak love thi movi', 'one fave'], True), ('W', ['thi touch', \"can't describ would feel someth like thi happen easi help restor one tini piec bigger problem someon life onli know photo\"], True), ('W', ['hahaha good call good call'], True), ('W', ['way make nice thing crass'], False), ('M', ['slower stop'], True), ('M', ['fascin', 'onc underlin one favourit thought know know'], True), ('W', ['traci soooooo good thi flick', 'work without'], True), ('W', ['anoth faux entertain uneduc news event'], False), ('M', ['well deserv'], True), ('M', ['kareem enough respect put suit tie'], False), ('M', ['omg', 'awesom vinc', ''], True), ('M', ['look forward cast vote seth tomorrow'], True), ('M', ['thank much'], True), ('M', [\"i'v alway wonder geniu come open plan offic\"], False), ('M', ['brilliant', 'name product spot'], True), ('W', ['thi poem read peopl train hire jcpenney', 'crazi one'], False), ('W', ['julia sweeney eloqu captur delus theist funni stori', 'hope one day thi watch audienc longer deal knock door kindli young men fiction', 'great present', 'let go god hold secular'], True), ('W', ['howard realli seem get skin gop call exactli', 'lol'], False), ('W', ['good job ladi', 'keep strive'], True), ('W', ['wow', 'shelbi awesom', 'go girl', 'proud'], True), ('W', ['bad thi girl', 'mafia looser'], False), ('W', ['ted allow show face like thi', 'must make lot first also thi woman never see arab world', 'even ani posist move forward backward thi accept ted'], False), ('W', ['dramedi comma', 'food thought', 'lol'], False), ('M', ['thurston gloucest heritag endors seth enthusiast'], True), ('M', ['tough loss guy joseph put good fight', 'one best hard fought game long time', 'thank support jame'], True), ('W', ['amen', 'thank father'], True), ('M', ['ask hi favorit cuban food', 'racist'], False), ('M', [\"dude you'r welcom\"], True), ('W', ['great', 'final shameless longer stigma woman honor qualiti'], True), ('W', ['good morn l welcom back'], True), ('W', ['debbi need help get improv mass transit', 'individu citi abl opt like rochest poor man would walk 21 mile work'], True), ('W', ['believ jodi veri game mean game', 'make juan martinez get hi goal game make hime danc like puppet'], True), ('W', [\"it' mani time get knock it' mani time get back jack dempsi\"], True), ('W', ['well stop hit peopl', 'probabl dont know'], True), ('M', ['good congressman congressman draw ferguson', 'brother ecstat'], True), ('M', ['donkey punch right pussi'], False), ('W', ['kinda gross lol'], False), ('M', ['may polit also inspir hi exampl rest profession diplomat corp'], True), ('W', ['love love love show great articl'], True), ('M', ['thank', \"i'm inspir\"], True), ('M', ['rep steve stiver find chemtrail', '', 'pleas stop haarp agenda 21', 'pleas step right thing'], False), ('W', ['one throw away life give enhanc rest us'], True), ('M', ['problem', 'merri christma'], True), ('M', ['fuck love thi guy', 'thi man ted'], True), ('M', ['two great represent oregon duck senat wyden'], True), ('M', [\"i'm never away littl busi real life\", ''], True), ('M', ['thank put us first mike', 'appreci'], True), ('M', ['invit pleas uganda africa lol'], True), ('M', ['trump 2016', 'islam need ban u cult preach hate bigotri murder rape incest bestial', 'room toler countri'], False), ('M', ['cannot believ pull thi talk total', 'worri second'], True), ('W', ['noth wrong leg day', 'upper bodi day get easier probabl happen becaus awhil normal leg'], True), ('W', ['great content', \"i'm seek support panel relat matter sxsw interact march 2011\", \"you'r interest place vote http panelpick sxsw com idea view 7068\", 'onli 200 2 000 panel get choos', 'winner base vote', 'thank'], True), ('M', ['could onli stop liber'], False), ('W', [\"thi present product best brightest ted claim human burn last it' veri short wick\", \"would like see sign paycheck she' obvious cater big corpor sponsorship\", 'shame'], False), ('M', ['live perth western australia copi arriv today', \"love surpris come home work can't wait start read train way work tomorrow\"], True), ('M', ['thank thank follow back'], True), ('W', ['two block away', 'fall pick torch'], True), ('W', [\"can't wait day photo op say i'm someth realiti i'm\"], False), ('M', ['thi crazi', 'parent allow thi', 'pay tuition boy safe place', 'real world'], False), ('M', ['thank follow back', 'dig profil pic', ''], True), ('M', ['pray nation jesu name'], True), ('M', [\"thank buddi i'm readi la\", \"can't wait\"], True), ('M', ['hire us hit 8 year high job open hit 13 year high', 'thank obama', 'http bit ly 1cszjrf share'], True), ('W', ['', 'go site maxbeechcr com'], True), ('W', ['congrat huge thank congresswoman water tireless effort', 'appreic behalf constitu abroad'], True), ('W', ['nice politician news deceit corrupt', \"dress husband' grave memori day one hero\"], True)]\n"
    }
   ],
   "source": [
    "preprocess_corpus(os.path.join(data_path, \"train.csv\"), os.path.join(output_path))\n",
    "\n",
    "preprocess_corpus(os.path.join(data_path, \"test.csv\"), os.path.join(output_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Exploration des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Complétez les fonctions retournant les informations suivantes (une fonction par information, chaque fonction prenant en argument un corpus composé d'une liste de phrases segmentées en tokens(tokenization)) ou une liste de genres et une liste de sentiments:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### a. Le nombre total de tokens (mots non distincts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def total_tokens(corpus: List[List[str]]) -> int:\n",
    "    total = 0\n",
    "    for sentence in corpus:\n",
    "        total += len(sentence)\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = [[\"Thanks\", \"I\", \"also\", \"love\", \"bacon\"], \n",
    "        [\"also\", \"why\", \"so\", \"beautiful\", \"louisa\", \"chirico\"], \n",
    "        [\"Trump\", \"got\", \"D+\", \"you\", \"mean\", \"you\"]]\n",
    "print(total_tokens(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### b. Le nombre total de types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def total_types(corpus: List[List[str]]) -> int:\n",
    "    types = []\n",
    "    for sentence in corpus:\n",
    "        for word in sentence:\n",
    "            types.append(word)\n",
    "    return len(set(types))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = [[\"Thanks\", \"I\", \"also\", \"love\", \"bacon\"], \n",
    "        [\"also\", \"why\", \"so\", \"beautiful\", \"louisa\", \"chirico\"], \n",
    "        [\"Trump\", \"got\", \"D+\", \"you\", \"mean\", \"you\"]]\n",
    "print(total_types(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### c. Le nombre total de phrases avec négation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def total_neg(corpus: List[List[str]]) -> int:\n",
    "    total = 0\n",
    "    for sentence in corpus:\n",
    "        for word in sentence:\n",
    "            if \"_NEG\" in word:\n",
    "                total += 1\n",
    "                break\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = [[\"Thanks\", \"I\", \"also\", \"love\", \"bacon\"], \n",
    "        [\"also\", \"why\", \"so\", \"beautiful\", \"louisa\", \"chirico\"], \n",
    "        [\"Trump\", \"got\", \"D+\", \"you\", \"did_NEG\", \"mean\", \"you\"]]\n",
    "print(total_neg(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### d. Le ratio token/type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def TTR(corpus: List[List[str]]) -> int:\n",
    "    return total_tokens(corpus) / total_types(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = [[\"Thanks\", \"I\", \"also\", \"love\", \"bacon\"], \n",
    "        [\"also\", \"why\", \"so\", \"beautiful\", \"louisa\", \"chirico\"], \n",
    "        [\"Trump\", \"got\", \"D+\", \"you\", \"mean\", \"you\"]]\n",
    "print(TTR(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### e. Le nombre total de lemmes distincts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distinct_number(data: List[str]) -> int:\n",
    "\n",
    "    unique_list = []\n",
    "    for doc in data:\n",
    "        for sentence in doc[1]: \n",
    "            for word in sentence.split():\n",
    "                if word not in unique_list: \n",
    "                    unique_list.append(word) \n",
    "\n",
    "    return len(unique_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "17"
     },
     "metadata": {}
    }
   ],
   "source": [
    "def get_lem_count(corpus: List[object]) -> int:\n",
    "    #call function Lemmatisez\n",
    "    return get_distinct_number(corpus)\n",
    "\n",
    "display(get_lem_count(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### f. Le nombre total de racines (stems) distinctes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "17"
     },
     "metadata": {}
    }
   ],
   "source": [
    "def get_stem_count(corpus: List[object]) -> int:\n",
    "    #call function stemmiser\n",
    "    return get_distinct_number(corpus)\n",
    "\n",
    "display(get_stem_count(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### g. Le nombre total de documents (par classe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "{'N': 2, 'P': 2}"
     },
     "metadata": {}
    }
   ],
   "source": [
    "def get_documents_count(corpus: List[object]) -> object:\n",
    "    result = {}\n",
    "    for doc in corpus: \n",
    "        sentiment = doc[2]\n",
    "        if sentiment in result:\n",
    "            result[sentiment] += 1\n",
    "        else:\n",
    "            result.setdefault(sentiment, 1)\n",
    "\n",
    "    return result\n",
    "\n",
    "display(get_documents_count(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### h. Le nombre total de phrases (par classe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "{'N': 4, 'P': 3}"
     },
     "metadata": {}
    }
   ],
   "source": [
    "def get_sentences_count(corpus: List[object]) -> object:\n",
    "    result = {}\n",
    "    for doc in corpus: \n",
    "        sentiment = doc[2]\n",
    "        sentences = doc[1]\n",
    "        if sentiment in result:\n",
    "            result[sentiment] += len(sentences)\n",
    "        else:\n",
    "            result.setdefault(sentiment, len(sentences))\n",
    "\n",
    "    return result\n",
    "\n",
    "display(get_sentences_count(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### i. Le nombre total de phrases avec négation (par classe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "{'N': 2, 'P': 1}"
     },
     "metadata": {}
    }
   ],
   "source": [
    "def get_sentences_neg_count(corpus: List[object]) -> object:\n",
    "    result = {}\n",
    "    for doc in corpus: \n",
    "        sentiment = doc[2]\n",
    "        sentences_neg = [sentence for sentence in doc[1] if sentence.find(\"NEG\") >= 0]\n",
    "        if sentiment in result:\n",
    "            result[sentiment] += len(sentences_neg)\n",
    "        else:\n",
    "            result.setdefault(sentiment, len(sentences_neg))\n",
    "\n",
    "    return result\n",
    "    \n",
    "display(get_sentences_neg_count(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### j. Le pourcentage de réponses positives par genre de la personne à qui cette réponse est faite (op_gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "{'M': '50.0%', 'W': '50.0%'}"
     },
     "metadata": {}
    }
   ],
   "source": [
    "def get_positive_answers(corpus: List[object]) -> object:\n",
    "    result = {}\n",
    "    for doc in corpus: \n",
    "        sentiment = doc[2]\n",
    "        genre = doc[0]\n",
    "        if sentiment == \"P\" :\n",
    "            if genre in result:\n",
    "                result[genre] += 1\n",
    "            else:\n",
    "                result.setdefault(genre, 1)\n",
    "\n",
    "    response = {}\n",
    "    response[\"M\"] = str((result[\"M\"] / len(result)) * 100) + \"%\"\n",
    "    response[\"W\"] = str((result[\"W\"] / len(result)) * 100) + \"%\"\n",
    "    return response\n",
    "\n",
    "display(get_positive_answers(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 2) Écrivez la fonction explore(corpus, sentiments, genders) qui calcule et affiche toutes ces informations, précédées d'une légende reprenant l’énoncé de chaque question (a,b, ….j)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "17"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "17"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "{'N': 2, 'P': 2}"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "{'N': 4, 'P': 3}"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "{'N': 2, 'P': 1}"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "{'M': '50.0%', 'W': '50.0%'}"
     },
     "metadata": {}
    }
   ],
   "source": [
    "def explore(\n",
    "    corpus: List[object]\n",
    ") -> None:\n",
    "    #display(total_tokens(corpus))\n",
    "    #display(total_types(corpus))\n",
    "    #display(total_neg(corpus))\n",
    "    #display(TTR(corpus))\n",
    "\n",
    "    display(get_lem_count(corpus))\n",
    "    display(get_stem_count(corpus))\n",
    "    display(get_documents_count(corpus))\n",
    "    display(get_sentences_count(corpus))\n",
    "    display(get_sentences_neg_count(corpus))\n",
    "    display(get_positive_answers(corpus))\n",
    "\n",
    "explore(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 3) Calculer une table de fréquence (lemme, rang (le mot le plus fréquent a le rang 1 etc.) ; fréquence (le nombre de fois où il a été vu dans le corpus).  Seuls les N mots les plus fréquents du vocabulaire (N est un paramètre) doivent être gardés. Vous devez stocker les 1000 premières lignes de cette table dans un fichier nommé table_freq.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 2. Classification automatique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### a) Classification  automatique avec un modèle sac de mots (unigrammes), Naive Bayes et la régression logistique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "En utilisant la librairie scikitLearn et l’algorithme Multinomial Naive Bayes et Logistic Regression, effectuez la classification des textes avec un modèle sac de mots unigramme pondéré avec TF-IDF.  Vous devez entrainer chaque modèle sur l’ensemble d’entrainement et le construire à partir de votre fichier corpus_train.csv. \n",
    "\n",
    "Construisez et sauvegardez votre modèle sac de mots avec les données d’entrainement en testant les pré-traitements suivants (séparément et en combinaison): tokenisation, lemmatisation, stemming, normalisation des négations, et suppression des mots outils. Vous ne devez garder que la combinaison d’opérations qui vous donne les meilleures performances sur le corpus de test. Indiquez dans un commentaire les pré-traitements qui vous amènent à votre meilleure performance (voir la section 3 – évaluation). Il est possible que la combinaison optimale ne soit pas la même selon que vous utilisiez la régression logistique ou Naive Bayes. On s’attend à avoir deux modèles optimaux, un pour Naive Bayes, et un avec régression logistique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sac de mots + TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer #CountVectorizer, TfidfTransformer\n",
    "\n",
    "# create combinations of pre-processing methods here\n",
    "#pre_processing_ops = [\"normalisation_neg\" , \"tokenisation\" , \"lemmatisation\" , \"stemming\" , \"suppression_stopwords\"]\n",
    "\n",
    "train_data = read_data(os.path.join(data_path, \"train.csv\"))\n",
    "test_data = read_data(os.path.join(data_path, \"test.csv\"))\n",
    "\n",
    "train_data = ([text.lower() for text in train_data[0]], train_data[2], train_data[1])\n",
    "test_data = ([text.lower() for text in test_data[0]], test_data[2], test_data[1])\n",
    "\n",
    "X_train, y_train, X_test, y_test, X_train_tfidf = [], [], [], [], []\n",
    "\n",
    "# apply combinations of pre-processing methods here\n",
    "for i, pre_process_set in enumerate(combinations):\n",
    "#     processed_train_data, processed_test_data = pre_process_combination(pre_process_set, train_data, test_data)\n",
    "\n",
    "    X_train[i], y_train[i] = processed_train_data[0:2], processed_train_data[2]\n",
    "    X_test[i], y_test[i] = processed_test_data[0:2], processed_test_data[2]\n",
    "\n",
    "    X_train_tfidf[i] = TfidfVectorizer().fit_transform(X_train[i]).toarray()\n",
    "    print(X_train_tfidf[i].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "y_pred = []\n",
    "for i in len(combinations):\n",
    "    model = MultinomialNB()\n",
    "    model.fit(X_train_tfidf[i], y_train[i])\n",
    "\n",
    "    y_pred[i] = classifier.predict(X_test_tfidf[i]) # or X_test ?\n",
    "    # accuracy: np.mean(y_pred[i] == y_test[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Régression Logistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "    \n",
    "for i in len(combinations):\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train_tfidf[i], y_train[i])\n",
    "\n",
    "    y_pred[i] = classifier.predict(X_test_tfidf[i]) # or X_test ?\n",
    "    # accuracy: np.mean(y_pred[i] == y_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save/load a model\n",
    "\n",
    "with open('text_classifier', 'wb') as picklefile:\n",
    "    pickle.dump(classifier, picklefile)\n",
    "    \n",
    "with open('text_classifier', 'rb') as training_model:\n",
    "    model = pickle.load(training_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "###  b) Autre représentation pour l’analyse de sentiments et classification automatique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "On vous propose maintenant d’utiliser une nouvelle représentation de chaque document à classifier.\n",
    "Vous devez créer à partir de votre corpus la table suivante :\n",
    "\n",
    "| Vocabulaire | Freq-positive | Freq-négative |\n",
    "|-------------|---------------|---------------|\n",
    "| happy | 10 | 1 |\n",
    "| ... | ... | ... |\n",
    "\n",
    "Où :\n",
    "\n",
    "• Vocabulaire représente tous les types (mots uniques) de votre corpus d’entrainement\n",
    "\n",
    "• Freq-positive : représente la somme des fréquences du mot dans tous les documents de la classe positive\n",
    "\n",
    "• Freq-négative : représente la somme des fréquences du mot dans tous les documents de la classe négative\n",
    "\n",
    "Notez qu’en Python, vous pouvez créer un dictionnaire associant à tout (mot, classe) une fréquence.\n",
    "Ensuite il vous suffit de représenter chaque document par un vecteur à 3 dimensions dont le premier élément représente un biais (initialisé à 1), le deuxième élément représente la somme des fréquences positives (freq-pos) de tous les mots uniques (types) du document et enfin le troisième élément représente la somme des fréquences négative (freq-neg) de tous les mots uniques du document. \n",
    "\n",
    "En utilisant cette représentation ainsi que les pré-traitements suggérés, trouvez le meilleur modèle possible en testant la régression logistique et Naive Bayes. Vous ne devez fournir que le code de votre meilleur modèle dans votre notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "[('I', 0, 1),\n ('am', 0, 1),\n ('home', 1, 1),\n ('NEG', 1, 2),\n ('You', 0, 2),\n ('are', 0, 2),\n ('late', 0, 1),\n ('He', 0, 1),\n ('is', 1, 1),\n ('fine', 0, 1),\n (\"let's\", 1, 0),\n ('go', 1, 0),\n ('Im', 1, 0),\n ('happy', 1, 0),\n ('alone', 0, 1),\n ('Life', 1, 0),\n ('good', 1, 0)]"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "[(1, 4, 15), (2, 6, 3), (3, 0, 5), (4, 3, 1)]"
     },
     "metadata": {}
    }
   ],
   "source": [
    "def calculFrequences(corpus: List[object]) -> object:\n",
    "    dic = {}\n",
    "    for doc in corpus: \n",
    "        sentiment = doc[2]\n",
    "        for sentence in doc[1]: \n",
    "            for word in sentence.split():\n",
    "                if word in dic:\n",
    "                    dic[word][sentiment] += 1\n",
    "                else:\n",
    "                    dic_sent = {'P': 0, 'N': 0}\n",
    "                    dic_sent[sentiment]  = 1\n",
    "                    dic.setdefault(word, dic_sent)\n",
    "    return dic\n",
    "\n",
    "def representData1(freq: object) -> object:\n",
    "    result = []\n",
    "    for word in freq:\n",
    "        result.append((word, freq[word][\"P\"], freq[word][\"N\"]))\n",
    "        \n",
    "    return result\n",
    "\n",
    "def representData2(freq: object, corpus: List[object]) -> object:\n",
    "    result = []\n",
    "    doc_index = 0\n",
    "    for doc in corpus: \n",
    "        pos_freq = 0\n",
    "        neg_freg = 0\n",
    "        doc_index += 1\n",
    "        for sentence in doc[1]: \n",
    "            for word in sentence.split():\n",
    "                pos_freq += freq[word][\"P\"]\n",
    "                neg_freg += freq[word][\"N\"]\n",
    "        result.append((doc_index, pos_freq, neg_freg))\n",
    "\n",
    "    return result\n",
    "\n",
    "freq = calculFrequences(corpus)\n",
    "display(representData1(freq))\n",
    "display(representData2(freq, corpus))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 3. Évaluation et discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### a) Pour déterminer la performance de vos modèles, vous devez tester vos modèles de classification sur l’ensemble de test et générer vos résultats pour chaque modèle dans une table avec les métriques suivantes : Accuracy et pour chaque classe, la précision, le rappel et le F1 score. On doit voir cette table générée dans votre notebook avec la liste de vos modèles de la section 2 et leurs performances respectives. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "for i in len(combinations):\n",
    "    print(classification_report(y_test[i], y_pred[i], target_names=[\"P\", \"N\"]))\n",
    "    print(confusion_matrix(y_test[i], y_pred[i]))\n",
    "    print(accuracy_score(y_test[i], y_pred[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### b) Générez un graphique qui représente la performance moyenne (mean accuracy – 10 Fold cross-validation) de vos différents modèles par tranches de 500 textes sur l’ensemble d’entrainement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### c) Que se passe-t-il lorsque le paramètre de régularisation de la régression logisque (C) est augmenté ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analyse et discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) En considérant les deux types de représentations, répondez aux question suivantes en reportant la question dans le notebook et en inscrivant votre réponse:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Quel est l’impact de l’annotation de la négation ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) La suppression des stopwords est-elle une bonne idée pour l’analyse de sentiments ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d) Le stemming et/ou la lemmatisation sont-ils souhaitables dans le cadre de l’analyse de sentiments ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Contribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complétez la section en haut du notebook indiquant la contribution de chaque membre de l’équipe en indiquant ce qui a été effectué par chaque membre et le pourcentage d’effort du membre dans le TP. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}