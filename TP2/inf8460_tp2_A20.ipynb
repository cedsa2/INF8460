{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "inf8460_tp2_A20 - 3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYaltRsowqfU"
      },
      "source": [
        "## École Polytechnique de Montréal\n",
        "## Département Génie Informatique et Génie Logiciel\n",
        "\n",
        "## INF8460 – Traitement automatique de la langue naturelle - TP2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92idNN-XwvMP"
      },
      "source": [
        "## Objectifs d'apprentissage: \n",
        "\n",
        "•\tExplorer les modèles d’espace vectoriel (vector space models) comme représentations distribuées de la sémantique des mots \n",
        "•\tImplémenter la fréquence de co-occurrence et la PPMI\n",
        "•\tComprendre différentes mesures de distance entre vecteurs de mots \n",
        "•\tExplorer l’intérêt de la réduction de dimensionnalité \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEoOfK-Rw3zT"
      },
      "source": [
        "## Équipe et contributions \n",
        "Veuillez indiquer la contribution effective de chaque membre de l'équipe en pourcentage et en indiquant les modules ou questions sur lesquelles chaque membre a travaillé"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0mJOdtlw4xC"
      },
      "source": [
        "Equipe 8:\n",
        "\n",
        "Cedric Sadeu (1869737): 1/3\n",
        "\n",
        "Mamoudou Sacko (1924187): 1/3\n",
        "\n",
        "Oumayma Messoussi (2016797): 1/3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qu3pnt6mPiNZ"
      },
      "source": [
        "\n",
        "\n",
        "## Support de google Colab\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-U9ANA6ZV27",
        "outputId": "eb3bf187-4d38-46bd-a05e-e18e3712346d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!wget https://staff.fnwi.uva.nl/e.bruni/resources/MEN.tar.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-07 01:13:15--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
            "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84125825 (80M) [application/x-gzip]\n",
            "Saving to: ‘aclImdb_v1.tar.gz’\n",
            "\n",
            "aclImdb_v1.tar.gz   100%[===================>]  80.23M  12.6MB/s    in 7.7s    \n",
            "\n",
            "2020-10-07 01:13:22 (10.4 MB/s) - ‘aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
            "\n",
            "--2020-10-07 01:13:23--  https://staff.fnwi.uva.nl/e.bruni/resources/MEN.tar.gz\n",
            "Resolving staff.fnwi.uva.nl (staff.fnwi.uva.nl)... 146.50.61.62\n",
            "Connecting to staff.fnwi.uva.nl (staff.fnwi.uva.nl)|146.50.61.62|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 90373 (88K) [application/x-gzip]\n",
            "Saving to: ‘MEN.tar.gz’\n",
            "\n",
            "MEN.tar.gz          100%[===================>]  88.25K   239KB/s    in 0.4s    \n",
            "\n",
            "2020-10-07 01:13:24 (239 KB/s) - ‘MEN.tar.gz’ saved [90373/90373]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVrz-1_uZApz"
      },
      "source": [
        "! tar -xzf aclImdb_v1.tar.gz\n",
        "! tar -xzf MEN.tar.gz\n",
        "! mkdir -p vsm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TewW7eDjPbjs"
      },
      "source": [
        "## Librairies externes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rK102dx5Poti",
        "outputId": "0857de7f-6026-4504-d011-64bcda07be83",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kClu3RY7YFfv"
      },
      "source": [
        "from collections import Counter, defaultdict\n",
        "from itertools import chain\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import itertools\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import glob\n",
        "import re\n",
        "import math\n",
        "import time\n",
        "\n",
        "from nltk.corpus import stopwords as all_stopwords\n",
        "from nltk import word_tokenize\n",
        "from nltk.util import ngrams\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "from scipy.stats import spearmanr\n",
        "from scipy.spatial.distance import euclidean, cosine\n",
        "from IPython.display import display\n",
        "from sklearn.decomposition import TruncatedSVD"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oRJolmeRlxW"
      },
      "source": [
        "## Valeurs globales"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqfHWnRoRlFY"
      },
      "source": [
        "DIRNAME_ACL =  os.path.join(os.getcwd(), \"aclImdb/\")\n",
        "DIRNAME_MEN =  os.path.join(os.getcwd(), \"MEN/\")\n",
        "DIRNAME_VSM =  os.path.join(os.getcwd(), \"vsm\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "hGQ15KsihAOp"
      },
      "source": [
        "## 1. Prétraitement (20 points)\n",
        "\n",
        "**a)**\tLe jeu de données est séparé en deux répertoires `train/`et `test`, chacun contenant eux-mêmes deux sous-répertoires `pos/` et `neg/` pour les revues positives et négatives. Un fichier `readme` décrit plus précisément les données. Commencez par lire ces données, en gardant séparées les données d'entraînement et de test. La fonction doit mettre les mots en minuscules,  supprimer les stopwords (vous devez utiliser ceux de NLTK) et afficher le nombre total de phrases d’entrainement,  le nombre total de phrases d’entrainement positives et négatives et le nombre total de phrases de test avec le nombre total de phrases de test positives et négatives ;"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "5Lk9pYtUKWvf"
      },
      "source": [
        "train_data = {'id': [], 'text': [], 'rating': [], 'sentiment': []}\n",
        "test_data = {'id': [], 'text': [], 'rating': [], 'sentiment': []}\n",
        "\n",
        "for name in ['train/pos', 'train/neg']:\n",
        "    sentiment = name.split('/')[1]\n",
        "    for file in glob.glob(DIRNAME_ACL+name+\"/*.txt\"):\n",
        "        review_id = int(file.split(\"/\")[-1].split(\".\")[0].split(\"_\")[0])\n",
        "        rating = int(file.split(\"/\")[-1].split(\".\")[0].split(\"_\")[1]) / 10\n",
        "        f = open(file, encoding=\"utf8\")\n",
        "        content = f.read()\n",
        "        \n",
        "        train_data['id'].append(review_id)\n",
        "        train_data['text'].append(content)\n",
        "        train_data['rating'].append(rating)\n",
        "        train_data['sentiment'].append(sentiment)\n",
        "                \n",
        "for name in ['test/pos', 'test/neg']:\n",
        "    sentiment = name.split('/')[1]\n",
        "    for file in glob.glob(DIRNAME_ACL+name+\"/*.txt\"):\n",
        "        review_id = int(file.split(\"/\")[-1].split(\".\")[0].split(\"_\")[0])\n",
        "        rating = int(file.split(\"/\")[-1].split(\".\")[0].split(\"_\")[1]) / 10\n",
        "        f = open(file, encoding=\"utf8\")\n",
        "        content = f.read()\n",
        "        \n",
        "        test_data['id'].append(review_id)\n",
        "        test_data['text'].append(content)\n",
        "        test_data['rating'].append(rating)\n",
        "        test_data['sentiment'].append(sentiment)\n",
        "\n",
        "train_df = pd.DataFrame (train_data, columns = ['id', 'text', 'rating', 'sentiment'])\n",
        "test_df = pd.DataFrame (test_data, columns = ['id', 'text', 'rating', 'sentiment'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a24NJt1fqPvX",
        "outputId": "f1edab28-be35-402b-c778-5d20dc5c062f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        }
      },
      "source": [
        "print(train_df.shape)\n",
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(25000, 4)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>rating</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4630</td>\n",
              "      <td>This is the second Eytan Fox film I have seen....</td>\n",
              "      <td>0.9</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10964</td>\n",
              "      <td>When it first came out, this work by the Meyse...</td>\n",
              "      <td>0.7</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4696</td>\n",
              "      <td>even though this movie is quite old, no matter...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4807</td>\n",
              "      <td>I really thought that this movie was superb. N...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5258</td>\n",
              "      <td>The Ladies Man is laugh out loud funny, with a...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      id                                               text  rating sentiment\n",
              "0   4630  This is the second Eytan Fox film I have seen....     0.9       pos\n",
              "1  10964  When it first came out, this work by the Meyse...     0.7       pos\n",
              "2   4696  even though this movie is quite old, no matter...     1.0       pos\n",
              "3   4807  I really thought that this movie was superb. N...     1.0       pos\n",
              "4   5258  The Ladies Man is laugh out loud funny, with a...     1.0       pos"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXhEHXM0qPvd",
        "outputId": "7b41e9f6-1c49-4ffc-f7ff-fcb6303b94a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        }
      },
      "source": [
        "print(test_df.shape)\n",
        "test_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(25000, 4)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>rating</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5739</td>\n",
              "      <td>Paul Naschy as a ghostly security guard in thi...</td>\n",
              "      <td>0.7</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10964</td>\n",
              "      <td>In many ways, the filmic career of independent...</td>\n",
              "      <td>0.7</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4807</td>\n",
              "      <td>Having seen 'only' about 200 Hong Kong films i...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7259</td>\n",
              "      <td>This movie almost has everything. The action i...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>11405</td>\n",
              "      <td>All you need is great house, a babysitter and ...</td>\n",
              "      <td>0.8</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      id                                               text  rating sentiment\n",
              "0   5739  Paul Naschy as a ghostly security guard in thi...     0.7       pos\n",
              "1  10964  In many ways, the filmic career of independent...     0.7       pos\n",
              "2   4807  Having seen 'only' about 200 Hong Kong films i...     1.0       pos\n",
              "3   7259  This movie almost has everything. The action i...     1.0       pos\n",
              "4  11405  All you need is great house, a babysitter and ...     0.8       pos"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QkY2X64qPvk"
      },
      "source": [
        "def preprocess(review):\n",
        "    soup = BeautifulSoup(review, \"html.parser\")\n",
        "    text = soup.get_text()\n",
        "    stop_words = set(all_stopwords.words('english'))\n",
        "    \n",
        "    tokens = nltk.word_tokenize(text.lower())\n",
        "    return [w for w in tokens if w.isalpha() and not w in stop_words] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lboz72YyqPvo",
        "outputId": "b11f4367-d6bb-42f3-ae18-3534b79a3cfe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "start = time.time()\n",
        "train_df['text'] = train_df['text'].apply(preprocess)\n",
        "print(time.time() - start)\n",
        "\n",
        "test_df['text'] = test_df['text'].apply(preprocess)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "54.52512502670288\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSJ0xg41qPvs",
        "outputId": "7d76f8de-f7d0-4ec7-e297-216a7687b458",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        }
      },
      "source": [
        "print(train_df.shape)\n",
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(25000, 4)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>rating</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4630</td>\n",
              "      <td>[second, eytan, fox, film, seen, fantastic, ac...</td>\n",
              "      <td>0.9</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10964</td>\n",
              "      <td>[first, came, work, meysels, brothers, much, c...</td>\n",
              "      <td>0.7</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4696</td>\n",
              "      <td>[even, though, movie, quite, old, matter, many...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4807</td>\n",
              "      <td>[really, thought, movie, superb, history, corr...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5258</td>\n",
              "      <td>[ladies, man, laugh, loud, funny, great, diver...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      id                                               text  rating sentiment\n",
              "0   4630  [second, eytan, fox, film, seen, fantastic, ac...     0.9       pos\n",
              "1  10964  [first, came, work, meysels, brothers, much, c...     0.7       pos\n",
              "2   4696  [even, though, movie, quite, old, matter, many...     1.0       pos\n",
              "3   4807  [really, thought, movie, superb, history, corr...     1.0       pos\n",
              "4   5258  [ladies, man, laugh, loud, funny, great, diver...     1.0       pos"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GyWSOQbqPvv",
        "outputId": "02cc3678-71e6-464e-cb08-628e9aa4c738",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        }
      },
      "source": [
        "print(test_df.shape)\n",
        "test_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(25000, 4)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>rating</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5739</td>\n",
              "      <td>[paul, naschy, ghostly, security, guard, scari...</td>\n",
              "      <td>0.7</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10964</td>\n",
              "      <td>[many, ways, filmic, career, independent, lege...</td>\n",
              "      <td>0.7</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4807</td>\n",
              "      <td>[seen, hong, kong, films, time, say, film, amo...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7259</td>\n",
              "      <td>[movie, almost, everything, action, cool, funn...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>11405</td>\n",
              "      <td>[need, great, house, babysitter, phone, simon,...</td>\n",
              "      <td>0.8</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      id                                               text  rating sentiment\n",
              "0   5739  [paul, naschy, ghostly, security, guard, scari...     0.7       pos\n",
              "1  10964  [many, ways, filmic, career, independent, lege...     0.7       pos\n",
              "2   4807  [seen, hong, kong, films, time, say, film, amo...     1.0       pos\n",
              "3   7259  [movie, almost, everything, action, cool, funn...     1.0       pos\n",
              "4  11405  [need, great, house, babysitter, phone, simon,...     0.8       pos"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkdaAuybqPv0",
        "outputId": "78f15b6b-23ce-4932-bd80-f28bf952ed11",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"\"\"TRAIN\\n\\t - le nombre total de phrases d’entrainement: %d\n",
        "              \\n\\t - le nombre total de phrases d’entrainement positives: %d\n",
        "              \\n\\t - le nombre total de phrases d’entrainement négatives: %d\\n\"\"\" % \n",
        "      (train_df.shape[0], train_df[train_df[\"sentiment\"] == \"pos\"].shape[0], \n",
        "       train_df[train_df[\"sentiment\"] == \"neg\"].shape[0]))\n",
        "\n",
        "print(\"\"\"TEST\\n\\t - le nombre total de phrases de test: %d\n",
        "              \\n\\t - le nombre total de phrases de test positives: %d\n",
        "              \\n\\t - le nombre total de phrases de test négatives: %d\\n\"\"\" % \n",
        "      (test_df.shape[0], test_df[test_df[\"sentiment\"] == \"pos\"].shape[0], \n",
        "       test_df[test_df[\"sentiment\"] == \"neg\"].shape[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAIN\n",
            "\t - le nombre total de phrases d’entrainement: 25000\n",
            "              \n",
            "\t - le nombre total de phrases d’entrainement positives: 12500\n",
            "              \n",
            "\t - le nombre total de phrases d’entrainement négatives: 12500\n",
            "\n",
            "TEST\n",
            "\t - le nombre total de phrases de test: 25000\n",
            "              \n",
            "\t - le nombre total de phrases de test positives: 12500\n",
            "              \n",
            "\t - le nombre total de phrases de test négatives: 12500\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "_tVXpVcSh9eY"
      },
      "source": [
        "**b)**\tCréez la fonction `build_voc()` qui extrait les unigrammes de l’ensemble d’entraînement et conserve ceux qui ont une fréquence d’occurrence d'au moins 5 et imprime le nombre de mots dans le vocabulaire. Sauvegardez-le dans un fichier `vocab.txt` (un mot par ligne) dans le répertoire aclImdb."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DlUsutVGbvj"
      },
      "source": [
        "def build_voc(corpus, unk_cutoff=5):\n",
        "    unigrams = defaultdict(int)\n",
        "    for review in corpus:\n",
        "        for word in review:\n",
        "            unigrams[word] += 1\n",
        "\n",
        "    print(\"Unigrammes: \" + str(len(unigrams)))\n",
        "\n",
        "    unigrams = {k: v for k, v in sorted(unigrams.items(), key=lambda item: item[1], reverse=True) if v >= 5}\n",
        "\n",
        "    print(\"Taille du vocab (apres filtrage freq > 5): \" + str(len(unigrams)))\n",
        "\n",
        "    f = open('vocab.txt', 'w')\n",
        "    for w in unigrams.keys():\n",
        "        f.write(w + '\\n')\n",
        "\n",
        "    return unigrams"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBdQirmwqPwA",
        "outputId": "3bd74e5f-a8c8-4c0e-9b7b-40acc4a404d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "start = time.time()\n",
        "train_unigrams = build_voc(train_df['text'])\n",
        "print(time.time() - start)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unigrammes: 72029\n",
            "Taille du vocab (apres filtrage freq > 5): 27760\n",
            "0.6960606575012207\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "oEIUW6EpnvQV"
      },
      "source": [
        "## 2. Matrices de co-occurence (30 points)\n",
        "\n",
        "Pour les matrices de cette section, vous pourrez utiliser [des array `numpy`](https://docs.scipy.org/doc/numpy/reference/arrays.ndarray.html) ou des DataFrame [`pandas`](https://pandas.pydata.org/pandas-docs/stable/). \n",
        "\n",
        "Ressources utiles :  le [*quickstart tutorial*](https://numpy.org/devdocs/user/quickstart.html) de numpy et le guide [10 minutes to pandas](https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "fadNCxBcnyeT"
      },
      "source": [
        "**a)** A partir des textes du corpus d’entrainement (neg/pos), vous devez construire une matrice de co-occurrence mot × mot M(w,w) qui contient les 5000 unigrammes les plus fréquents sous forme de **cadre panda**. Le contexte de co-occurrence est une fenêtre de +/-5 mots autour du mot cible. Le poids est la fréquence de co-occurrence simple. Sauvegardez votre matrice dans un fichier tp2_mat5.csv dans le répertoire vsm.\n",
        "\n",
        "Attention, le mot lui même de doit pas être compté dans sa co-occurence. Exemple : \n",
        "Corpus: [ \"I go to school every day by bus\", \"i go to theatre every night by bus\"]\n",
        "\n",
        "Co-occurence(\"every\", fenetre=2) = [ (to, 2), (by, 2), (school, 1), (day, 1), (theatre, 1), (night, 1), (bus, 0), (every, 0), (go, 0). (i,0) ]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGvv5VKK26kw",
        "outputId": "ce258ab3-f5ce-46da-cfb1-7d1db03fab74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        }
      },
      "source": [
        "train_top_5000 = list(train_unigrams.keys())[:5000] # or read first 5000 lines from vocab.txt\n",
        "train_5000_df = pd.DataFrame(train_top_5000, columns=['unigram'])\n",
        "train_5000_df.shape\n",
        "train_5000_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>unigram</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>movie</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>film</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>one</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>like</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>good</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  unigram\n",
              "0   movie\n",
              "1    film\n",
              "2     one\n",
              "3    like\n",
              "4    good"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yTUiZHm6hjQ"
      },
      "source": [
        "def matrix_creator(corpus, vocab, n, scaled) :\n",
        "    M = {}\n",
        "    unigrams = []\n",
        "\n",
        "    for review in corpus :\n",
        "        for word in review :\n",
        "            if word not in M :\n",
        "                M[word] = {}\n",
        "            unigrams.append(word)\n",
        "\n",
        "        for i, word in enumerate(review) :\n",
        "            window = [i+k for k in range(-n,n+1,1) if i+k != i and i+k in range(len(review))]\n",
        "            voisinage = M[word]\n",
        "            for j in window :\n",
        "                if word == review[j]:\n",
        "                    continue\n",
        "                if review[j] not in voisinage :\n",
        "                    voisinage[review[j]] = 0\n",
        "                voisinage[review[j]] += 1 if not scaled else 1 / abs(i - j)\n",
        "\n",
        "    return [(k, M[k]) for k in vocab] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ppKltvBhkUh",
        "outputId": "60e78370-2c2a-44c3-d060-2bc58c93c8e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "start = time.time()\n",
        "result = matrix_creator(train_df['text'], train_top_5000, 5, False)\n",
        "tmp = pd.DataFrame(result)\n",
        "Mww = pd.json_normalize(tmp[1])\n",
        "Mww = Mww[tmp[0]].fillna(0).set_index(tmp[0])\n",
        "Mww = pd.DataFrame(Mww)\n",
        "print(time.time() - start)\n",
        "\n",
        "print(Mww)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "150.1029634475708\n",
            "             movie    film     one    like  ...  region  shoddy  exposition  gender\n",
            "0                                           ...                                    \n",
            "movie          0.0  2204.0  3756.0  3763.0  ...     5.0     8.0         6.0     6.0\n",
            "film        2204.0     0.0  3279.0  2576.0  ...    10.0    16.0        15.0    11.0\n",
            "one         3756.0  3279.0     0.0  1635.0  ...     6.0     6.0         4.0     3.0\n",
            "like        3763.0  2576.0  1635.0     0.0  ...     2.0     7.0         4.0     2.0\n",
            "good        3540.0  2473.0  1331.0   983.0  ...     0.0     4.0         0.0     2.0\n",
            "...            ...     ...     ...     ...  ...     ...     ...         ...     ...\n",
            "tyler          4.0     3.0     4.0     5.0  ...     0.0     0.0         0.0     0.0\n",
            "region         5.0    10.0     6.0     2.0  ...     0.0     0.0         0.0     0.0\n",
            "shoddy         8.0    16.0     6.0     7.0  ...     0.0     0.0         0.0     0.0\n",
            "exposition     6.0    15.0     4.0     4.0  ...     0.0     0.0         0.0     0.0\n",
            "gender         6.0    11.0     3.0     2.0  ...     0.0     0.0         0.0     0.0\n",
            "\n",
            "[5000 rows x 5000 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h33uV4FGAbdn"
      },
      "source": [
        "pd.DataFrame(Mww).to_csv('Mww.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eHuntV5sh5o",
        "outputId": "dd57ba34-3f30-4605-96b3-f8b14d822b07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "start = time.time()\n",
        "result = matrix_creator(train_df['text'], train_top_5000, 5, True)\n",
        "tmp = pd.DataFrame(result)\n",
        "Mww_scaled = pd.json_normalize(tmp[1])\n",
        "Mww_scaled = Mww_scaled[tmp[0]].fillna(0).set_index(tmp[0])\n",
        "Mww_scaled = pd.DataFrame(Mww_scaled)\n",
        "print(time.time() - start)\n",
        "\n",
        "print(Mww_scaled)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "156.1667718887329\n",
            "                  movie         film  ...  exposition    gender\n",
            "0                                     ...                      \n",
            "movie          0.000000   788.200000  ...    2.033333  2.900000\n",
            "film         788.200000     0.000000  ...    6.100000  4.700000\n",
            "one         1660.766667  1448.483333  ...    1.333333  1.666667\n",
            "like        1848.800000  1219.616667  ...    1.450000  0.583333\n",
            "good        1888.966667  1289.850000  ...    0.000000  0.450000\n",
            "...                 ...          ...  ...         ...       ...\n",
            "tyler          1.083333     0.700000  ...    0.000000  0.000000\n",
            "region         2.250000     4.800000  ...    0.000000  0.000000\n",
            "shoddy         2.966667     7.933333  ...    0.000000  0.000000\n",
            "exposition     2.033333     6.100000  ...    0.000000  0.000000\n",
            "gender         2.900000     4.700000  ...    0.000000  0.000000\n",
            "\n",
            "[5000 rows x 5000 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xx4gOyuxBD0o"
      },
      "source": [
        "pd.DataFrame(Mww_scaled).to_csv('Mww_scaled.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "-vzXMLl3rEBl"
      },
      "source": [
        "**b)** Calculez maintenant une matrice de cooccurrence mais en ajustant les fréquences basées sur la proximité du mot cible par exemple en les multipliant par 1/𝑑 où d est la distance en jetons (mots) de la cible. Sauvegardez votre matrice (toujours sous forme de cadre panda) dans un fichier tp2_mat5_scaled.csv dans le répertoire vsm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "az48q-61sxQh"
      },
      "source": [
        "Voir fonction dessus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "osW2EZ5utFsf"
      },
      "source": [
        "**c)**\tVous devez créer une fonction `pmi` qui prend le cadre panda de la matrice $M(w,w)$ et un paramètre boolean flag qui est à True lorsque l'on désire calculer PPMI et à False quand on veut calculer PMI. La fonction transforme la matrice en entrée en une matrice $M’(w,w)$ avec les valeurs PMI ou PPMI selon la valeur du paramètre booléen. La fonction retourne le nouveau cadre panda correspondant. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "lksK36hntUHP"
      },
      "source": [
        "Pour une matrice  $X_{m \\times n}$:\n",
        "\n",
        "\n",
        "$$\\textbf{colsum}(X, j) = \\sum_{i=1}^{m}X_{ij}$$\n",
        "\n",
        "$$\\textbf{sum}(X) = \\sum_{i=1}^{m}\\sum_{j=1}^{n} X_{ij}$$\n",
        "\n",
        "$$\\textbf{expected}(X, i, j) = \n",
        "\\frac{\n",
        "  \\textbf{rowsum}(X, i) \\cdot \\textbf{colsum}(X, j)\n",
        "}{\n",
        "  \\textbf{sum}(X)\n",
        "}$$\n",
        "\n",
        "\n",
        "$$\\textbf{pmi}(X, i, j) = \\log\\left(\\frac{X_{ij}}{\\textbf{expected}(X, i, j)}\\right)$$\n",
        "\n",
        "$$\\textbf{ppmi}(X, i, j) = \n",
        "\\begin{cases}\n",
        "\\textbf{pmi}(X, i, j) & \\textrm{if } \\textbf{pmi}(X, i, j) > 0 \\\\\n",
        "0 & \\textrm{otherwise}\n",
        "\\end{cases}$$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "OgkN4mw6r19a"
      },
      "source": [
        "def mat_list_to_df(mat_result, unigramme_list):\n",
        "    arr = np.array(mat_result)\n",
        "    t_arr = arr.T\n",
        "    arr_list = t_arr.tolist()\n",
        "    dict_result = {}\n",
        "    for i, word in enumerate(unigramme_list):\n",
        "        dict_result[word] = arr_list[i]\n",
        "    df = pd.DataFrame(data = dict_result, index = unigramme_list)\n",
        "    return df\n",
        "\n",
        "def probability_matrix(mat_Result):\n",
        "    total = 0\n",
        "    for line in mat_Result:\n",
        "        total += sum(line)\n",
        "    arr = np.array(mat_Result)\n",
        "    with np.errstate(divide='ignore'):\n",
        "        arr_prob = np.divide(arr, total)\n",
        "    return arr_prob.tolist()\n",
        "\n",
        "def ppmi_pmi(df_Result, is_ppmi):\n",
        "\n",
        "    temp = df_Result.to_numpy()\n",
        "    mat_Result = list(temp.tolist())\n",
        "    mat_prob = probability_matrix(mat_Result)\n",
        "    arr = np.array(mat_prob)\n",
        "    sum_arr_column = arr.sum(axis=0)\n",
        "    sum_arr_line = arr.sum(axis=1)\n",
        "\n",
        "    for i, line in enumerate(mat_prob):\n",
        "        for j, item in enumerate(line):\n",
        "            if item == 0:\n",
        "                mat_prob[i][j] = 0.0\n",
        "            elif is_ppmi:         \n",
        "                mat_prob[i][j] = max(math.log(item / (sum_arr_column[j]*sum_arr_line[i]), 2), 0)\n",
        "            else:\n",
        "                mat_prob[i][j] = math.log(item / (sum_arr_column[j]*sum_arr_line[i]), 2)\n",
        "    \n",
        "    return mat_list_to_df(mat_prob, list(df_Result.columns))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "U_KI11vmty0c"
      },
      "source": [
        "**d)** Créer les matrice PMIs et PPMIs en vous basant sur les deux matrices que vous avez déjà créée\tSauvegardez vos matrices dans un fichier tp2_mat5<\\_scaled>_{pmi|ppmi}.csv toujours dans le répertoire vsm. \n",
        "\n",
        "(votre nom de fichier doit contenir \"_scaled\" s'il est formé à partir Mww_scaled et \"pmi\" si le flag est false \"ppmi\" sinon) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "ML-oJwAgt0X4",
        "outputId": "df6825c5-8968-4439-d7af-bbb3182311ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "start = time.time()\n",
        "result_ppmi_df = ppmi_pmi(Mww, True)\n",
        "print(time.time() - start)\n",
        "print(result_ppmi_df)\n",
        "result_ppmi_df.to_csv('tp2_mat5_ppmi.csv')\n",
        "\n",
        "start = time.time()\n",
        "result_pmi_df = ppmi_pmi(Mww, False)\n",
        "print(time.time() - start)\n",
        "print(result_pmi_df)\n",
        "result_pmi_df.to_csv('tp2_mat5_pmi.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "22.27034091949463\n",
            "               movie      film       one  ...    shoddy  exposition    gender\n",
            "movie       0.000000  0.000000  0.023057  ...  0.000000    0.000000  0.000000\n",
            "film        0.000000  0.000000  0.000000  ...  0.652134    0.618209  0.255346\n",
            "one         0.023057  0.000000  0.000000  ...  0.000000    0.000000  0.000000\n",
            "like        0.384492  0.000000  0.000000  ...  0.413003    0.000000  0.000000\n",
            "good        0.698302  0.326841  0.027854  ...  0.007592    0.000000  0.000000\n",
            "...              ...       ...       ...  ...       ...         ...       ...\n",
            "tyler       0.000000  0.000000  0.000000  ...  0.000000    0.000000  0.000000\n",
            "region      0.000000  0.128129  0.000000  ...  0.000000    0.000000  0.000000\n",
            "shoddy      0.000000  0.652134  0.000000  ...  0.000000    0.000000  0.000000\n",
            "exposition  0.000000  0.618209  0.000000  ...  0.000000    0.000000  0.000000\n",
            "gender      0.000000  0.255346  0.000000  ...  0.000000    0.000000  0.000000\n",
            "\n",
            "[5000 rows x 5000 columns]\n",
            "20.463147401809692\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LasfaGNo7L_m",
        "outputId": "617c25c0-26ea-4806-d594-ed24dfb1dee1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "start = time.time()\n",
        "result_ppmi_sc_df = ppmi_pmi(Mww_scaled, True)\n",
        "print(time.time() - start)\n",
        "print(result_ppmi_sc_df)\n",
        "result_ppmi_sc_df.to_csv('tp2_mat5_scaled_ppmi.csv')\n",
        "\n",
        "start = time.time()\n",
        "result_pmi_sc_df = ppmi_pmi(Mww_scaled, False)\n",
        "print(time.time() - start)\n",
        "print(result_pmi_sc_df)\n",
        "result_pmi_sc_df.to_csv('tp2_mat5_scaled_pmi.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "21.404537677764893\n",
            "               movie      film      one  ...    shoddy  exposition    gender\n",
            "movie       0.000000  0.000000  0.00000  ...  0.000000    0.000000  0.000000\n",
            "film        0.000000  0.000000  0.00000  ...  0.716377    0.427021  0.197189\n",
            "one         0.000000  0.000000  0.00000  ...  0.000000    0.000000  0.000000\n",
            "like        0.440225  0.000000  0.00000  ...  0.000000    0.000000  0.000000\n",
            "good        0.851384  0.455020  0.08143  ...  0.000000    0.000000  0.000000\n",
            "...              ...       ...      ...  ...       ...         ...       ...\n",
            "tyler       0.000000  0.000000  0.00000  ...  0.000000    0.000000  0.000000\n",
            "region      0.000000  0.174144  0.00000  ...  0.000000    0.000000  0.000000\n",
            "shoddy      0.000000  0.716377  0.00000  ...  0.000000    0.000000  0.000000\n",
            "exposition  0.000000  0.427021  0.00000  ...  0.000000    0.000000  0.000000\n",
            "gender      0.000000  0.197189  0.00000  ...  0.000000    0.000000  0.000000\n",
            "\n",
            "[5000 rows x 5000 columns]\n",
            "20.893553972244263\n",
            "               movie      film       one  ...    shoddy  exposition    gender\n",
            "movie       0.000000 -1.763126 -0.081108  ... -0.856737   -1.311971 -0.653449\n",
            "film       -1.763126  0.000000 -0.124384  ...  0.716377    0.427021  0.197189\n",
            "one        -0.081108 -0.124384  0.000000  ... -0.040257   -1.159948 -0.691704\n",
            "like        0.440225 -0.005907 -0.028960  ... -0.055299   -0.672340 -1.839684\n",
            "good        0.851384  0.455020  0.081430  ... -0.758805    0.000000 -1.833929\n",
            "...              ...       ...       ...  ...       ...         ...       ...\n",
            "tyler      -2.092244 -2.568264 -0.483415  ...  0.000000    0.000000  0.000000\n",
            "region     -1.072995  0.174144 -0.290946  ...  0.000000    0.000000  0.000000\n",
            "shoddy     -0.856737  0.716377 -0.040257  ...  0.000000    0.000000  0.000000\n",
            "exposition -1.311971  0.427021 -1.159948  ...  0.000000    0.000000  0.000000\n",
            "gender     -0.653449  0.197189 -0.691704  ...  0.000000    0.000000  0.000000\n",
            "\n",
            "[5000 rows x 5000 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZiLBtO9wnGu",
        "outputId": "364be0da-0ebc-417f-b52b-41409eddb5d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        }
      },
      "source": [
        "result_ppmi_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>movie</th>\n",
              "      <th>film</th>\n",
              "      <th>one</th>\n",
              "      <th>like</th>\n",
              "      <th>good</th>\n",
              "      <th>would</th>\n",
              "      <th>even</th>\n",
              "      <th>time</th>\n",
              "      <th>really</th>\n",
              "      <th>story</th>\n",
              "      <th>see</th>\n",
              "      <th>much</th>\n",
              "      <th>could</th>\n",
              "      <th>well</th>\n",
              "      <th>get</th>\n",
              "      <th>people</th>\n",
              "      <th>bad</th>\n",
              "      <th>also</th>\n",
              "      <th>great</th>\n",
              "      <th>first</th>\n",
              "      <th>made</th>\n",
              "      <th>way</th>\n",
              "      <th>make</th>\n",
              "      <th>movies</th>\n",
              "      <th>think</th>\n",
              "      <th>characters</th>\n",
              "      <th>character</th>\n",
              "      <th>watch</th>\n",
              "      <th>films</th>\n",
              "      <th>many</th>\n",
              "      <th>two</th>\n",
              "      <th>seen</th>\n",
              "      <th>never</th>\n",
              "      <th>little</th>\n",
              "      <th>acting</th>\n",
              "      <th>plot</th>\n",
              "      <th>best</th>\n",
              "      <th>love</th>\n",
              "      <th>show</th>\n",
              "      <th>life</th>\n",
              "      <th>...</th>\n",
              "      <th>clarke</th>\n",
              "      <th>roommate</th>\n",
              "      <th>sentinel</th>\n",
              "      <th>relax</th>\n",
              "      <th>electric</th>\n",
              "      <th>dysfunctional</th>\n",
              "      <th>tomorrow</th>\n",
              "      <th>choppy</th>\n",
              "      <th>examination</th>\n",
              "      <th>gregory</th>\n",
              "      <th>roof</th>\n",
              "      <th>alec</th>\n",
              "      <th>generated</th>\n",
              "      <th>fingers</th>\n",
              "      <th>ho</th>\n",
              "      <th>complaining</th>\n",
              "      <th>adopted</th>\n",
              "      <th>unlikable</th>\n",
              "      <th>da</th>\n",
              "      <th>bud</th>\n",
              "      <th>matched</th>\n",
              "      <th>alicia</th>\n",
              "      <th>items</th>\n",
              "      <th>muddled</th>\n",
              "      <th>invites</th>\n",
              "      <th>greatness</th>\n",
              "      <th>bernard</th>\n",
              "      <th>filling</th>\n",
              "      <th>knocked</th>\n",
              "      <th>dud</th>\n",
              "      <th>policeman</th>\n",
              "      <th>tacky</th>\n",
              "      <th>les</th>\n",
              "      <th>automatically</th>\n",
              "      <th>ordered</th>\n",
              "      <th>tyler</th>\n",
              "      <th>region</th>\n",
              "      <th>shoddy</th>\n",
              "      <th>exposition</th>\n",
              "      <th>gender</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>movie</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.023057</td>\n",
              "      <td>0.384492</td>\n",
              "      <td>0.698302</td>\n",
              "      <td>0.422466</td>\n",
              "      <td>0.241866</td>\n",
              "      <td>0.283324</td>\n",
              "      <td>0.599997</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.637430</td>\n",
              "      <td>0.288885</td>\n",
              "      <td>0.361314</td>\n",
              "      <td>0.079987</td>\n",
              "      <td>0.018666</td>\n",
              "      <td>0.193751</td>\n",
              "      <td>0.890248</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.609331</td>\n",
              "      <td>0.598753</td>\n",
              "      <td>0.772781</td>\n",
              "      <td>0.045549</td>\n",
              "      <td>0.524690</td>\n",
              "      <td>0.296342</td>\n",
              "      <td>0.635505</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.044950</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.115890</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.876434</td>\n",
              "      <td>0.203810</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.342297</td>\n",
              "      <td>0.398804</td>\n",
              "      <td>0.311948</td>\n",
              "      <td>0.305089</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.280305</td>\n",
              "      <td>0.457756</td>\n",
              "      <td>1.289101</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.415320</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.616954</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.470113</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.095717</td>\n",
              "      <td>0.527136</td>\n",
              "      <td>0.091493</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>film</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.326841</td>\n",
              "      <td>0.225054</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.230446</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.353863</td>\n",
              "      <td>0.325570</td>\n",
              "      <td>0.143639</td>\n",
              "      <td>0.265272</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.181472</td>\n",
              "      <td>0.054713</td>\n",
              "      <td>0.420084</td>\n",
              "      <td>0.510446</td>\n",
              "      <td>0.623606</td>\n",
              "      <td>0.006928</td>\n",
              "      <td>0.301055</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.086710</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.337255</td>\n",
              "      <td>0.089916</td>\n",
              "      <td>0.246923</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.519424</td>\n",
              "      <td>0.111339</td>\n",
              "      <td>0.107948</td>\n",
              "      <td>0.029768</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.415291</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.603781</td>\n",
              "      <td>0.002168</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.597515</td>\n",
              "      <td>0.092442</td>\n",
              "      <td>0.610977</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.141091</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.354895</td>\n",
              "      <td>0.067481</td>\n",
              "      <td>1.010073</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.347943</td>\n",
              "      <td>0.246405</td>\n",
              "      <td>0.448469</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.130712</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.340448</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.580052</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.128129</td>\n",
              "      <td>0.652134</td>\n",
              "      <td>0.618209</td>\n",
              "      <td>0.255346</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>one</th>\n",
              "      <td>0.023057</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.027854</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.355893</td>\n",
              "      <td>0.065921</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.071530</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.030686</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.177448</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.132090</td>\n",
              "      <td>0.365593</td>\n",
              "      <td>0.333980</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.038528</td>\n",
              "      <td>1.494353</td>\n",
              "      <td>0.197422</td>\n",
              "      <td>0.011515</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.220840</td>\n",
              "      <td>1.397679</td>\n",
              "      <td>0.198003</td>\n",
              "      <td>0.637808</td>\n",
              "      <td>1.152363</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.421808</td>\n",
              "      <td>0.023471</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.099010</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.172633</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.403548</td>\n",
              "      <td>0.735856</td>\n",
              "      <td>0.065898</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.945716</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.372129</td>\n",
              "      <td>0.327997</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.172633</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.200598</td>\n",
              "      <td>1.165100</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.697313</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>like</th>\n",
              "      <td>0.384492</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.569553</td>\n",
              "      <td>0.059891</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.565380</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.240583</td>\n",
              "      <td>0.490832</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.038478</td>\n",
              "      <td>0.692988</td>\n",
              "      <td>0.278921</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.186521</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.346436</td>\n",
              "      <td>1.244465</td>\n",
              "      <td>0.384802</td>\n",
              "      <td>0.087742</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.215840</td>\n",
              "      <td>0.951823</td>\n",
              "      <td>0.215429</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.091014</td>\n",
              "      <td>0.094198</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.230212</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.533902</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.820329</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.279088</td>\n",
              "      <td>0.358383</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.202254</td>\n",
              "      <td>0.538955</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.607969</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.245451</td>\n",
              "      <td>0.271709</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.286506</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.084225</td>\n",
              "      <td>0.893952</td>\n",
              "      <td>1.168431</td>\n",
              "      <td>0.610294</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.071355</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.413003</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>good</th>\n",
              "      <td>0.698302</td>\n",
              "      <td>0.326841</td>\n",
              "      <td>0.027854</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.097442</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.900549</td>\n",
              "      <td>0.587106</td>\n",
              "      <td>0.124358</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.031490</td>\n",
              "      <td>0.452498</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.239403</td>\n",
              "      <td>0.361139</td>\n",
              "      <td>0.406001</td>\n",
              "      <td>0.078291</td>\n",
              "      <td>0.057700</td>\n",
              "      <td>0.108202</td>\n",
              "      <td>0.286359</td>\n",
              "      <td>0.486428</td>\n",
              "      <td>0.476146</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.466959</td>\n",
              "      <td>0.247345</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.383618</td>\n",
              "      <td>0.241143</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.282098</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.128491</td>\n",
              "      <td>0.081348</td>\n",
              "      <td>1.374276</td>\n",
              "      <td>0.035697</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.417998</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.759583</td>\n",
              "      <td>0.019236</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.331841</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.2169</td>\n",
              "      <td>0.447899</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.425415</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.198544</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.007592</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 5000 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          movie      film       one  ...    shoddy  exposition    gender\n",
              "movie  0.000000  0.000000  0.023057  ...  0.000000    0.000000  0.000000\n",
              "film   0.000000  0.000000  0.000000  ...  0.652134    0.618209  0.255346\n",
              "one    0.023057  0.000000  0.000000  ...  0.000000    0.000000  0.000000\n",
              "like   0.384492  0.000000  0.000000  ...  0.413003    0.000000  0.000000\n",
              "good   0.698302  0.326841  0.027854  ...  0.007592    0.000000  0.000000\n",
              "\n",
              "[5 rows x 5000 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "QwGaKZyZ8h36"
      },
      "source": [
        "## 3. Test de PPMI (20 points)\n",
        "\n",
        "Pour le test des matrices de cooccurrences, nous allons comparer deux mesures de distance entre deux vecteurs, la distance euclidienne et la distance cosinus provenant du module [scipy.spatial.distance](https://docs.scipy.org/doc/scipy/reference/spatial.distance.html)\n",
        "\n",
        "**Distance Euclidienne**\n",
        "\n",
        "La distance euclidienne entre deux vecteurs $u$ et $v$ de dimension $n$ est\n",
        "\n",
        "$$\\textbf{euclidean}(u, v) = \n",
        "\\sqrt{\\sum_{i=1}^{n}|u_{i} - v_{i}|^{2}}$$\n",
        "\n",
        "En deux dimensions, cela correspond à la longueur de la ligne droite entre deux points.\n",
        "\n",
        "**Distance Cosinus**\n",
        "\n",
        "\n",
        "La distance cosinus entre deux vecteurs $u$ et $v$ de dimension $n$ s'écrit :\n",
        "\n",
        "$$\\textbf{cosine}(u, v) = \n",
        "1 - \\frac{\\sum_{i=1}^{n} u_{i} \\cdot v_{i}}{\\|u\\|_{2} \\cdot \\|v\\|_{2}}$$\n",
        "\n",
        "Le terme de droite dans la soustraction mesure l'angle entre $u$ et $v$; on l'appelle la *similarité cosinus* entre $u$ et $v$.\n",
        "\n",
        "\\\\\n",
        "\n",
        "**a)**\tImplémentez la fonction voisins(mot, pd, distance) qui prend un mot en entrée et une métrique de distance et qui retourne les n mots les plus similaires selon la mesure. Pour un mot w, elle ordonne tous les mots du vocabulaire en fonction de leur distance de w en utilisant la métrique de distance distance (par défaut: cosine)sur le vsm pd. Les mesures de distance à tester sont : la distance Euclidienne et la distance cosinus implantées ci-dessus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "y6WzHmQ08eNG"
      },
      "source": [
        "def voisins(word, df, n, distfunc=cosine):\n",
        "    assert distfunc.__name__ == 'cosine' or distfunc.__name__ == 'euclidean', \"distance metric not supported\"\n",
        "    assert word in df.columns, \"word not in df\"\n",
        "    order = True if distfunc.__name__ == 'euclidean' else False\n",
        "\n",
        "    closest = {}\n",
        "    for w in df.columns:\n",
        "        if w == word:\n",
        "            continue\n",
        "        distance = distfunc(df[word], df[w])\n",
        "        closest[w] = distance\n",
        "    closest = {k: v for k, v in sorted(closest.items(), key=lambda item: item[1], reverse=order)}\n",
        "\n",
        "    return list(closest.keys())[:n], list(closest.values())[:n]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "ulI3R22m-AQq"
      },
      "source": [
        "**b)** En utilisant le cadre panda associé aux matrices Mww et Mww scaled, trouvez les 5 mots les plus similaires au mot « beautiful » et affichez-les, pour chacune des deux distances."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "7YqjqTYNMOQv",
        "outputId": "1dcb8152-cab1-412d-fcc6-1e9f7da38822",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(voisins('beautiful', Mww, 5, euclidean))\n",
        "print(voisins('beautiful', Mww_scaled, 5, cosine))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(['movie', 'film', 'one', 'like', 'good'], [14089.164205161354, 11184.159423041143, 8532.992499703723, 6955.693782794065, 5842.1051000474135])\n",
            "(['amazing', 'also', 'however', 'wonderful', 'although'], [0.1574254021414464, 0.16056471525151417, 0.16951529676034738, 0.17668250674375796, 0.17743236238854254])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "0_yvpfqIBL1a"
      },
      "source": [
        "**c)** En utilisant les cadres panda associés aux matrices PMIs, trouvez les 5 mots les plus similaires au mot « beautiful » et affichez-les, pour chacune des deux distances."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "EHL_A0uEMPzO",
        "outputId": "e6b45a68-dea3-4bb4-8104-9151b2878155",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(voisins('beautiful', result_pmi_df, 5, euclidean))\n",
        "print(voisins('beautiful', result_pmi_df, 5, cosine))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(['bad', 'orders', 'officer', 'sends', 'weapon'], [102.04224518453064, 100.32570330509236, 100.17312615612182, 99.92420765152575, 99.72726079811119])\n",
            "(['wonderful', 'beauty', 'cinematography', 'gorgeous', 'perfect'], [0.6776632941554144, 0.6969494495625018, 0.6994158377456005, 0.7043181934888443, 0.7138009799274339])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "nW9kJOsUBSFf"
      },
      "source": [
        "**d)** En utilisant les cadres panda associés aux matrices PPMIs, trouvez les 5 mots les plus similaires au mot\n",
        "« beautiful » et affichez-les, pour chacune des deux distances. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "oBb5xv9PMQ9y",
        "outputId": "6b1837ea-cd77-4ae6-e981-4dc9014e44c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(voisins('beautiful', result_ppmi_df, 5, euclidean))\n",
        "print(voisins('beautiful', result_ppmi_df, 5, cosine))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(['orders', 'colonel', 'secretary', 'officer', 'assistant'], [88.99850048867745, 88.49787876325419, 88.37442627704499, 88.21623202572202, 88.18776700866722])\n",
            "(['gorgeous', 'beauty', 'wonderful', 'cinematography', 'woman'], [0.5864706015291566, 0.6018621511627195, 0.6059481605247956, 0.6118684740882059, 0.6272274889134483])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "B-Q3RfJXzdF8"
      },
      "source": [
        "**e)** Que constatez-vous entre la différence de performance de la distance euclidienne et la distance cosinus ? Que constatez-vous entre les différents types de matrices de cooccurrence ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "DcgcU9OtBg5-"
      },
      "source": [
        "On remarque qu'avec la distance cosinus, les mots detectés comme les plus similaires sont semantiquement plus proches du mot \"beautiful\" que ceux retournés par la distance euclidienne. En regardant les matrices de PMI et PPMI, on constate que les scores de PMI donne une meuilleure correspondance entre le mot \"beautiful\" et ses voisins (4 mots sur 5)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "q_COF24sBi5N"
      },
      "source": [
        "## 4.\tRéduction de dimensionnalité (20 points)\n",
        "\n",
        "**a)** Ecrivez une fonction lsa qui prend en entrée un cadre panda pd (qui contient votre matrice / vsm) et un paramètre K (qui indique le nombre de dimensions finales), et qui applique LSA avec ce paramètre k sur la matrice et retourne le vsm réduit sous forme de cadre panda."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbrN-VXT5AYf"
      },
      "source": [
        "def lsa(df, k):\n",
        "    # print(df)\n",
        "    U, s, V = np.linalg.svd(df, full_matrices=False)\n",
        "    S = np.diag(s[:k])\n",
        "    Ck = np.dot(U[:, :k], S)\n",
        "    df_r =  pd.DataFrame(Ck).T\n",
        "    df_r.columns = df.keys()\n",
        "    return  df_r"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "pZDhlR2ZBwag"
      },
      "source": [
        "**b)** Exécutez lsa sur les cadres panda associés à vos matrices Mww et Mww_scaled avec une dimension k=100"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "4Fh1yGAQBvsg",
        "outputId": "c9034aaa-c1ce-41fe-cd1e-974f4d3f6768",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "Mww_lsa = lsa(Mww, 100)\n",
        "Mww_scaled_lsa = lsa(Mww_scaled, 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "             movie    film     one    like  ...  region  shoddy  exposition  gender\n",
            "0                                           ...                                    \n",
            "movie          0.0  2204.0  3756.0  3763.0  ...     5.0     8.0         6.0     6.0\n",
            "film        2204.0     0.0  3279.0  2576.0  ...    10.0    16.0        15.0    11.0\n",
            "one         3756.0  3279.0     0.0  1635.0  ...     6.0     6.0         4.0     3.0\n",
            "like        3763.0  2576.0  1635.0     0.0  ...     2.0     7.0         4.0     2.0\n",
            "good        3540.0  2473.0  1331.0   983.0  ...     0.0     4.0         0.0     2.0\n",
            "...            ...     ...     ...     ...  ...     ...     ...         ...     ...\n",
            "tyler          4.0     3.0     4.0     5.0  ...     0.0     0.0         0.0     0.0\n",
            "region         5.0    10.0     6.0     2.0  ...     0.0     0.0         0.0     0.0\n",
            "shoddy         8.0    16.0     6.0     7.0  ...     0.0     0.0         0.0     0.0\n",
            "exposition     6.0    15.0     4.0     4.0  ...     0.0     0.0         0.0     0.0\n",
            "gender         6.0    11.0     3.0     2.0  ...     0.0     0.0         0.0     0.0\n",
            "\n",
            "[5000 rows x 5000 columns]\n",
            "                  movie         film  ...  exposition    gender\n",
            "0                                     ...                      \n",
            "movie          0.000000   788.200000  ...    2.033333  2.900000\n",
            "film         788.200000     0.000000  ...    6.100000  4.700000\n",
            "one         1660.766667  1448.483333  ...    1.333333  1.666667\n",
            "like        1848.800000  1219.616667  ...    1.450000  0.583333\n",
            "good        1888.966667  1289.850000  ...    0.000000  0.450000\n",
            "...                 ...          ...  ...         ...       ...\n",
            "tyler          1.083333     0.700000  ...    0.000000  0.000000\n",
            "region         2.250000     4.800000  ...    0.000000  0.000000\n",
            "shoddy         2.966667     7.933333  ...    0.000000  0.000000\n",
            "exposition     2.033333     6.100000  ...    0.000000  0.000000\n",
            "gender         2.900000     4.700000  ...    0.000000  0.000000\n",
            "\n",
            "[5000 rows x 5000 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOLZk5dRDSVL"
      },
      "source": [
        "print(Mww_lsa)\n",
        "print(Mww_scaled_lsa)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "0IQvZbddB4Zp"
      },
      "source": [
        "**c)** En utilisant les matrices de co-occurrence (de base et scalés) réduites avec LSA, trouvez les 5 mots les plus similaires au mot « beautiful » selon la distance cosinus et affichez-les."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "4BPts630Mw3t",
        "outputId": "898fb431-0d0f-446d-915f-bfae58bcc154",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(voisins('beautiful', Mww_lsa, 5, cosine))\n",
        "print(voisins('beautiful', Mww_scaled_lsa, 5, cosine))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(['beauty', 'gorgeous', 'amazing', 'lovely', 'moving'], [0.032669409202906174, 0.03657954413484443, 0.04208923847784096, 0.042402514897163956, 0.04251957653355276])\n",
            "(['gorgeous', 'lovely', 'powerful', 'beauty', 'moving'], [0.05065235807436552, 0.05951329170048869, 0.05961045482887939, 0.06405762226993739, 0.07215139001822224])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "y_wxbYFPCCCN"
      },
      "source": [
        "d) En utilisant les matrices PMIs et PPMIs réduites avec lsa, trouvez les 5 mots les plus similaires au mot « beautiful » selon la distance cosinus et affichez-les"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "QqgCh0WNM1Jl",
        "outputId": "02975a65-57e9-407c-bdc7-b61d808eb1e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "ppmi_lsa = lsa(result_ppmi_df, 100)\n",
        "pmi_lsa = lsa(result_pmi_df, 100)\n",
        "ppmi_lsa_scaled = lsa(result_ppmi_sc_df, 100)\n",
        "pmi_lsa_scaled = lsa(result_pmi_sc_df, 100)\n",
        "\n",
        "print(voisins('beautiful', ppmi_lsa, 5, cosine))\n",
        "print(voisins('beautiful', ppmi_lsa_scaled, 5, cosine))\n",
        "print(voisins('beautiful', pmi_lsa, 5, cosine))\n",
        "print(voisins('beautiful', pmi_lsa_scaled, 5, cosine))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(['gorgeous', 'beauty', 'lovely', 'wonderful', 'stunning'], [0.12760579614031764, 0.13771878225400447, 0.1755551581903143, 0.18865325555909485, 0.18935457217382423])\n",
            "(['gorgeous', 'lovely', 'stunning', 'beauty', 'wonderful'], [0.1150025841222665, 0.16724701757559501, 0.17983857142273496, 0.19269401657335683, 0.20377703421375237])\n",
            "(['wonderful', 'nice', 'beauty', 'perfect', 'amazing'], [0.24793610676783162, 0.277462073128817, 0.2899269744357047, 0.29695605685101367, 0.3178627577431469])\n",
            "(['wonderful', 'nice', 'amazing', 'gorgeous', 'beauty'], [0.2946922586535222, 0.36872790968350555, 0.3912010451738721, 0.40424849751648173, 0.413443756722526])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "Qfwq16JtCSFy"
      },
      "source": [
        "**e)** En utilisant sklearn.decomposition.TruncatedSVD, créez les matrices réduites à partir des mêmes matrices que celles de la question précédentes (la matrice pmi et la matrice pmi_scaled) Puis tester ces nouvelles matrices LSA pour trouver les 5 mots les plus similaires au mot « beautiful » \n",
        "\n",
        "Ici aussi, nous voulons aussi obtenir des matrices de dimension k=100"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "Mj-aOpSSM2_O"
      },
      "source": [
        "def sklearn_svd(df, k):\n",
        "    svd_model = TruncatedSVD(n_components=k)\n",
        "    df_r = svd_model.fit_transform(df)\n",
        "    df_r =  pd.DataFrame(df_r).T\n",
        "    df_r.columns = df.keys()\n",
        "    return  df_r"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLVkmSaiHGvS",
        "outputId": "217ef7f2-4b6c-42a0-9095-e364793f7fd1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "ppmi_svd = sklearn_svd(result_ppmi_df, 100)\n",
        "pmi_svd = sklearn_svd(result_pmi_df, 100)\n",
        "ppmi_svd_scaled = sklearn_svd(result_ppmi_sc_df, 100)\n",
        "pmi_svd_scaled = sklearn_svd(result_pmi_sc_df, 100)\n",
        "\n",
        "print(voisins('beautiful', ppmi_svd, 5, cosine))\n",
        "print(voisins('beautiful', ppmi_svd_scaled, 5, cosine))\n",
        "print(voisins('beautiful', pmi_svd, 5, cosine))\n",
        "print(voisins('beautiful', pmi_svd_scaled, 5, cosine))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(['gorgeous', 'beauty', 'stunning', 'wonderful', 'lovely'], [0.13635057215259017, 0.14477190292361775, 0.1739820015877136, 0.17523504033250148, 0.18091811565242333])\n",
            "(['gorgeous', 'lovely', 'beauty', 'stunning', 'wonderful'], [0.11110285774789697, 0.16167138914130685, 0.18842995122898665, 0.19146061866476305, 0.19259086331273823])\n",
            "(['wonderful', 'nice', 'perfect', 'beauty', 'amazing'], [0.24238432439533164, 0.27146541720778616, 0.2891781822647449, 0.307952544809302, 0.30884441201256585])\n",
            "(['wonderful', 'nice', 'amazing', 'beauty', 'perfect'], [0.2863561546786706, 0.34021860017258165, 0.3925005691837269, 0.41003174806305653, 0.4172124514280279])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "9i7di2PRCTnh"
      },
      "source": [
        "f) Commentez vos résultats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "ubED1oiwNJpm"
      },
      "source": [
        "Les 2 methodes LSA et SVD detectent les memes mots similaires a \"beautiful\", c'est juste l'ordre des mots qui varient (et les scores/distances associees different legerement)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "K3avEFcPG0-e"
      },
      "source": [
        "## 5. Évaluation (10 points)\n",
        "\n",
        "Il est temps d’évaluer l’intérêt de nos modèles de vecteurs. Nous allons pour cela utiliser un ensemble de données de similarité de mots (relatedness) The MEN Test Collection, qui se trouve dans le répertoire test. L’ensemble de données contient une paire de mots avec un score de similarité attribué par des humains. En d’autres termes, une ligne (un exemple) de l’ensemble de données est de la forme : \\<mot_1> \\<mot_2> \\<score>.\n",
        "\n",
        "Pour aligner les distances obtenues avec vos métriques, ce score est converti en nombre réel négatif par la fonction read_test_dataset que vous avez dans le squelette du TP.\n",
        "\n",
        "La métrique d’évaluation est le coefficient de corrélation de Spearman 𝜌 entre les scores humains et vos distances (voir https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient). \n",
        "\n",
        "Nous allons maintenant évaluer les différents vsm obtenus sur l'ensemble de données: MEN_dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "7j5LjMlav4pQ"
      },
      "source": [
        "#### Fonctions pour lire le jeu de données MEN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "NodeMtrwGw0O"
      },
      "source": [
        "def read_test_dataset(\n",
        "        src_filename,\n",
        "        delimiter=','):\n",
        "    with open(src_filename) as f:\n",
        "        reader = csv.reader(f, delimiter=delimiter)\n",
        "        for row in reader:\n",
        "            w1 = row[0].strip().lower()\n",
        "            w2 = row[1].strip().lower()\n",
        "            score = row[2]\n",
        "            score = -float(score)\n",
        "            yield (w1, w2, score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "sTmkM5EDG-pA"
      },
      "source": [
        "# Retourne un itérable sur le jeu de données MEN\n",
        "def men_dataset():\n",
        "    src_filename = os.path.join(\n",
        "        DIRNAME_MEN, 'MEN_dataset_natural_form_full')\n",
        "    return read_test_dataset(\n",
        "        src_filename, delimiter=' ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "uPd__Q4NHUlp"
      },
      "source": [
        "def evaluate(ds, df, distfunc=cosine):\n",
        "    \"\"\"\n",
        "    ds : iterator\n",
        "       retourne des tuples (word1, word2, score).\n",
        "\n",
        "    df : pd.DataFrame\n",
        "        le modèle vsm à évaluer\n",
        "\n",
        "    distfunc : la mesure de distance entre vecteurs\n",
        "  \n",
        "    Retour: le coefficient de correlation de Spearman entre les scores de l'ensemble de données de test \n",
        "    et celui du modele vsm qui se présente sous la forme d'un cadre Panda pd avec les colonnes\n",
        "    ['word1', 'word2', 'score', 'distance'].\n",
        "    \"\"\"\n",
        "    data = []\n",
        "    for w1, w2, score in ds():\n",
        "        d = {'word1': w1, 'word2': w2,'score': score}\n",
        "        if w1 not in df.index or w2 not in df.index:\n",
        "            continue\n",
        "        else:\n",
        "            w1 = df.loc[w1]\n",
        "            w2 = df.loc[w2] \n",
        "        d['distance'] = distfunc(w1, w2)\n",
        "        data.append(d)\n",
        "\n",
        "    data = pd.DataFrame(data)\n",
        "    rho, pvalue = spearmanr(data['score'].values, b=data['distance'].values)\n",
        "    return rho, data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "MBrTrq8BHdLs"
      },
      "source": [
        "**a)**\tTestez chacun de vos modèles vsm (Matrice de base, matrice scalée, les PMIs et PPMIs et toutes les matrices LSA (de base, scalée, pmi, ppmi) en appelant la fonction evaluate avec les deux mesure de distance (euclidienne et cosinus) et affichez vos résultats dans une seule table."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "UL5s4dT4QiX_",
        "outputId": "7021c7a7-f4b4-44f7-a1f9-e77f0bbebf58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "vsm_models = [Mww, Mww_scaled, result_pmi_df, result_ppmi_df, result_pmi_sc_df, result_ppmi_sc_df] # pmi_lsa, ppmi_lsa, pmi_lsa_scaled, ppmi_lsa_scaled\n",
        "for model in vsm_models:\n",
        "    rho, data = evaluate(men_dataset, model, distfunc=cosine)\n",
        "    print(rho, data)\n",
        "\n",
        "for model in vsm_models:\n",
        "    rho, data = evaluate(men_dataset, model, distfunc=euclidean)\n",
        "    print(rho, data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.004063860230346588         word1        word2  score  distance\n",
            "0       river        water  -49.0  0.327278\n",
            "1        rain        storm  -49.0  0.559845\n",
            "2       dance      dancers  -49.0  0.412789\n",
            "3      camera  photography  -49.0  0.300166\n",
            "4       sunny     sunshine  -48.0  0.655110\n",
            "..        ...          ...    ...       ...\n",
            "925       car       tongue   -4.0  0.442126\n",
            "926      fish      theatre   -3.0  0.509976\n",
            "927       hot       zombie   -3.0  0.253801\n",
            "928  children         ford   -3.0  0.427045\n",
            "929     grave          hat   -2.0  0.542818\n",
            "\n",
            "[930 rows x 4 columns]\n",
            "0.07551986097318225         word1        word2  score  distance\n",
            "0       river        water  -49.0  0.482563\n",
            "1        rain        storm  -49.0  0.736418\n",
            "2       dance      dancers  -49.0  0.554667\n",
            "3      camera  photography  -49.0  0.538064\n",
            "4       sunny     sunshine  -48.0  0.830768\n",
            "..        ...          ...    ...       ...\n",
            "925       car       tongue   -4.0  0.639554\n",
            "926      fish      theatre   -3.0  0.680656\n",
            "927       hot       zombie   -3.0  0.390265\n",
            "928  children         ford   -3.0  0.652683\n",
            "929     grave          hat   -2.0  0.760564\n",
            "\n",
            "[930 rows x 4 columns]\n",
            "0.33137739698426183         word1        word2  score  distance\n",
            "0       river        water  -49.0  0.737185\n",
            "1        rain        storm  -49.0  0.875757\n",
            "2       dance      dancers  -49.0  0.803568\n",
            "3      camera  photography  -49.0  0.749690\n",
            "4       sunny     sunshine  -48.0  0.858291\n",
            "..        ...          ...    ...       ...\n",
            "925       car       tongue   -4.0  0.944867\n",
            "926      fish      theatre   -3.0  0.916032\n",
            "927       hot       zombie   -3.0  0.806345\n",
            "928  children         ford   -3.0  0.895885\n",
            "929     grave          hat   -2.0  0.895426\n",
            "\n",
            "[930 rows x 4 columns]\n",
            "0.25013929945369867         word1        word2  score  distance\n",
            "0       river        water  -49.0  0.730166\n",
            "1        rain        storm  -49.0  0.876068\n",
            "2       dance      dancers  -49.0  0.773630\n",
            "3      camera  photography  -49.0  0.676349\n",
            "4       sunny     sunshine  -48.0  0.858683\n",
            "..        ...          ...    ...       ...\n",
            "925       car       tongue   -4.0  0.882041\n",
            "926      fish      theatre   -3.0  0.909065\n",
            "927       hot       zombie   -3.0  0.793220\n",
            "928  children         ford   -3.0  0.824089\n",
            "929     grave          hat   -2.0  0.893041\n",
            "\n",
            "[930 rows x 4 columns]\n",
            "0.4147775315823167         word1        word2  score  distance\n",
            "0       river        water  -49.0  0.761722\n",
            "1        rain        storm  -49.0  0.878726\n",
            "2       dance      dancers  -49.0  0.834569\n",
            "3      camera  photography  -49.0  0.812570\n",
            "4       sunny     sunshine  -48.0  0.867941\n",
            "..        ...          ...    ...       ...\n",
            "925       car       tongue   -4.0  0.978724\n",
            "926      fish      theatre   -3.0  0.939381\n",
            "927       hot       zombie   -3.0  0.838233\n",
            "928  children         ford   -3.0  0.941144\n",
            "929     grave          hat   -2.0  0.912415\n",
            "\n",
            "[930 rows x 4 columns]\n",
            "0.34958922144675275         word1        word2  score  distance\n",
            "0       river        water  -49.0  0.742202\n",
            "1        rain        storm  -49.0  0.878216\n",
            "2       dance      dancers  -49.0  0.781486\n",
            "3      camera  photography  -49.0  0.716363\n",
            "4       sunny     sunshine  -48.0  0.866484\n",
            "..        ...          ...    ...       ...\n",
            "925       car       tongue   -4.0  0.893932\n",
            "926      fish      theatre   -3.0  0.921524\n",
            "927       hot       zombie   -3.0  0.819766\n",
            "928  children         ford   -3.0  0.849818\n",
            "929     grave          hat   -2.0  0.911881\n",
            "\n",
            "[930 rows x 4 columns]\n",
            "0.14377103326253793         word1        word2  score    distance\n",
            "0       river        water  -49.0  112.035709\n",
            "1        rain        storm  -49.0   81.516869\n",
            "2       dance      dancers  -49.0  238.641991\n",
            "3      camera  photography  -49.0  527.170750\n",
            "4       sunny     sunshine  -48.0   58.180753\n",
            "..        ...          ...    ...         ...\n",
            "925       car       tongue   -4.0  370.271522\n",
            "926      fish      theatre   -3.0  132.804367\n",
            "927       hot       zombie   -3.0  186.026880\n",
            "928  children         ford   -3.0  453.102637\n",
            "929     grave          hat   -2.0   77.524190\n",
            "\n",
            "[930 rows x 4 columns]\n",
            "0.1476070667921813         word1        word2  score    distance\n",
            "0       river        water  -49.0   64.848629\n",
            "1        rain        storm  -49.0   53.758439\n",
            "2       dance      dancers  -49.0  137.195286\n",
            "3      camera  photography  -49.0  343.556635\n",
            "4       sunny     sunshine  -48.0   43.990630\n",
            "..        ...          ...    ...         ...\n",
            "925       car       tongue   -4.0  196.720929\n",
            "926      fish      theatre   -3.0   74.132340\n",
            "927       hot       zombie   -3.0  118.242695\n",
            "928  children         ford   -3.0  241.553292\n",
            "929     grave          hat   -2.0   52.864801\n",
            "\n",
            "[930 rows x 4 columns]\n",
            "0.07942494244412274         word1        word2  score   distance\n",
            "0       river        water  -49.0  90.793728\n",
            "1        rain        storm  -49.0  98.857416\n",
            "2       dance      dancers  -49.0  90.058367\n",
            "3      camera  photography  -49.0  81.234921\n",
            "4       sunny     sunshine  -48.0  87.526768\n",
            "..        ...          ...    ...        ...\n",
            "925       car       tongue   -4.0  93.191787\n",
            "926      fish      theatre   -3.0  91.319119\n",
            "927       hot       zombie   -3.0  86.286045\n",
            "928  children         ford   -3.0  89.665364\n",
            "929     grave          hat   -2.0  96.922542\n",
            "\n",
            "[930 rows x 4 columns]\n",
            "-0.02018285543555737         word1        word2  score   distance\n",
            "0       river        water  -49.0  88.698878\n",
            "1        rain        storm  -49.0  98.306494\n",
            "2       dance      dancers  -49.0  86.186486\n",
            "3      camera  photography  -49.0  72.176996\n",
            "4       sunny     sunshine  -48.0  87.299583\n",
            "..        ...          ...    ...        ...\n",
            "925       car       tongue   -4.0  86.026381\n",
            "926      fish      theatre   -3.0  90.036448\n",
            "927       hot       zombie   -3.0  82.191985\n",
            "928  children         ford   -3.0  81.560068\n",
            "929     grave          hat   -2.0  96.050909\n",
            "\n",
            "[930 rows x 4 columns]\n",
            "0.2131290816919803         word1        word2  score   distance\n",
            "0       river        water  -49.0  94.524730\n",
            "1        rain        storm  -49.0  98.321843\n",
            "2       dance      dancers  -49.0  93.643685\n",
            "3      camera  photography  -49.0  91.729387\n",
            "4       sunny     sunshine  -48.0  85.531023\n",
            "..        ...          ...    ...        ...\n",
            "925       car       tongue   -4.0  99.986835\n",
            "926      fish      theatre   -3.0  91.613727\n",
            "927       hot       zombie   -3.0  92.175129\n",
            "928  children         ford   -3.0  97.150250\n",
            "929     grave          hat   -2.0  97.026052\n",
            "\n",
            "[930 rows x 4 columns]\n",
            "-0.0025490411727879165         word1        word2  score   distance\n",
            "0       river        water  -49.0  89.133414\n",
            "1        rain        storm  -49.0  96.800685\n",
            "2       dance      dancers  -49.0  85.605239\n",
            "3      camera  photography  -49.0  74.696030\n",
            "4       sunny     sunshine  -48.0  84.691647\n",
            "..        ...          ...    ...        ...\n",
            "925       car       tongue   -4.0  87.234432\n",
            "926      fish      theatre   -3.0  88.181690\n",
            "927       hot       zombie   -3.0  83.298536\n",
            "928  children         ford   -3.0  82.441592\n",
            "929     grave          hat   -2.0  94.793524\n",
            "\n",
            "[930 rows x 4 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "JjJwYKsTKXCR"
      },
      "source": [
        "**b)**\tCommentez vos résultats d'évaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "-ZQhdrxFQjsb"
      },
      "source": [
        "On remarque qu'on a toujours les memes scores de similarite."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wyrbru1nML2F"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}