{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "inf8460_projet_A20_equipe8.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6984723399ad4429a78be66fead09207": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_4792c5db1d1045f98440ef4ca34b433c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_30d4f585019e465a94958402d26bde1d",
              "IPY_MODEL_1d9e9b5dbe094c33a3176780ff37b9f1"
            ]
          }
        },
        "4792c5db1d1045f98440ef4ca34b433c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "30d4f585019e465a94958402d26bde1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c135251c65474c4b9d8df448cea1be6e",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 473,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 473,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9ef19809a9c147ec83e5c7e94c555c9c"
          }
        },
        "1d9e9b5dbe094c33a3176780ff37b9f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_27e7045593af4d63a4c1fdd31e719c20",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 473/473 [00:00&lt;00:00, 1.53kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a6a0c7287e7b489ca1bb5d8100af8b99"
          }
        },
        "c135251c65474c4b9d8df448cea1be6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9ef19809a9c147ec83e5c7e94c555c9c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "27e7045593af4d63a4c1fdd31e719c20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a6a0c7287e7b489ca1bb5d8100af8b99": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "594581a8da644221a1c41a298ed4b39d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0bfa0231a34040e6b252629f85399b59",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_8ac6053a92c24954b1d2e67edadebd65",
              "IPY_MODEL_a53cecd910c54965acbff9d5b1dbfc21"
            ]
          }
        },
        "0bfa0231a34040e6b252629f85399b59": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8ac6053a92c24954b1d2e67edadebd65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_82c5a18e88f94fd9bc745fde8a369c27",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 260793700,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 260793700,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b01380a6701f491e8412580ad0b01262"
          }
        },
        "a53cecd910c54965acbff9d5b1dbfc21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9270001477b54b50a389f04c2406c6f4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 261M/261M [00:04&lt;00:00, 55.3MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bf3a5271b73f452188b40e6602771780"
          }
        },
        "82c5a18e88f94fd9bc745fde8a369c27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b01380a6701f491e8412580ad0b01262": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9270001477b54b50a389f04c2406c6f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bf3a5271b73f452188b40e6602771780": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5edaed461fdc44219061b87f71dc4957": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0a6d742b0ccd4cd38f2e6510dc486b4b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b4028828b1ea4f1689e406da40bf7adb",
              "IPY_MODEL_ea8f52bc3b5f44c290833426acee4c89"
            ]
          }
        },
        "0a6d742b0ccd4cd38f2e6510dc486b4b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b4028828b1ea4f1689e406da40bf7adb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_eecccd9cfb6d41a194556fee365c869c",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 213450,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 213450,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3086765b1fb942738e2717619ebadcf7"
          }
        },
        "ea8f52bc3b5f44c290833426acee4c89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8486f5fda8b4459d97968dabfd5a4f23",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 213k/213k [00:00&lt;00:00, 3.19MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bef3fd3b6ff44e99829190215b0778dc"
          }
        },
        "eecccd9cfb6d41a194556fee365c869c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3086765b1fb942738e2717619ebadcf7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8486f5fda8b4459d97968dabfd5a4f23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bef3fd3b6ff44e99829190215b0778dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "77fe4a29c05e447c94427d18d9aebeb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_97e283ae201e4dbdb65042da24e0594d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2c19edbf8b08457090aebf43ce972e14",
              "IPY_MODEL_3a201cc58a7d439783c14921b67f1b37"
            ]
          }
        },
        "97e283ae201e4dbdb65042da24e0594d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2c19edbf8b08457090aebf43ce972e14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c1cf65935503489fb2b7d0a6d744ed31",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 230,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 230,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_53571776f9794f34a734db33c7e2a5dd"
          }
        },
        "3a201cc58a7d439783c14921b67f1b37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a4c24addae744d1e8912ec698d2912c9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 230/230 [00:00&lt;00:00, 235B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_23b914d2eea2417986b572301325955e"
          }
        },
        "c1cf65935503489fb2b7d0a6d744ed31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "53571776f9794f34a734db33c7e2a5dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a4c24addae744d1e8912ec698d2912c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "23b914d2eea2417986b572301325955e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvon_hH2C87M"
      },
      "source": [
        "# **INF8460 A20 Project: Open-domain questions answering**\n",
        "\n",
        "<br>\n",
        "\n",
        "Equipe 8:\n",
        "\n",
        "\n",
        "*   Cedric Sadeu (Glove, ranking with classification)\n",
        "*   Mamoudou Sacko (pretraitement + TF-IDF, cosine ranking)\n",
        "*   Oumayma Messoussi (PCP Bert, ML/DL for ranking)\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMtbIoAD31sx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b6941d7-58f0-4df2-f88f-8fe778ee9780"
      },
      "source": [
        "!pip install transformers pytorch-pretrained-bert # pytorch-nlp pytorch_transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.5.1)\n",
            "Requirement already satisfied: pytorch-pretrained-bert in /usr/local/lib/python3.6/dist-packages (0.6.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: sentencepiece==0.1.91 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: tokenizers==0.9.3 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.16.21)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.7.0+cu101)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.10.0)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.20.0,>=1.19.21 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (1.19.21)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.7.4.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (0.16.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.20.0,>=1.19.21->boto3->pytorch-pretrained-bert) (2.8.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxkOcrpE4tCW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "378347fa-8b02-4796-d025-07a99fa72f75"
      },
      "source": [
        "import io\n",
        "import os\n",
        "import math\n",
        "import nltk\n",
        "import time\n",
        "import torch\n",
        "import random\n",
        "import sklearn\n",
        "import zipfile\n",
        "import operator\n",
        "import requests\n",
        "import functools\n",
        "import itertools\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "import multiprocessing\n",
        "from functools import partial\n",
        "from typing import Dict, List, Tuple\n",
        "from collections import Counter, defaultdict\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "from scipy.spatial.distance import euclidean, cosine\n",
        "from transformers import pipeline, Trainer, TrainingArguments\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, BertTokenizer, BertModel, BertForQuestionAnswering\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfLr5d9n8Nyp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8209bcc0-1411-4eca-93d9-717cd4c83a98"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!ls '/content/drive/My Drive/Colab Notebooks/INF8460/Project/'"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "data\t\t\t\t  LSTMSiameseTextSimilarity  pytorch_model.bin\n",
            "inf8460_projet_A20_equipe8.ipynb  output\t\t     yahooLTR_C14.tgz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-B1THH6Y1YJj"
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/Colab Notebooks/INF8460/Project/')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIcemP-_-mKa"
      },
      "source": [
        "### Lecture des donnees"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSVtCiWK-_Ut"
      },
      "source": [
        "def read_data(path: str) -> Tuple[List[int], List[str]]:\n",
        "    data = pd.read_csv(path)\n",
        "    ids = data[\"id\"].tolist()\n",
        "    paragraphs = data[\"paragraph\"].tolist()\n",
        "    return ids, paragraphs\n",
        "\n",
        "def read_questions(path: str) -> Tuple[List[int], List[str], List[int], List[str]]:\n",
        "    data = pd.read_csv(path)\n",
        "    ids = data[\"id\"].tolist()\n",
        "    questions = data[\"question\"].tolist()\n",
        "    if \"test\" not in path:\n",
        "        paragraph_ids = data[\"paragraph_id\"].tolist()\n",
        "        answers = data[\"answer\"].tolist()\n",
        "        return ids, questions, paragraph_ids, answers\n",
        "    else:\n",
        "        return ids, questions\n",
        "\n",
        "def save_to_csv(path: str, corpus):\n",
        "    df = pd.DataFrame(corpus, columns= list(corpus.keys())).head()\n",
        "    df.to_csv (os.path.join(output_path, path), index = False, header=True)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6h3I8Gl3769R"
      },
      "source": [
        "data_path = \"data\"\n",
        "output_path = \"/content/drive/My Drive/Colab Notebooks/INF8460/Project/output\"\n",
        "\n",
        "# train_data = read_data(os.path.join(data_path, \"/content/drive/My Drive/Colab Notebooks/INF8460/Project/data/corpus.csv\"))\n",
        "train_ids = read_questions(os.path.join(data_path, \"/content/drive/My Drive/Colab Notebooks/INF8460/Project/data/train_ids.csv\"))\n",
        "test_ids = read_questions(os.path.join(data_path, \"/content/drive/My Drive/Colab Notebooks/INF8460/Project/data/test.csv\"))\n",
        "\n",
        "\n",
        "# paragraphs = [\" \".join(sentence.split()).lower() for sentence in train_data[1]]\n",
        "questions = [\" \".join(sentence.split()).lower() for sentence in train_ids[1]]\n",
        "test_questions = [\" \".join(sentence.split()).lower() for sentence in test_ids[1]]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tj5wul4l-pMg"
      },
      "source": [
        "### Pretraitement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6Qv4veV-ojM"
      },
      "source": [
        "class Preprocess(object):\n",
        "    def __init__(self, lemmatize=True):\n",
        "        self.stopwords = set(nltk.corpus.stopwords.words(\"english\"))\n",
        "        self.lemmatize = lemmatize\n",
        "\n",
        "    def preprocess_pipeline(self, data):\n",
        "        clean_tokenized_data = self._clean_doc(data)\n",
        "        if self.lemmatize:\n",
        "            clean_tokenized_data = self._lemmatize(clean_tokenized_data)\n",
        "\n",
        "        return clean_tokenized_data\n",
        "\n",
        "    def _clean_doc(self, data):\n",
        "        tokenizer = nltk.tokenize.RegexpTokenizer(r\"\\w+\")\n",
        "        return [\n",
        "            [\n",
        "                token.lower()\n",
        "                for token in tokenizer.tokenize(review)\n",
        "                if token.lower() not in self.stopwords\n",
        "                and len(token) > 1\n",
        "                and token.isalpha()\n",
        "            ]\n",
        "            for review in data\n",
        "        ]\n",
        "\n",
        "    def _lemmatize(self, data):\n",
        "        lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "        return [[lemmatizer.lemmatize(word) for word in review] for review in data]\n",
        "\n",
        "    def convert_to_reviews(self, tokenized_reviews):\n",
        "        reviews = []\n",
        "        for tokens in tokenized_reviews:\n",
        "            reviews.append(\" \".join(tokens))\n",
        "\n",
        "        return reviews"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mybu06h-xpN"
      },
      "source": [
        "pre = Preprocess()\n",
        "\n",
        "# paragraphs_tokenized = pre.preprocess_pipeline(paragraphs)\n",
        "questions_tokenized = pre.preprocess_pipeline(questions)\n",
        "test_questions_tokenized = pre.preprocess_pipeline(test_questions)\n",
        "\n",
        "# paragraphs_text = [\" \".join(sentence) for sentence in paragraphs_tokenized]\n",
        "questions_text = [\" \".join(sentence) for sentence in questions_tokenized]\n",
        "test_questions_text = [\" \".join(sentence) for sentence in test_questions_tokenized]\n",
        "\n",
        "# del paragraphs_tokenized\n",
        "del questions_tokenized\n",
        "del test_questions_tokenized"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xJqabvHFNvq"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## **1. Plongements lexicaux**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQREt5DLDV1E"
      },
      "source": [
        "### TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5mTUyWXDXj7"
      },
      "source": [
        "def buildVocab(X) -> object:\n",
        "  vectorizer = CountVectorizer(min_df=0, lowercase=False)\n",
        "  vectorizer.fit(X)\n",
        "  return vectorizer.vocabulary_\n",
        "\n",
        "def getTfIdfReprentation(data, vectorizer) -> object: \n",
        "  data_tfidf = vectorizer.fit_transform(data)\n",
        "  features = vectorizer.get_feature_names()\n",
        "  dense = data_tfidf.todense()\n",
        "  return dense # data_tfidf\n",
        "\n",
        "# def getTfIdfEmbedded(vocab, data, feature = 5) -> object:\n",
        "#   vectorizer = TfidfVectorizer(max_features=15) \n",
        "#   data_tfidf = vectorizer.fit_transform(data)\n",
        "#   features = vectorizer.get_feature_names()\n",
        "#   dense = data_tfidf.todense()\n",
        "#   denselist = dense.tolist()\n",
        "#   df = pd.DataFrame(\n",
        "#     denselist,columns=features)\n",
        "#   return df\n",
        "\n",
        "\n",
        "def get_doc_embedded(X, vocab, embeddings) -> object:\n",
        "  X_embedded = np.zeros((len(X), len(embeddings)), dtype=float)\n",
        "\n",
        "  for i, doc in enumerate(X):\n",
        "    vec = np.zeros((1, len(embeddings)), dtype=float)\n",
        "    tokens = doc.split() #new_question_tfidf\n",
        "    cpt = 0\n",
        "    for word in tokens:\n",
        "      if(word in vocab):\n",
        "        cpt += 1\n",
        "        vec += embeddings[word]\n",
        "    vec /= cpt\n",
        "    X_embedded[i] = vec\n",
        "  return X_embedded\n",
        "\n",
        "  def getMedian(corpus):\n",
        "    total_lenght = sorted([len(doc) for doc in corpus])\n",
        "    return total_lenght[int(len(total_lenght) * 2/3)]\n",
        "\n",
        "  \n",
        "def sklearn_svd(df, k):\n",
        "    svd_model = TruncatedSVD(n_components=k)\n",
        "    df_r = svd_model.fit_transform(df)\n",
        "    return  df_r"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCJPGU0V_LBG"
      },
      "source": [
        "corpus = {'id': train_data[0], 'paragraph': paragraphs_tfidf }\n",
        "save_to_csv(\"corpus.csv\", corpus)\n",
        "\n",
        "train_ids = {'id': train_ids[0], 'question': paragraphs_tfidf, 'paragraph_id': train_ids[2], 'answer': train_ids[3] }\n",
        "save_to_csv(\"train_ids.csv\", train_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7njiavPiDYDu"
      },
      "source": [
        "### GloVe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7TJbIqyjxHt",
        "outputId": "6e94a646-0b4e-47dc-93d2-048d20b8b0bf"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip\n",
        "!rm glove.6B.50d.txt\n",
        "!rm glove.6B.100d.txt\n",
        "!rm glove.6B.200d.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-11-17 23:35:47--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2020-11-17 23:35:48--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2020-11-17 23:35:48--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: â€˜glove.6B.zipâ€™\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  1.94MB/s    in 6m 28s  \n",
            "\n",
            "2020-11-17 23:42:16 (2.12 MB/s) - â€˜glove.6B.zipâ€™ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7c0c06T2DZX1"
      },
      "source": [
        "def read_from_csv(path):\n",
        "    \"\"\" \n",
        "    reads a matrix from a csv\n",
        "    \"\"\"\n",
        "    data = pd.read_csv(path)\n",
        "    data = data.dropna(axis=1,how='all')\n",
        "    return (data.to_numpy().T).tolist()\n",
        "\n",
        "def get_lines_gloves(line):\n",
        "    \"\"\" \n",
        "    this function takes:\n",
        "    line: a line from the glove text file (a string)\n",
        "    returns a tuple (word, embeddings vector)\n",
        "    \"\"\"\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    return word, np.asarray(values[1:], dtype=float)\n",
        "\n",
        "def get_gloves_dict(path = \"glove.6B.300d.txt\"):\n",
        "    \"\"\" \n",
        "    this function takes:\n",
        "    path: to a  glove text file (a string)\n",
        "    returns a dict {key=word:Value=embeddings vector}\n",
        "    \"\"\"\n",
        "    with open(path, \"r\", encoding=\"UTF-8\") as f:\n",
        "            lines = f.readlines()\n",
        "    p = multiprocessing.Pool()\n",
        "    result = p.map(get_lines_gloves, lines)\n",
        "    p.close()\n",
        "    p.join()\n",
        "    p.terminate()\n",
        "    return dict(result)\n",
        "\n",
        "def get_plong_doc(doc, embeddings_dict, len_vec_emb):\n",
        "    \"\"\"\n",
        "    this functions takes in:\n",
        "    doc: a string representing a doc in the corpus ex:'il est'\n",
        "    embeddings_dict: a dict {key=word:Value=embeddings}\n",
        "    len_vec_emb: the length of the embedding vector (d)\n",
        "    return an embedding vector for the doc \n",
        "    this result is the mean of the vector embedding of each word\n",
        "    \"\"\"\n",
        "    vectorizer = CountVectorizer()\n",
        "    temp_ = vectorizer.fit([doc]).vocabulary_\n",
        "    vec = np.zeros(len_vec_emb, dtype=float)\n",
        "    for word in temp_.keys():\n",
        "        vec += (embeddings_dict.get(word, 0) * temp_[word])\n",
        "    return vec / sum(temp_.values())\n",
        "\n",
        "def get_plong_corpus(corpus, embeddings_dict):\n",
        "    \"\"\"\n",
        "    his functions takes in:\n",
        "    corpus: ['je vais' 'il est']a list of strings representing the corpus. each string in the list is document in the corpus\n",
        "    embeddings_dict: a dict {key=word:Value=embeddings}\n",
        "    return a list of embedding vector [] each vector is the embedding vector for a doc\n",
        "    \"\"\"\n",
        "    p = multiprocessing.Pool()\n",
        "    result = p.map(partial(get_plong_doc, embeddings_dict=embeddings_dict, len_vec_emb=len(list(embeddings_dict.items())[0][1])), corpus)\n",
        "    p.close()\n",
        "    p.join()\n",
        "    p.terminate()\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Pphx1xmi4hP"
      },
      "source": [
        "path = \"/content/drive/My Drive/Colab Notebooks/INF8460/Project/output/corpus.csv\"\n",
        "datat = read_from_csv(path)\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit(datat[1]).vocabulary_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7JqLpSjjIJ5"
      },
      "source": [
        "glove_dict = get_gloves_dict()\n",
        "key_set = set(X.keys()) & set(glove_dict.keys())\n",
        "glove_dict_vocab_corpus = {key: glove_dict[key] for key in key_set}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZojYC8ajNn7"
      },
      "source": [
        "plongement_doc = get_plong_corpus(datat[1], glove_dict_vocab_corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDGfEFkqDaBy"
      },
      "source": [
        "### Plongements contextuels prÃ©-entraÃ®nÃ©s / non prÃ©-entraÃ®nÃ©s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdpH4bcTGFd5"
      },
      "source": [
        "\n",
        "\n",
        "> #### Huggingface ready pipeline\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoAa99LV85Ug",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290,
          "referenced_widgets": [
            "6984723399ad4429a78be66fead09207",
            "4792c5db1d1045f98440ef4ca34b433c",
            "30d4f585019e465a94958402d26bde1d",
            "1d9e9b5dbe094c33a3176780ff37b9f1",
            "c135251c65474c4b9d8df448cea1be6e",
            "9ef19809a9c147ec83e5c7e94c555c9c",
            "27e7045593af4d63a4c1fdd31e719c20",
            "a6a0c7287e7b489ca1bb5d8100af8b99",
            "594581a8da644221a1c41a298ed4b39d",
            "0bfa0231a34040e6b252629f85399b59",
            "8ac6053a92c24954b1d2e67edadebd65",
            "a53cecd910c54965acbff9d5b1dbfc21",
            "82c5a18e88f94fd9bc745fde8a369c27",
            "b01380a6701f491e8412580ad0b01262",
            "9270001477b54b50a389f04c2406c6f4",
            "bf3a5271b73f452188b40e6602771780",
            "5edaed461fdc44219061b87f71dc4957",
            "0a6d742b0ccd4cd38f2e6510dc486b4b",
            "b4028828b1ea4f1689e406da40bf7adb",
            "ea8f52bc3b5f44c290833426acee4c89",
            "eecccd9cfb6d41a194556fee365c869c",
            "3086765b1fb942738e2717619ebadcf7",
            "8486f5fda8b4459d97968dabfd5a4f23",
            "bef3fd3b6ff44e99829190215b0778dc",
            "77fe4a29c05e447c94427d18d9aebeb7",
            "97e283ae201e4dbdb65042da24e0594d",
            "2c19edbf8b08457090aebf43ce972e14",
            "3a201cc58a7d439783c14921b67f1b37",
            "c1cf65935503489fb2b7d0a6d744ed31",
            "53571776f9794f34a734db33c7e2a5dd",
            "a4c24addae744d1e8912ec698d2912c9",
            "23b914d2eea2417986b572301325955e"
          ]
        },
        "outputId": "2b093b39-008d-47cc-e5f0-ca10e4b8394e"
      },
      "source": [
        "question = \"How many parameters does BERT-large have?\"\n",
        "answer_text = r\"\"\"BERT-large is really big... it has 24-layers and an embedding size of 1,024, \n",
        "                  for a total of 340M parameters! Altogether it is 1.34GB, so expect it to take \n",
        "                  a couple minutes to download to your Colab instance.\"\"\"\n",
        "\n",
        "nlp = pipeline(\"question-answering\")\n",
        "result = nlp(question=question, context=answer_text)\n",
        "print(f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6984723399ad4429a78be66fead09207",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=473.0, style=ProgressStyle(description_â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "594581a8da644221a1c41a298ed4b39d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=260793700.0, style=ProgressStyle(descriâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5edaed461fdc44219061b87f71dc4957",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=213450.0, style=ProgressStyle(descriptiâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "77fe4a29c05e447c94427d18d9aebeb7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=230.0, style=ProgressStyle(description_â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1374: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Answer: '340M', score: 0.7121, start: 111, end: 115\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8j7a2v3hCsO"
      },
      "source": [
        "\n",
        "\n",
        "> #### DistilBERT SQuAD pre-trained\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0f_V0EC6GHZ5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b3a4dcf-f8ae-419d-f6aa-7c87f3be32d6"
      },
      "source": [
        "questions = [\"How many parameters does BERT-large have?\"]\n",
        "answer_text = r\"\"\"BERT-large is really big... it has 24-layers and an embedding size of 1,024, \n",
        "                  for a total of 340M parameters! Altogether it is 1.34GB, so expect it to take \n",
        "                  a couple minutes to download to your Colab instance.\"\"\"\n",
        "\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(\"distilbert-base-cased-distilled-squad\", \n",
        "                                                      return_dict=True, output_hidden_states = True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased-distilled-squad\", return_dict=True)\n",
        "\n",
        "for question in questions:\n",
        "    inputs = tokenizer(question, answer_text, add_special_tokens=True, return_tensors=\"pt\")\n",
        "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
        "\n",
        "    text_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "\n",
        "    # display tokens and ids\n",
        "    for token, id in zip(text_tokens, input_ids):\n",
        "        if id == tokenizer.sep_token_id:\n",
        "            print('')\n",
        "        print('{:<12} {:>6,}'.format(token, id))\n",
        "        if id == tokenizer.sep_token_id:\n",
        "            print('')\n",
        "\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "    last_hidden_states = outputs.hidden_states[-1]\n",
        "    print(last_hidden_states.shape)\n",
        "\n",
        "    answer_start = torch.argmax(outputs.start_logits)\n",
        "    answer_end = torch.argmax(outputs.end_logits) + 1\n",
        "\n",
        "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
        "\n",
        "    print(f\"Question: {question}\")\n",
        "    print(f\"Answer: {answer}, start: {answer_start}, end: {answer_end}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[CLS]           101\n",
            "How           1,731\n",
            "many          1,242\n",
            "parameters   11,934\n",
            "does          1,674\n",
            "B               139\n",
            "##ER          9,637\n",
            "##T           1,942\n",
            "-               118\n",
            "large         1,415\n",
            "have          1,138\n",
            "?               136\n",
            "\n",
            "[SEP]           102\n",
            "\n",
            "B               139\n",
            "##ER          9,637\n",
            "##T           1,942\n",
            "-               118\n",
            "large         1,415\n",
            "is            1,110\n",
            "really        1,541\n",
            "big           1,992\n",
            ".               119\n",
            ".               119\n",
            ".               119\n",
            "it            1,122\n",
            "has           1,144\n",
            "24            1,572\n",
            "-               118\n",
            "layers        8,798\n",
            "and           1,105\n",
            "an            1,126\n",
            "em            9,712\n",
            "##bed         4,774\n",
            "##ding        3,408\n",
            "size          2,060\n",
            "of            1,104\n",
            "1               122\n",
            ",               117\n",
            "02            5,507\n",
            "##4           1,527\n",
            ",               117\n",
            "for           1,111\n",
            "a               170\n",
            "total         1,703\n",
            "of            1,104\n",
            "340          16,984\n",
            "##M           2,107\n",
            "parameters   11,934\n",
            "!               106\n",
            "Alto         17,762\n",
            "##get        16,609\n",
            "##her         4,679\n",
            "it            1,122\n",
            "is            1,110\n",
            "1               122\n",
            ".               119\n",
            "34            3,236\n",
            "##GB         13,745\n",
            ",               117\n",
            "so            1,177\n",
            "expect        5,363\n",
            "it            1,122\n",
            "to            1,106\n",
            "take          1,321\n",
            "a               170\n",
            "couple        2,337\n",
            "minutes       1,904\n",
            "to            1,106\n",
            "download      9,133\n",
            "to            1,106\n",
            "your          1,240\n",
            "Cola         17,492\n",
            "##b           1,830\n",
            "instance      5,374\n",
            ".               119\n",
            "\n",
            "[SEP]           102\n",
            "\n",
            "torch.Size([1, 76, 768])\n",
            "Question: How many parameters does BERT-large have?\n",
            "Answer: 340M, start: 45, end: 47\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FEyx474lvfY"
      },
      "source": [
        "> #### BERT base pre-trained"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxtmBinzK9P5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ca3b65e-be99-419a-ce3b-c59370043e0b"
      },
      "source": [
        "torch.cuda.set_device(0)\n",
        "\n",
        "questions = [\"How many parameters does BERT-large have?\"]\n",
        "answer_text = r\"\"\"BERT-large is really big... it has 24-layers and an embedding size of 1,024, \n",
        "                  for a total of 340M parameters! Altogether it is 1.34GB, so expect it to take \n",
        "                  a couple minutes to download to your Colab instance.\"\"\"\n",
        "\n",
        "model = BertModel.from_pretrained(\"bert-base-cased\", return_dict=True)\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "# print(device)\n",
        "# # model = model.to(device)\n",
        "\n",
        "for question in questions:\n",
        "    inputs = tokenizer(question, return_tensors=\"pt\")\n",
        "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
        "\n",
        "    text_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "\n",
        "    # display tokens and ids\n",
        "    for token, id in zip(text_tokens, input_ids):\n",
        "        if id == tokenizer.sep_token_id:\n",
        "            print('')\n",
        "        print('{:<12} {:>6,}'.format(token, id))\n",
        "        if id == tokenizer.sep_token_id:\n",
        "            print('')\n",
        "\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "    last_hidden_states = outputs.last_hidden_state\n",
        "    print(last_hidden_states.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[CLS]           101\n",
            "How           1,731\n",
            "many          1,242\n",
            "parameters   11,934\n",
            "does          1,674\n",
            "B               139\n",
            "##ER          9,637\n",
            "##T           1,942\n",
            "-               118\n",
            "large         1,415\n",
            "have          1,138\n",
            "?               136\n",
            "\n",
            "[SEP]           102\n",
            "\n",
            "torch.Size([1, 13, 768])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOL6YSl_l1Af"
      },
      "source": [
        "> #### DistilBERT SQuAD training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7ZSxgj2LEvb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsAA2F1kDu0P"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## **2. Ordonnancement**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-rv-pd-D6IM"
      },
      "source": [
        "\n",
        "\n",
        "> #### cosine similarity\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AS6psQWBIZpE"
      },
      "source": [
        "def voisins(word, df, n, distfunc=cosine):\n",
        "    assert distfunc.__name__ == 'cosine' or distfunc.__name__ == 'euclidean', \"distance metric not supported\"\n",
        "    order = True if distfunc.__name__ == 'euclidean' else False\n",
        "\n",
        "    closest = {}\n",
        "    for w in df:\n",
        "        distance = distfunc(word, df[w])\n",
        "        closest[w] = distance\n",
        "\n",
        "    closest = {k: v for k, v in sorted(closest.items(), key=lambda item: item[1], reverse=order)}\n",
        "\n",
        "    return list(closest.keys())[:n], list(closest.values())[:n]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5GbmL_aSYwD"
      },
      "source": [
        "# paragraphs_vocab = buildVocab(paragraphs_text)\n",
        "questions_vocab = buildVocab(questions_text)\n",
        "\n",
        "questions_ids = train_ids[0]\n",
        "vectorizer = TfidfVectorizer(max_features=500) # vocabulary=questions_vocab\n",
        "questions_tfidf = getTfIdfReprentation(questions_text, vectorizer)\n",
        "\n",
        "dic_questions = {}\n",
        "for i, ids in enumerate(questions) :\n",
        "    dic_questions[questions_ids[i]] = questions_tfidf[i]\n",
        "\n",
        "#questions_tfidf_r = sklearn_svd(questions_tfidf, len(embeddings))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxrCn0Nh_FFh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "330aa3d0-7085-46e4-b31e-afb551b11777"
      },
      "source": [
        "start = time.time()\n",
        "\n",
        "ranking_list = {}\n",
        "for i in range(len(test_questions_text)):\n",
        "    new_question = test_questions_text[i]\n",
        "    new_question_tokenized = pre.preprocess_pipeline([new_question])\n",
        "    new_question_text = [\" \".join(sentence) for sentence in new_question_tokenized]\n",
        "    \n",
        "    new_question_tfidf = vectorizer.transform(new_question_text).todense()\n",
        "\n",
        "    topk_ids, topk_questions = voisins(new_question_tfidf, dic_questions, 3, distfunc=cosine)\n",
        "    print(topk_ids, topk_questions)\n",
        "\n",
        "    ranking_list[i] = topk_ids\n",
        "\n",
        "print(time.time() - start)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/spatial/distance.py:720: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[3736, 9277, 663] [0.21815962598654437, 0.3765213479496673, 0.5667729562836451]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/spatial/distance.py:720: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[549, 623, 1404] [0.0, 0.0, 0.0]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/spatial/distance.py:720: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[2927, 25752, 558] [0.28044591097594374, 0.28044591097594374, 0.4304412417979788]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/spatial/distance.py:720: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[964, 2990, 22204] [0.0, 0.0, 0.0]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/spatial/distance.py:720: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[10171, 1710, 67] [0.3970612842870358, 0.5506789894166402, 0.5716624108482339]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/spatial/distance.py:720: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[331, 2920, 1074] [0.5369323027345263, 0.5369715647490179, 0.6235697311139214]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/spatial/distance.py:720: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[85, 5948, 96348] [0.21686895949687934, 0.21686895949687934, 0.3258161538570762]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/spatial/distance.py:720: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1240, 3571, 17243] [0.0, 0.0, 0.0]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/spatial/distance.py:720: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[584, 247, 2] [0.38061932977127344, 0.466747743096798, 0.5520734461509238]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/spatial/distance.py:720: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[599, 5, 69] [0.6237818822519656, 0.6877793564683086, 0.7594926247939646]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/spatial/distance.py:720: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[3253, 6324, 19286] [0.0, 0.0, 0.0]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/spatial/distance.py:720: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-a714d48550b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mnew_question_tfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_question_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtopk_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopk_questions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvoisins\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_question_tfidf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdic_questions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcosine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopk_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopk_questions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-46-d53811c39928>\u001b[0m in \u001b[0;36mvoisins\u001b[0;34m(word, df, n, distfunc)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mclosest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mdistance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mclosest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/scipy/spatial/distance.py\u001b[0m in \u001b[0;36mcosine\u001b[0;34m(u, v, w)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;31m# cosine distance is also referred to as 'uncentered correlation',\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;31m#   or 'reflective correlation'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 766\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcorrelation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcentered\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    767\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/scipy/spatial/distance.py\u001b[0m in \u001b[0;36mcorrelation\u001b[0;34m(u, v, w, centered)\u001b[0m\n\u001b[1;32m    715\u001b[0m         \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mu\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mumu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m         \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mvmu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 717\u001b[0;31m     \u001b[0muv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    718\u001b[0m     \u001b[0muu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m     \u001b[0mvv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36maverage\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36maverage\u001b[0;34m(a, axis, weights, returned)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m         \u001b[0mavg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m         \u001b[0mscl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mavg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mavg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_float16_result\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dtype'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_float16_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mrcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOd9JHBwR_td"
      },
      "source": [
        "# save to file\n",
        "save_to_csv(\"test_cosine_sim_questions.csv\", ranking_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Ic4372DEFvR"
      },
      "source": [
        "\n",
        "\n",
        "> #### LambdaMART with lightgbm\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYzHFoo0d8aI"
      },
      "source": [
        "\n",
        "\n",
        "> #### LSTM Siamese text similarity\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4v49V-JIeAb_"
      },
      "source": [
        "# !git clone https://github.com/amansrivastava17/lstm-siamese-text-similarity.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPuQlVrweGQD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "092c6073-4e96-4ae7-b966-7f504c3cefca"
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/Colab Notebooks/INF8460/Project/LSTMSiameseTextSimilarity/')\n",
        "!wget https://github.com/brmson/dataset-sts/tree/master/data/sts/sick2014/SICK_train.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-11-14 22:43:07--  https://github.com/brmson/dataset-sts/tree/master/data/sts/sick2014/SICK_train.txt\n",
            "Resolving github.com (github.com)... 192.30.255.112\n",
            "Connecting to github.com (github.com)|192.30.255.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://github.com/brmson/dataset-sts/blob/master/data/sts/sick2014/SICK_train.txt [following]\n",
            "--2020-11-14 22:43:08--  https://github.com/brmson/dataset-sts/blob/master/data/sts/sick2014/SICK_train.txt\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜SICK_train.txt.1â€™\n",
            "\n",
            "\rSICK_train.txt.1        [<=>                 ]       0  --.-KB/s               \rSICK_train.txt.1        [ <=>                ]   1.36M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2020-11-14 22:43:08 (30.3 MB/s) - â€˜SICK_train.txt.1â€™ saved [1430770]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Cdba8HkeeJ2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "851d84e1-96c2-4f52-9d5c-585a65a9762d"
      },
      "source": [
        "from model import SiameseBiLSTM\n",
        "from inputHandler import word_embed_meta_data, create_test_data\n",
        "from config import siamese_config\n",
        "import pandas as pd\n",
        "\n",
        "############ Data Preperation ##########\n",
        "\n",
        "df = pd.read_csv('lstm-siamese-text-similarity/sample_data.csv')\n",
        "\n",
        "sentences1 = list(df['sentences1'])\n",
        "sentences2 = list(df['sentences2'])\n",
        "is_similar = list(df['is_similar'])\n",
        "del df\n",
        "\n",
        "######## Word Embedding ############\n",
        "\n",
        "tokenizer, embedding_matrix = word_embed_meta_data(sentences1 + sentences2,  siamese_config['EMBEDDING_DIM'])\n",
        "\n",
        "embedding_meta_data = {\n",
        "\t'tokenizer': tokenizer,\n",
        "\t'embedding_matrix': embedding_matrix\n",
        "}\n",
        "\n",
        "## creating sentence pairs\n",
        "sentences_pair = [(x1, x2) for x1, x2 in zip(sentences1, sentences2)]\n",
        "del sentences1\n",
        "del sentences2\n",
        "\n",
        "######## Training ########\n",
        "\n",
        "class Configuration(object):\n",
        "    \"\"\"Dump stuff here\"\"\"\n",
        "\n",
        "CONFIG = Configuration()\n",
        "\n",
        "CONFIG.embedding_dim = siamese_config['EMBEDDING_DIM']\n",
        "CONFIG.max_sequence_length = siamese_config['MAX_SEQUENCE_LENGTH']\n",
        "CONFIG.number_lstm_units = siamese_config['NUMBER_LSTM']\n",
        "CONFIG.rate_drop_lstm = siamese_config['RATE_DROP_LSTM']\n",
        "CONFIG.number_dense_units = siamese_config['NUMBER_DENSE_UNITS']\n",
        "CONFIG.activation_function = siamese_config['ACTIVATION_FUNCTION']\n",
        "CONFIG.rate_drop_dense = siamese_config['RATE_DROP_DENSE']\n",
        "CONFIG.validation_split_ratio = siamese_config['VALIDATION_SPLIT']\n",
        "\n",
        "siamese = SiameseBiLSTM(CONFIG.embedding_dim , CONFIG.max_sequence_length, CONFIG.number_lstm_units , CONFIG.number_dense_units, CONFIG.rate_drop_lstm, CONFIG.rate_drop_dense, CONFIG.activation_function, CONFIG.validation_split_ratio)\n",
        "\n",
        "best_model_path = siamese.train_model(sentences_pair, is_similar, embedding_meta_data, model_save_directory='./')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Embedding matrix shape: (3052, 50)\n",
            "Null word embeddings: 1\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "Epoch 1/200\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.9770 - acc: 0.4219WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
            "Instructions for updating:\n",
            "use `tf.profiler.experimental.stop` instead.\n",
            "2/8 [======>.......................] - ETA: 3s - loss: 0.9832 - acc: 0.5000WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.2696s vs `on_train_batch_end` time: 0.9156s). Check your callbacks.\n",
            "8/8 [==============================] - 3s 382ms/step - loss: 0.8995 - acc: 0.5067 - val_loss: 0.7467 - val_acc: 0.3469\n",
            "Epoch 2/200\n",
            "8/8 [==============================] - 1s 149ms/step - loss: 0.8755 - acc: 0.5022 - val_loss: 0.7458 - val_acc: 0.4694\n",
            "Epoch 3/200\n",
            "8/8 [==============================] - 1s 156ms/step - loss: 0.7893 - acc: 0.5489 - val_loss: 0.7412 - val_acc: 0.4898\n",
            "Epoch 4/200\n",
            "8/8 [==============================] - 1s 141ms/step - loss: 0.7148 - acc: 0.6111 - val_loss: 0.7554 - val_acc: 0.4898\n",
            "Epoch 5/200\n",
            "8/8 [==============================] - 1s 171ms/step - loss: 0.6849 - acc: 0.6378 - val_loss: 0.7551 - val_acc: 0.4898\n",
            "Epoch 6/200\n",
            "8/8 [==============================] - 1s 152ms/step - loss: 0.7482 - acc: 0.5822 - val_loss: 0.7465 - val_acc: 0.4898\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDwJAlO1J_cw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a794331d-1eb8-4b6e-9db3-fa031c43d507"
      },
      "source": [
        "######## Testing ########\n",
        "\n",
        "from operator import itemgetter\n",
        "from keras.models import load_model\n",
        "\n",
        "model = load_model(best_model_path)\n",
        "\n",
        "test_sentence_pairs = [('What can make Physics easy to learn?','How can you make physics easy to learn?'),('How many times a day do a clocks hands overlap?','What does it mean that every time I look at the clock the numbers are the same?')]\n",
        "\n",
        "test_data_x1, test_data_x2, leaks_test = create_test_data(tokenizer,test_sentence_pairs,  siamese_config['MAX_SEQUENCE_LENGTH'])\n",
        "\n",
        "preds = list(model.predict([test_data_x1, test_data_x2, leaks_test], verbose=1).ravel())\n",
        "results = [(x, y, z) for (x, y), z in zip(test_sentence_pairs, preds)]\n",
        "results.sort(key=itemgetter(2), reverse=True)\n",
        "print(results)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "1/1 [==============================] - 0s 2ms/step\n",
            "[('What can make Physics easy to learn?', 'How can you make physics easy to learn?', 0.39372748), ('How many times a day do a clocks hands overlap?', 'What does it mean that every time I look at the clock the numbers are the same?', 0.169769)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrJjJW5ZOqOX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f02FORmkLEzj"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## **3. Extraction de reponse**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EN46aA0ALL-a"
      },
      "source": [
        "\n",
        "\n",
        "> #### Pytorch BERT squad 2.0\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "woIK3xzCLLSR",
        "outputId": "8d723460-1673-4efe-9566-5716272386ad"
      },
      "source": [
        "!git clone https://github.com/surbhardwaj/BERT-QnA-Squad_2.0_Finetuned_Model.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'BERT-QnA-Squad_2.0_Finetuned_Model'...\n",
            "remote: Enumerating objects: 60, done.\u001b[K\n",
            "remote: Total 60 (delta 0), reused 0 (delta 0), pack-reused 60\u001b[K\n",
            "Unpacking objects: 100% (60/60), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjYXIkGxPzdz"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import argparse\n",
        "from pytorch_pretrained_bert.tokenization import (BasicTokenizer,\n",
        "                                                  BertTokenizer,whitespace_tokenize)\n",
        "import collections\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset\n",
        "from pytorch_pretrained_bert.modeling import BertForQuestionAnswering, BertConfig\n",
        "import math\n",
        "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
        "                              TensorDataset)\n",
        "from tqdm import tqdm\n",
        "from termcolor import colored, cprint\n",
        "\n",
        "\n",
        "class SquadExample(object):\n",
        "    \"\"\"\n",
        "    A single training/test example for the Squad dataset.\n",
        "    For examples without an answer, the start and end position are -1.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 example_id,\n",
        "                 para_text,\n",
        "                 qas_id,\n",
        "                 question_text,\n",
        "                 doc_tokens,\n",
        "                unique_id):\n",
        "        self.qas_id = qas_id\n",
        "        self.question_text = question_text\n",
        "        self.doc_tokens = doc_tokens\n",
        "        self.example_id = example_id\n",
        "        self.para_text = para_text\n",
        "        self.unique_id = unique_id\n",
        "        \n",
        "\n",
        "    def __str__(self):\n",
        "        return self.__repr__()\n",
        "\n",
        "    def __repr__(self):\n",
        "        s = \"\"\n",
        "        s += \"qas_id: %s\" % (self.qas_id)\n",
        "        s += \", question_text: %s\" % (\n",
        "            self.question_text)\n",
        "        s += \", doc_tokens: [%s]\" % (\" \".join(self.doc_tokens))\n",
        "        \n",
        "        return s\n",
        "\n",
        "\n",
        "\n",
        "### Convert paragraph to tokens and returns question_text\n",
        "def read_squad_examples(input_data):\n",
        "    \"\"\"Read a SQuAD json file into a list of SquadExample.\"\"\"\n",
        "    def is_whitespace(c):\n",
        "        if c == \" \" or c == \"\\t\" or c == \"\\r\" or c == \"\\n\" or ord(c) == 0x202F:\n",
        "            return True\n",
        "        return False\n",
        "    i = 0\n",
        "    examples = []\n",
        "    for entry in input_data:\n",
        "        example_id = entry['id']\n",
        "        paragraph_text = entry['text']\n",
        "        doc_tokens = []\n",
        "        prev_is_whitespace = True\n",
        "        for c in paragraph_text:\n",
        "            if is_whitespace(c):\n",
        "                prev_is_whitespace = True\n",
        "            else:\n",
        "                if prev_is_whitespace:\n",
        "                    doc_tokens.append(c)\n",
        "                else:\n",
        "                    doc_tokens[-1] += c\n",
        "                prev_is_whitespace = False\n",
        "            \n",
        "        for qa in entry['ques']:\n",
        "            qas_id = i\n",
        "            question_text = qa\n",
        "            \n",
        "            \n",
        "            example = SquadExample(example_id = example_id,\n",
        "                    qas_id=qas_id,\n",
        "                    para_text = paragraph_text,               \n",
        "                    question_text=question_text,\n",
        "                    doc_tokens=doc_tokens,\n",
        "                    unique_id = i)\n",
        "            i+=1\n",
        "            examples.append(example)\n",
        "\n",
        "    return examples\n",
        "\n",
        "\n",
        "\n",
        "def _check_is_max_context(doc_spans, cur_span_index, position):\n",
        "    \"\"\"Check if this is the 'max context' doc span for the token.\"\"\"\n",
        "    best_score = None\n",
        "    best_span_index = None\n",
        "    for (span_index, doc_span) in enumerate(doc_spans):\n",
        "        end = doc_span.start + doc_span.length - 1\n",
        "        if position < doc_span.start:\n",
        "            continue\n",
        "        if position > end:\n",
        "            continue\n",
        "        num_left_context = position - doc_span.start\n",
        "        num_right_context = end - position\n",
        "        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n",
        "        if best_score is None or score > best_score:\n",
        "            best_score = score\n",
        "            best_span_index = span_index\n",
        "\n",
        "    return cur_span_index == best_span_index\n",
        "\n",
        "class InputFeatures(object):\n",
        "    \"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 unique_id,\n",
        "                 example_index,\n",
        "                 doc_span_index,\n",
        "                 tokens,\n",
        "                 token_is_max_context,\n",
        "                 token_to_orig_map,\n",
        "                 input_ids,\n",
        "                 input_mask,\n",
        "                 segment_ids):\n",
        " \n",
        "        self.doc_span_index = doc_span_index\n",
        "        self.unique_id = unique_id\n",
        "        self.example_index = example_index\n",
        "        self.tokens = tokens\n",
        "        self.token_is_max_context = token_is_max_context\n",
        "        self.token_to_orig_map = token_to_orig_map\n",
        "        self.input_ids = input_ids\n",
        "        self.input_mask = input_mask\n",
        "        self.segment_ids = segment_ids\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def convert_examples_to_features(examples, tokenizer, max_seq_length,\n",
        "                                 doc_stride, max_query_length):\n",
        "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
        "\n",
        "\n",
        "    features = []\n",
        "    unique_id = 1\n",
        "    for (example_index, example) in enumerate(examples):\n",
        "        query_tokens = tokenizer.tokenize(example.question_text)\n",
        "        ### Truncate the query if query length > max_query_length..\n",
        "        if len(query_tokens) > max_query_length:\n",
        "            query_tokens = query_tokens[0:max_query_length]\n",
        "\n",
        "        tok_to_orig_index = []\n",
        "        orig_to_tok_index = []\n",
        "        all_doc_tokens = []\n",
        "        for (i, token) in enumerate(example.doc_tokens):\n",
        "            orig_to_tok_index.append(len(all_doc_tokens))\n",
        "            sub_tokens = tokenizer.tokenize(token)\n",
        "            for sub_token in sub_tokens:\n",
        "                tok_to_orig_index.append(i)\n",
        "                all_doc_tokens.append(sub_token)\n",
        "\n",
        "        tok_start_position = None\n",
        "        tok_end_position = None\n",
        "\n",
        "        max_tokens_for_doc = max_seq_length - len(query_tokens) - 3\n",
        "    \n",
        "\n",
        "        # We can have documents that are longer than the maximum sequence length.\n",
        "        # To deal with this we do a sliding window approach, where we take chunks\n",
        "        # of the up to our max length with a stride of `doc_stride`.\n",
        "        _DocSpan = collections.namedtuple(  # pylint: disable=invalid-name\n",
        "            \"DocSpan\", [\"start\", \"length\"])\n",
        "        doc_spans = []\n",
        "        start_offset = 0\n",
        "        while start_offset < len(all_doc_tokens):\n",
        "            length = len(all_doc_tokens) - start_offset\n",
        "            if length > max_tokens_for_doc:\n",
        "                length = max_tokens_for_doc\n",
        "            doc_spans.append(_DocSpan(start=start_offset, length=length))\n",
        "            if start_offset + length == len(all_doc_tokens):\n",
        "                break\n",
        "            start_offset += min(length, doc_stride)\n",
        "\n",
        "        for (doc_span_index, doc_span) in enumerate(doc_spans):\n",
        "            tokens = []\n",
        "            token_to_orig_map = {}\n",
        "            token_is_max_context = {}\n",
        "            segment_ids = []\n",
        "            tokens.append(\"[CLS]\")\n",
        "            segment_ids.append(0)\n",
        "            for token in query_tokens:\n",
        "                tokens.append(token)\n",
        "                segment_ids.append(0)\n",
        "            tokens.append(\"[SEP]\")\n",
        "            segment_ids.append(0)\n",
        "\n",
        "            for i in range(doc_span.length):\n",
        "                split_token_index = doc_span.start + i\n",
        "                token_to_orig_map[len(tokens)] = tok_to_orig_index[split_token_index]\n",
        "\n",
        "                is_max_context = _check_is_max_context(doc_spans, doc_span_index,\n",
        "                                                       split_token_index)\n",
        "                token_is_max_context[len(tokens)] = is_max_context\n",
        "                tokens.append(all_doc_tokens[split_token_index])\n",
        "                segment_ids.append(1)\n",
        "            tokens.append(\"[SEP]\")\n",
        "            segment_ids.append(1)\n",
        "\n",
        "\n",
        "            input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "                # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "                # tokens are attended to.\n",
        "            input_mask = [1] * len(input_ids)\n",
        "\n",
        "            # Zero-pad up to the sequence length.\n",
        "            while len(input_ids) < max_seq_length:\n",
        "                input_ids.append(0)\n",
        "                input_mask.append(0)\n",
        "                segment_ids.append(0)\n",
        "\n",
        "            assert len(input_ids) == max_seq_length\n",
        "            assert len(input_mask) == max_seq_length\n",
        "            assert len(segment_ids) == max_seq_length\n",
        "\n",
        "            \n",
        "            features.append(InputFeatures(unique_id = unique_id,\n",
        "                            example_index = example_index,\n",
        "                            doc_span_index=doc_span_index,\n",
        "                            tokens=tokens,   \n",
        "                            token_is_max_context=token_is_max_context,\n",
        "                            token_to_orig_map=token_to_orig_map,\n",
        "                            input_ids=input_ids,\n",
        "                            input_mask=input_mask,\n",
        "                            segment_ids=segment_ids))\n",
        "            unique_id += 1\n",
        "\n",
        "            \n",
        "    \n",
        "    return features\n",
        "\n",
        "\n",
        "def _get_best_indexes(logits, n_best_size):\n",
        "    \"\"\"Get the n-best logits from a list.\"\"\"\n",
        "    index_and_score = sorted(enumerate(logits), key=lambda x: x[1], reverse=True)\n",
        "    best_indexes = []\n",
        "    for i in range(len(index_and_score)):\n",
        "        if i >= n_best_size:\n",
        "            break\n",
        "        best_indexes.append(index_and_score[i][0])\n",
        "   \n",
        "    return best_indexes\n",
        "\n",
        "\n",
        "\n",
        "def get_final_text(pred_text, orig_text, do_lower_case, verbose_logging=False):\n",
        "    \"\"\"Project the tokenized prediction back to the original text.\"\"\"\n",
        "\n",
        "    def _strip_spaces(text):\n",
        "        ns_chars = []\n",
        "        ns_to_s_map = collections.OrderedDict()\n",
        "        for (i, c) in enumerate(text):\n",
        "            if c == \" \":\n",
        "                continue\n",
        "            ns_to_s_map[len(ns_chars)] = i\n",
        "            ns_chars.append(c)\n",
        "        ns_text = \"\".join(ns_chars)\n",
        "        return (ns_text, ns_to_s_map)\n",
        "\n",
        "    \n",
        "    tokenizer = BasicTokenizer(do_lower_case=do_lower_case)\n",
        "\n",
        "    tok_text = \" \".join(tokenizer.tokenize(orig_text))\n",
        "\n",
        "    start_position = tok_text.find(pred_text)\n",
        "    if start_position == -1:\n",
        "        if verbose_logging:\n",
        "            logger.info(\n",
        "                \"Unable to find text: '%s' in '%s'\" % (pred_text, orig_text))\n",
        "            print(\"no answer\")\n",
        "        return orig_text\n",
        "    end_position = start_position + len(pred_text) - 1\n",
        "\n",
        "    (orig_ns_text, orig_ns_to_s_map) = _strip_spaces(orig_text)\n",
        "    (tok_ns_text, tok_ns_to_s_map) = _strip_spaces(tok_text)\n",
        "\n",
        "    if len(orig_ns_text) != len(tok_ns_text):\n",
        "        if verbose_logging:\n",
        "            logger.info(\"Length not equal after stripping spaces: '%s' vs '%s'\",\n",
        "                        orig_ns_text, tok_ns_text)\n",
        "        return orig_text\n",
        "\n",
        "    # We then project the characters in `pred_text` back to `orig_text` using\n",
        "    # the character-to-character alignment.\n",
        "    tok_s_to_ns_map = {}\n",
        "    for (i, tok_index) in tok_ns_to_s_map.items():\n",
        "        tok_s_to_ns_map[tok_index] = i\n",
        "\n",
        "    orig_start_position = None\n",
        "    if start_position in tok_s_to_ns_map:\n",
        "        ns_start_position = tok_s_to_ns_map[start_position]\n",
        "        if ns_start_position in orig_ns_to_s_map:\n",
        "            orig_start_position = orig_ns_to_s_map[ns_start_position]\n",
        "\n",
        "    if orig_start_position is None:\n",
        "        if verbose_logging:\n",
        "            logger.info(\"Couldn't map start position\")\n",
        "        return orig_text\n",
        "\n",
        "    orig_end_position = None\n",
        "    if end_position in tok_s_to_ns_map:\n",
        "        ns_end_position = tok_s_to_ns_map[end_position]\n",
        "        if ns_end_position in orig_ns_to_s_map:\n",
        "            orig_end_position = orig_ns_to_s_map[ns_end_position]\n",
        "\n",
        "    if orig_end_position is None:\n",
        "        if verbose_logging:\n",
        "            logger.info(\"Couldn't map end position\")\n",
        "        return orig_text\n",
        "\n",
        "    output_text = orig_text[orig_start_position:(orig_end_position + 1)]\n",
        "    return output_text\n",
        "\n",
        "    \n",
        "    \n",
        "_PrelimPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n",
        "        \"PrelimPrediction\",\n",
        "[\"feature_index\", \"start_index\", \"end_index\", \"start_logit\", \"end_logit\"])\n",
        "\n",
        "\n",
        "def _compute_softmax(scores):\n",
        "    \"\"\"Compute softmax probability over raw logits.\"\"\"\n",
        "    if not scores:\n",
        "        return []\n",
        "\n",
        "    max_score = None\n",
        "    for score in scores:\n",
        "        if max_score is None or score > max_score:\n",
        "            max_score = score\n",
        "\n",
        "    exp_scores = []\n",
        "    total_sum = 0.0\n",
        "    for score in scores:\n",
        "        x = math.exp(score - max_score)\n",
        "        exp_scores.append(x)\n",
        "        total_sum += x\n",
        "\n",
        "    probs = []\n",
        "    for score in exp_scores:\n",
        "        probs.append(score / total_sum)\n",
        "    return probs\n",
        "\n",
        "\n",
        "_NbestPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n",
        "    \"NbestPrediction\", [\"text\", \"start_logit\", \"end_logit\"])\n",
        "\n",
        "def predict(examples, all_features, all_results, max_answer_length, thresh):\n",
        "\n",
        "    n_best_size = 10\n",
        "    \n",
        "    ### Adding index to feature ###\n",
        "    example_index_to_features = collections.defaultdict(list)\n",
        "    for feature in all_features:\n",
        "        example_index_to_features[feature.example_index].append(feature)\n",
        "     \n",
        "    unique_id_to_result = {}\n",
        "    for result in all_results:\n",
        "        unique_id_to_result[result.unique_id] = result\n",
        "        \n",
        "        \n",
        "    all_predictions = collections.OrderedDict()\n",
        "   \n",
        "    \n",
        "    \n",
        "    for example in examples:\n",
        "        index = 0\n",
        "        features = example_index_to_features[example.unique_id]\n",
        "        prelim_predictions = []\n",
        "       \n",
        "        for (feature_index, feature) in enumerate(features):\n",
        "            result = unique_id_to_result[feature.unique_id]\n",
        "            start_indexes = _get_best_indexes(result.start_logits, n_best_size)\n",
        "            end_indexes = _get_best_indexes(result.end_logits, n_best_size)\n",
        "            for start_index in start_indexes:\n",
        "                    for end_index in end_indexes:\n",
        "                     #### we remove the indexes which are invalid @\n",
        "                        if start_index >= len(feature.tokens):\n",
        "                            continue\n",
        "                        if end_index >= len(feature.tokens):\n",
        "\n",
        "                            continue\n",
        "                        if start_index not in feature.token_to_orig_map:\n",
        "                            continue\n",
        "                        if end_index not in feature.token_to_orig_map:\n",
        "                            continue\n",
        "                        if not feature.token_is_max_context.get(start_index, False):\n",
        "                            continue\n",
        "                        if end_index < start_index:\n",
        "                            continue\n",
        "                        length = end_index - start_index + 1\n",
        "                        if length > max_answer_length:\n",
        "                            continue\n",
        "\n",
        "                        prelim_predictions.append(\n",
        "                                        _PrelimPrediction(\n",
        "                                            feature_index=feature_index,\n",
        "                                            start_index=start_index,\n",
        "                                            end_index=end_index,\n",
        "                                            start_logit=result.start_logits[start_index],\n",
        "                                            end_logit=result.end_logits[end_index]))\n",
        "\n",
        "\n",
        "        prelim_predictions = sorted(\n",
        "            prelim_predictions,\n",
        "            key=lambda x: (x.start_logit + x.end_logit),\n",
        "            reverse=True) \n",
        "            \n",
        "    \n",
        "         \n",
        "        seen_predictions = {}\n",
        "        nbest = []\n",
        "        for pred in prelim_predictions:\n",
        "            if len(nbest) >= n_best_size:\n",
        "                break\n",
        "                \n",
        "            feature = features[pred.feature_index]\n",
        "            if pred.start_index > 0:  # this is a non-null prediction\n",
        "                tok_tokens = feature.tokens[pred.start_index:(pred.end_index + 1)]\n",
        "                orig_doc_start = feature.token_to_orig_map[pred.start_index]\n",
        "                orig_doc_end = feature.token_to_orig_map[pred.end_index]\n",
        "                orig_tokens = example.doc_tokens[orig_doc_start:(orig_doc_end + 1)]\n",
        "                tok_text = \" \".join(tok_tokens)\n",
        "\n",
        "                # De-tokenize WordPieces that have been split off.\n",
        "                tok_text = tok_text.replace(\" ##\", \"\")\n",
        "                tok_text = tok_text.replace(\"##\", \"\")\n",
        "\n",
        "                # Clean whitespace\n",
        "                tok_text = tok_text.strip()\n",
        "                tok_text = \" \".join(tok_text.split())\n",
        "                orig_text = \" \".join(orig_tokens)\n",
        "\n",
        "                final_text = get_final_text(tok_text, orig_text, True)\n",
        "                if final_text in seen_predictions:\n",
        "                    continue\n",
        "\n",
        "                seen_predictions[final_text] = True\n",
        "            else:\n",
        "                final_text = \"\"\n",
        "                seen_predictions[final_text] = True\n",
        "\n",
        "            nbest.append(\n",
        "                _NbestPrediction(\n",
        "                    text=final_text,\n",
        "                    start_logit=pred.start_logit,\n",
        "                    end_logit=pred.end_logit))\n",
        "        \n",
        "        \n",
        "    \n",
        "        if not nbest:\n",
        "                nbest.append(\n",
        "                    _NbestPrediction(text=\"empty\", start_logit=0.0, end_logit=0.0))\n",
        "\n",
        "        assert len(nbest) >= 1\n",
        "        \n",
        "        \n",
        "        total_scores = []\n",
        "        best_non_null_entry = None\n",
        "        for entry in nbest:\n",
        "            total_scores.append(entry.start_logit + entry.end_logit)\n",
        "            if not best_non_null_entry:\n",
        "                if entry.text:\n",
        "                    best_non_null_entry = entry\n",
        "\n",
        "    \n",
        "        probs = _compute_softmax(total_scores)\n",
        "        nbest_json = []\n",
        "        for (i, entry) in enumerate(nbest):\n",
        "\n",
        "            output = collections.OrderedDict()\n",
        "            output[\"text\"] = entry.text if probs[i] > thresh else \"<No Answer>\"\n",
        "            output[\"probability\"] = probs[i]\n",
        "            output[\"start_logit\"] = entry.start_logit if probs[i] > thresh else -1\n",
        "            output[\"end_logit\"] = entry.end_logit if probs[i] > thresh else -1\n",
        "            nbest_json.append(output)\n",
        "\n",
        "        assert len(nbest_json) >= 1\n",
        "        all_predictions[example] = nbest_json[0][\"text\"]\n",
        "        index=+1\n",
        "    return all_predictions\n",
        "        \n",
        "\n",
        "                \n",
        "\n",
        "RawResult = collections.namedtuple(\"RawResult\",\n",
        "                                   [\"unique_id\", \"start_logits\", \"end_logits\"])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def main(para_file=\"BERT-QnA-Squad_2.0_Finetuned_Model/Input_file.txt\"):\n",
        "    \n",
        "    # args = parser.parse_args()\n",
        "    model_path = \"/content/drive/My Drive/Colab Notebooks/INF8460/Project/pytorch_model.bin\"\n",
        "    \n",
        "    if torch.cuda.is_available:\n",
        "        print('GPU available')\n",
        "    else:\n",
        "        print('Please set GPU')\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available else \"cpu\")\n",
        "    n_gpu = torch.cuda.device_count()\n",
        "    \n",
        "    ### Raeding paragraph\n",
        "    f = open(para_file, 'r')\n",
        "    para = f.read()\n",
        "    f.close()\n",
        "    \n",
        "    ## Reading question\n",
        "#     f = open(ques_file, 'r')\n",
        "#     ques = f.read()\n",
        "#     f.close()\n",
        "    \n",
        "    para_list = para.split('\\n\\n')\n",
        "    \n",
        "    input_data = []\n",
        "    i = 1\n",
        "    for para in para_list :\n",
        "        paragraphs = {}\n",
        "        splits = para.split('\\nQuestions:')\n",
        "        paragraphs['id'] = i\n",
        "        paragraphs['text'] = splits[0].replace('Paragraph:', '').strip('\\n')\n",
        "        paragraphs['ques']=splits[1].lstrip('\\n').split('\\n')\n",
        "        input_data.append(paragraphs)\n",
        "        i+=1\n",
        "       \n",
        "    \n",
        "    ## input_data is a list of dictionary which has a paragraph and questions\n",
        "\n",
        "    \n",
        "    examples = read_squad_examples(input_data)\n",
        "    # print(examples)\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "    \n",
        "    \n",
        "    eval_features = convert_examples_to_features(\n",
        "            examples = examples,\n",
        "            tokenizer=tokenizer,\n",
        "            max_seq_length=384,\n",
        "            doc_stride=128,\n",
        "            max_query_length=64)\n",
        "    \n",
        "    # print(eval_features)\n",
        "    \n",
        "    \n",
        "    all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n",
        "    all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n",
        "    all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n",
        "    all_example_index = torch.arange(all_input_ids.size(0), dtype=torch.long)\n",
        "    \n",
        "    ### Loading Pretrained model for QnA \n",
        "    config = BertConfig(\"BERT-QnA-Squad_2.0_Finetuned_Model//Results/bert_config.json\")\n",
        "    model = BertForQuestionAnswering(config)\n",
        "    model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
        "    model.to(device)\n",
        "   \n",
        "\n",
        "    pred_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_example_index)\n",
        "    # Run prediction for full data\n",
        "    pred_sampler = SequentialSampler(pred_data)\n",
        "    pred_dataloader = DataLoader(pred_data, sampler=pred_sampler, batch_size=9)\n",
        "    \n",
        "    predictions = []\n",
        "    for input_ids, input_mask, segment_ids, example_indices in tqdm(pred_dataloader):\n",
        "        input_ids = input_ids.to(device)\n",
        "        input_mask = input_mask.to(device)\n",
        "        segment_ids = segment_ids.to(device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            batch_start_logits, batch_end_logits = model(input_ids, segment_ids, input_mask)\n",
        "            \n",
        "    \n",
        "        features=[]\n",
        "        example = []\n",
        "        all_results = []\n",
        "       \n",
        "        for i, example_index in enumerate(example_indices):\n",
        "                start_logits = batch_start_logits[i].detach().cpu().tolist()\n",
        "                end_logits =   batch_end_logits[i].detach().cpu().tolist()\n",
        "                feature = eval_features[example_index.item()]\n",
        "                unique_id = int(feature.unique_id)\n",
        "                features.append(feature)\n",
        "                all_results.append(RawResult(unique_id=unique_id,\n",
        "                                             start_logits=start_logits,\n",
        "                                             end_logits=end_logits))\n",
        "                \n",
        "       \n",
        "        output = predict(examples, features, all_results, 30, 0.25)\n",
        "        print('Output:\\n')\n",
        "        print(output)\n",
        "        print('\\n')\n",
        "        predictions.append(output)\n",
        " \n",
        "   \n",
        "    ### For printing the results ####\n",
        "    index = None\n",
        "    for example in examples:\n",
        "        if index!= example.example_id:\n",
        "            # print(example.para_text)\n",
        "            index = example.example_id\n",
        "            print('\\n')\n",
        "            print(colored('***********Question and Answers *************', 'red'))\n",
        "          \n",
        "        ques_text = colored(example.question_text, 'blue')\n",
        "        print(ques_text)\n",
        "        prediction = colored(predictions[math.floor(example.unique_id/12)][example], 'green', attrs=['reverse', 'blink'])\n",
        "        print(prediction)\n",
        "        print('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PaxFAIIMP4eS",
        "outputId": "81e9ae9c-31fe-4929-9c62-4129d4aceace"
      },
      "source": [
        "main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU available\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  5.38it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Output:\n",
            "\n",
            "OrderedDict([(qas_id: 0, question_text: What is the formula of alkyl group?, doc_tokens: [Ethers are a class of organic compounds that contain an ether groupâ€”an oxygen atom connected to two alkyl or aryl groups. They have the general formula Râ€“Oâ€“Râ€², where R and Râ€² represent the alkyl or aryl groups. Ethers can again be classified into two varieties: if the alkyl groups are the same on both sides of the oxygen atom, then it is a simple or symmetrical ether, whereas if they are different, the ethers are called mixed or unsymmetrical ethers. A typical example of the first group is the solvent and anesthetic diethyl ether, commonly referred to simply as \"ether\" (CH3â€“CH2â€“Oâ€“CH2â€“CH3). Ethers are common in organic chemistry and even more prevalent in biochemistry, as they are common linkages in carbohydrates and lignin.], 'Râ€“Oâ€“Râ€²'), (qas_id: 1, question_text: What is symmetrical ether ?, doc_tokens: [Ethers are a class of organic compounds that contain an ether groupâ€”an oxygen atom connected to two alkyl or aryl groups. They have the general formula Râ€“Oâ€“Râ€², where R and Râ€² represent the alkyl or aryl groups. Ethers can again be classified into two varieties: if the alkyl groups are the same on both sides of the oxygen atom, then it is a simple or symmetrical ether, whereas if they are different, the ethers are called mixed or unsymmetrical ethers. A typical example of the first group is the solvent and anesthetic diethyl ether, commonly referred to simply as \"ether\" (CH3â€“CH2â€“Oâ€“CH2â€“CH3). Ethers are common in organic chemistry and even more prevalent in biochemistry, as they are common linkages in carbohydrates and lignin.], 'if the alkyl groups are the same on both sides of the oxygen atom'), (qas_id: 2, question_text: What does BERT achieves ?, doc_tokens: [BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE benchmark to 80.4% (7.6% absolute improvement), MultiNLI accuracy to 86.7% (5.6% absolute improvement) and the SQuAD v1.1 question answering Test F1 to 93.2 (1.5 absolute improvement), outperforming human performance by 2.0.], 'new state-of-the-art results'), (qas_id: 3, question_text: What is the BERT's accuracy on MultiNLI ?, doc_tokens: [BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE benchmark to 80.4% (7.6% absolute improvement), MultiNLI accuracy to 86.7% (5.6% absolute improvement) and the SQuAD v1.1 question answering Test F1 to 93.2 (1.5 absolute improvement), outperforming human performance by 2.0.], '86.7%'), (qas_id: 4, question_text: What is Bert's F1 score on Squad v1.1 ?, doc_tokens: [BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE benchmark to 80.4% (7.6% absolute improvement), MultiNLI accuracy to 86.7% (5.6% absolute improvement) and the SQuAD v1.1 question answering Test F1 to 93.2 (1.5 absolute improvement), outperforming human performance by 2.0.], '93.2')])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[31m***********Question and Answers *************\u001b[0m\n",
            "\u001b[34mWhat is the formula of alkyl group?\u001b[0m\n",
            "\u001b[5m\u001b[7m\u001b[32mRâ€“Oâ€“Râ€²\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[34mWhat is symmetrical ether ?\u001b[0m\n",
            "\u001b[5m\u001b[7m\u001b[32mif the alkyl groups are the same on both sides of the oxygen atom\u001b[0m\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[31m***********Question and Answers *************\u001b[0m\n",
            "\u001b[34mWhat does BERT achieves ?\u001b[0m\n",
            "\u001b[5m\u001b[7m\u001b[32mnew state-of-the-art results\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[34mWhat is the BERT's accuracy on MultiNLI ?\u001b[0m\n",
            "\u001b[5m\u001b[7m\u001b[32m86.7%\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[34mWhat is Bert's F1 score on Squad v1.1 ?\u001b[0m\n",
            "\u001b[5m\u001b[7m\u001b[32m93.2\u001b[0m\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBW9bndfP9K2",
        "outputId": "70360b66-ec7d-473c-a5de-9a8bf75b1ef3"
      },
      "source": [
        "main(\"sample_project.txt\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU available\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.60it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Output:\n",
            "\n",
            "OrderedDict([(qas_id: 0, question_text: Who heads an office or division?, doc_tokens: [In 1982, Regis McKenna was brought in to shape the marketing and launch of the Macintosh. Later the Regis McKenna team grew to include Jane Anderson, Katie Cadigan and Andy Cunningham, who eventually led the Apple account for the agency. Cunningham and Anderson were the primary authors of the Macintosh launch plan. The launch of the Macintosh pioneered many different tactics that are used today in launching technology products, including the \"multiple exclusive,\" event marketing (credited to John Sculley, who brought the concept over from Pepsi), creating a mystique around a product and giving an inside look into a product's creation.], 'Regis McKenna'), (qas_id: 1, question_text: Who leaders the sub-divisions of offices or divisions?, doc_tokens: [In 1982, Regis McKenna was brought in to shape the marketing and launch of the Macintosh. Later the Regis McKenna team grew to include Jane Anderson, Katie Cadigan and Andy Cunningham, who eventually led the Apple account for the agency. Cunningham and Anderson were the primary authors of the Macintosh launch plan. The launch of the Macintosh pioneered many different tactics that are used today in launching technology products, including the \"multiple exclusive,\" event marketing (credited to John Sculley, who brought the concept over from Pepsi), creating a mystique around a product and giving an inside look into a product's creation.], 'Regis McKenna'), (qas_id: 2, question_text: What helped Avicenna forget the Metaphysics of Aristotle?, doc_tokens: [In 1982, Regis McKenna was brought in to shape the marketing and launch of the Macintosh. Later the Regis McKenna team grew to include Jane Anderson, Katie Cadigan and Andy Cunningham, who eventually led the Apple account for the agency. Cunningham and Anderson were the primary authors of the Macintosh launch plan. The launch of the Macintosh pioneered many different tactics that are used today in launching technology products, including the \"multiple exclusive,\" event marketing (credited to John Sculley, who brought the concept over from Pepsi), creating a mystique around a product and giving an inside look into a product's creation.], '<No Answer>'), (qas_id: 3, question_text: Besides using 3kV DC what other power type is used in the former Soviet Union countries?, doc_tokens: [3 kV DC is used in Belgium, Italy, Spain, Poland, the northern Czech Republic, Slovakia, Slovenia, South Africa, Chile, and former Soviet Union countries (also using 25 kV 50 Hz AC). It was formerly used by the Milwaukee Road from Harlowton, Montana to Seattle-Tacoma, across the Continental Divide and including extensive branch and loop lines in Montana, and by the Delaware, Lackawanna & Western Railroad (now New Jersey Transit, converted to 25 kV AC) in the United States, and the Kolkata suburban railway (Bardhaman Main Line) in India, before it was converted to 25 kV 50 Hz AC.], '25 kV 50 Hz AC'), (qas_id: 4, question_text: What does the railway system of US use DC or AC?, doc_tokens: [3 kV DC is used in Belgium, Italy, Spain, Poland, the northern Czech Republic, Slovakia, Slovenia, South Africa, Chile, and former Soviet Union countries (also using 25 kV 50 Hz AC). It was formerly used by the Milwaukee Road from Harlowton, Montana to Seattle-Tacoma, across the Continental Divide and including extensive branch and loop lines in Montana, and by the Delaware, Lackawanna & Western Railroad (now New Jersey Transit, converted to 25 kV AC) in the United States, and the Kolkata suburban railway (Bardhaman Main Line) in India, before it was converted to 25 kV 50 Hz AC.], '25 kV AC'), (qas_id: 5, question_text: What helped Avicenna forget the Metaphysics of Aristotle?, doc_tokens: [3 kV DC is used in Belgium, Italy, Spain, Poland, the northern Czech Republic, Slovakia, Slovenia, South Africa, Chile, and former Soviet Union countries (also using 25 kV 50 Hz AC). It was formerly used by the Milwaukee Road from Harlowton, Montana to Seattle-Tacoma, across the Continental Divide and including extensive branch and loop lines in Montana, and by the Delaware, Lackawanna & Western Railroad (now New Jersey Transit, converted to 25 kV AC) in the United States, and the Kolkata suburban railway (Bardhaman Main Line) in India, before it was converted to 25 kV 50 Hz AC.], '25 kV 50 Hz AC.')])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[31m***********Question and Answers *************\u001b[0m\n",
            "\u001b[34mWho heads an office or division?\u001b[0m\n",
            "\u001b[5m\u001b[7m\u001b[32mRegis McKenna\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[34mWho leaders the sub-divisions of offices or divisions?\u001b[0m\n",
            "\u001b[5m\u001b[7m\u001b[32mRegis McKenna\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[34mWhat helped Avicenna forget the Metaphysics of Aristotle?\u001b[0m\n",
            "\u001b[5m\u001b[7m\u001b[32m<No Answer>\u001b[0m\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[31m***********Question and Answers *************\u001b[0m\n",
            "\u001b[34mBesides using 3kV DC what other power type is used in the former Soviet Union countries?\u001b[0m\n",
            "\u001b[5m\u001b[7m\u001b[32m25 kV 50 Hz AC\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[34mWhat does the railway system of US use DC or AC?\u001b[0m\n",
            "\u001b[5m\u001b[7m\u001b[32m25 kV AC\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[34mWhat helped Avicenna forget the Metaphysics of Aristotle?\u001b[0m\n",
            "\u001b[5m\u001b[7m\u001b[32m25 kV 50 Hz AC.\u001b[0m\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZ4O-vMtqtXE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}