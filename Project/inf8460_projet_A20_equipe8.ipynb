{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "inf8460_projet_A20_equipe8.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "7njiavPiDYDu",
        "3FEyx474lvfY",
        "3m8kiDbChKlY",
        "U-rv-pd-D6IM",
        "nYzHFoo0d8aI"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvon_hH2C87M"
      },
      "source": [
        "# **INF8460 A20 Project: Open-domain questions answering**\n",
        "\n",
        "<br>\n",
        "\n",
        "Equipe 8:\n",
        "\n",
        "\n",
        "*   Cedric Sadeu (Glove, ranking with classification)\n",
        "*   Mamoudou Sacko (pretraitement + TF-IDF, cosine ranking)\n",
        "*   Oumayma Messoussi (PCP Bert, ML/DL for ranking)\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMtbIoAD31sx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a693cd83-ae5e-4c3d-d77c-1ce5a17fdadc"
      },
      "source": [
        "!pip install sent2vec transformers pytorch-pretrained-bert # pytorch-nlp pytorch_transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sent2vec\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/c6/1f57065edbc772d9529e4a5f75cb812f29bcc2bf59b8e4c34c8ecfd83fe3/sent2vec-0.2.0-py3-none-any.whl\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/83/e74092e7f24a08d751aa59b37a9fc572b2e4af3918cb66f7766c3affb1b4/transformers-3.5.1-py3-none-any.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 7.9MB/s \n",
            "\u001b[?25hCollecting pytorch-pretrained-bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 38.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from sent2vec) (1.7.0+cu101)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (from sent2vec) (2.2.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from sent2vec) (1.18.5)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (from sent2vec) (3.6.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting tokenizers==0.9.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/34/b39eb9994bc3c999270b69c9eea40ecc6f0e97991dba28282b9fd32d44ee/tokenizers-0.9.3-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 40.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Collecting sentencepiece==0.1.91\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 37.6MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 46.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Collecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/59/7f/4ade91fbb684c6f28a6e56028d9f9d2de4297761850d083579779f07c0de/boto3-1.16.25-py2.py3-none-any.whl (129kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 59.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->sent2vec) (3.7.4.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->sent2vec) (0.16.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->sent2vec) (2.0.4)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->sent2vec) (7.4.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->sent2vec) (1.0.4)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy->sent2vec) (1.1.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy->sent2vec) (1.0.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->sent2vec) (0.8.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->sent2vec) (0.4.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy->sent2vec) (1.0.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy->sent2vec) (50.3.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->sent2vec) (3.0.4)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim->sent2vec) (1.4.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim->sent2vec) (3.0.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim->sent2vec) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.11.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Collecting s3transfer<0.4.0,>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/79/e6afb3d8b0b4e96cefbdc690f741d7dd24547ff1f94240c997a26fa908d3/s3transfer-0.3.3-py2.py3-none-any.whl (69kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 11.2MB/s \n",
            "\u001b[?25hCollecting botocore<1.20.0,>=1.19.25\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ef/d5/c0c33ca15e31062220ac5964f3492409eaf90a5cf5399503cd8264f2f8e9/botocore-1.19.25-py2.py3-none-any.whl (6.9MB)\n",
            "\u001b[K     |████████████████████████████████| 6.9MB 32.2MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy->sent2vec) (2.0.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.20.0,>=1.19.25->boto3->pytorch-pretrained-bert) (2.8.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy->sent2vec) (3.4.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=9521c72f50ebdf6f51b1403e4ba5962bf1ebb39b0b5e954107cdc17aefb7d858\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "\u001b[31mERROR: botocore 1.19.25 has requirement urllib3<1.27,>=1.25.4; python_version != \"3.4\", but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers, sent2vec, jmespath, botocore, s3transfer, boto3, pytorch-pretrained-bert\n",
            "Successfully installed boto3-1.16.25 botocore-1.19.25 jmespath-0.10.0 pytorch-pretrained-bert-0.6.2 s3transfer-0.3.3 sacremoses-0.0.43 sent2vec-0.2.0 sentencepiece-0.1.91 tokenizers-0.9.3 transformers-3.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxkOcrpE4tCW"
      },
      "source": [
        "import io\n",
        "import os\n",
        "import math\n",
        "import nltk\n",
        "import time\n",
        "import torch\n",
        "import random\n",
        "import sklearn\n",
        "import zipfile\n",
        "import operator\n",
        "import requests\n",
        "import functools\n",
        "import itertools\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "import multiprocessing\n",
        "from functools import partial\n",
        "from typing import Dict, List, Tuple\n",
        "from collections import Counter, defaultdict\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "from scipy.spatial.distance import euclidean, cosine\n",
        "from transformers import pipeline, Trainer, TrainingArguments\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, BertTokenizer, BertModel, BertForQuestionAnswering\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfLr5d9n8Nyp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "babb56b1-c6e8-4f0d-9345-2a10314fdfaa"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!ls '/content/drive/My Drive/Colab Notebooks/INF8460/Project/'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            " correction_script.py\t\t   'mlp_regression_msmarco_tfidf_4957*2.h5'\n",
            " create_embeddings.py\t\t    MSMARCO\n",
            " data\t\t\t\t    output\n",
            " data_handling.py\t\t    __pycache__\n",
            " inf8460_projet_A20_equipe8.ipynb   pytorch_model.bin\n",
            " LSTMSiameseTextSimilarity\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-B1THH6Y1YJj"
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/Colab Notebooks/INF8460/Project/')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIcemP-_-mKa"
      },
      "source": [
        "### Lecture des donnees"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6h3I8Gl3769R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e50212e7-6c4d-43ad-ab3c-fe9c0e7da757"
      },
      "source": [
        "from data_handling import *\n",
        "\n",
        "data_path = \"data\"\n",
        "output_path = \"/content/drive/My Drive/Colab Notebooks/INF8460/Project/output\"\n",
        "\n",
        "train_data = read_data(os.path.join(data_path, \"/content/drive/My Drive/Colab Notebooks/INF8460/Project/data/corpus.csv\"))\n",
        "# train_ids = read_questions(os.path.join(data_path, \"/content/drive/My Drive/Colab Notebooks/INF8460/Project/data/train_ids.csv\"))\n",
        "val_ids = read_questions(os.path.join(data_path, \"/content/drive/My Drive/Colab Notebooks/INF8460/Project/data/val_ids.csv\"))\n",
        "# test_ids = read_questions(os.path.join(data_path, \"/content/drive/My Drive/Colab Notebooks/INF8460/Project/data/test.csv\"))\n",
        "\n",
        "\n",
        "paragraphs = [\" \".join(sentence.split()).lower() for sentence in train_data[1]]\n",
        "# questions = [\" \".join(sentence.split()).lower() for sentence in train_ids[1]]\n",
        "val_questions = [\" \".join(sentence.split()).lower() for sentence in val_ids[1]]\n",
        "# test_questions = [\" \".join(sentence.split()).lower() for sentence in test_ids[1]]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tj5wul4l-pMg"
      },
      "source": [
        "### Pretraitement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mybu06h-xpN"
      },
      "source": [
        "from data_handling import *\n",
        "pre = Preprocess()\n",
        "\n",
        "paragraphs_tokenized = pre.preprocess_pipeline(paragraphs)\n",
        "# questions_tokenized = pre.preprocess_pipeline(questions)\n",
        "val_questions_tokenized = pre.preprocess_pipeline(val_questions)\n",
        "# test_questions_tokenized = pre.preprocess_pipeline(test_questions)\n",
        "\n",
        "paragraphs_text = [\" \".join(sentence) for sentence in paragraphs_tokenized]\n",
        "# questions_text = [\" \".join(sentence) for sentence in questions_tokenized]\n",
        "val_questions_text = [\" \".join(sentence) for sentence in val_questions_tokenized]\n",
        "# test_questions_text = [\" \".join(sentence) for sentence in test_questions_tokenized]\n",
        "\n",
        "del paragraphs_tokenized\n",
        "# del questions_tokenized\n",
        "del val_questions_tokenized\n",
        "# del test_questions_tokenized"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xJqabvHFNvq"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## **1. Plongements lexicaux**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQREt5DLDV1E"
      },
      "source": [
        "### TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCJPGU0V_LBG"
      },
      "source": [
        "from create_embeddings import buildVocab, getTfIdfReprentation, get_doc_embedded, sklearn_svd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# paragraphs_vocab = buildVocab(paragraphs_text)\n",
        "# max_feat = int(0.01 * len(paragraphs_vocab))\n",
        "vectorizer = TfidfVectorizer(max_features=5000) # vocabulary=paragraphs_vocab\n",
        "paragraphs_tfidf = getTfIdfReprentation(paragraphs_text, vectorizer)\n",
        "\n",
        "val_questions_tfidf = vectorizer.transform(val_questions_text).todense()\n",
        "\n",
        "# corpus = {'id': train_data[0], 'paragraph': paragraphs_tfidf}\n",
        "# save_to_csv(\"corpus.csv\", corpus)\n",
        "\n",
        "# train_ids = {'id': train_ids[0], 'question': paragraphs_tfidf, 'paragraph_id': train_ids[2], 'answer': train_ids[3] }\n",
        "# save_to_csv(\"train_ids.csv\", train_ids)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7njiavPiDYDu"
      },
      "source": [
        "### GloVe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7TJbIqyjxHt"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip\n",
        "!rm glove.6B.50d.txt\n",
        "!rm glove.6B.100d.txt\n",
        "!rm glove.6B.200d.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Pphx1xmi4hP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "178df8fb-6ff8-426f-df9d-9f01b8334fc1"
      },
      "source": [
        "from data_handling import read_from_csv\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from create_embeddings import get_gloves_dict, get_lines_gloves, get_plong_corpus\n",
        "\n",
        "path = \"/content/drive/My Drive/Colab Notebooks/INF8460/Project/output/corpus.csv\"\n",
        "datat = read_from_csv(path)\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit(datat[1]).vocabulary_"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7JqLpSjjIJ5"
      },
      "source": [
        "glove_dict = get_gloves_dict()\n",
        "key_set = set(X.keys()) & set(glove_dict.keys())\n",
        "glove_dict_vocab_corpus = {key: glove_dict[key] for key in key_set}"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZojYC8ajNn7"
      },
      "source": [
        "plongement_doc = get_plong_corpus(datat[1], glove_dict_vocab_corpus)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDGfEFkqDaBy"
      },
      "source": [
        "### Plongements contextuels pré-entraînés"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FEyx474lvfY"
      },
      "source": [
        "> #### BERT base pre-trained"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxtmBinzK9P5"
      },
      "source": [
        "torch.cuda.set_device(0)\n",
        "\n",
        "questions = [\"How many parameters does BERT-large have?\"]\n",
        "answer_text = r\"\"\"BERT-large is really big... it has 24-layers and an embedding size of 1,024, \n",
        "                  for a total of 340M parameters! Altogether it is 1.34GB, so expect it to take \n",
        "                  a couple minutes to download to your Colab instance.\"\"\"\n",
        "\n",
        "model = BertModel.from_pretrained(\"bert-base-cased\", return_dict=True)\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "# print(device)\n",
        "# # model = model.to(device)\n",
        "\n",
        "for question in questions:\n",
        "    inputs = tokenizer(question, return_tensors=\"pt\")\n",
        "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
        "\n",
        "    text_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "\n",
        "    # display tokens and ids\n",
        "    for token, id in zip(text_tokens, input_ids):\n",
        "        if id == tokenizer.sep_token_id:\n",
        "            print('')\n",
        "        print('{:<12} {:>6,}'.format(token, id))\n",
        "        if id == tokenizer.sep_token_id:\n",
        "            print('')\n",
        "\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "    last_hidden_states = outputs.last_hidden_state\n",
        "    print(last_hidden_states.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3m8kiDbChKlY"
      },
      "source": [
        "> #### spacy BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rZyS2Y-hKlY"
      },
      "source": [
        "from scipy import spatial\n",
        "from sent2vec.vectorizer import Vectorizer\n",
        "\n",
        "sentences = [\n",
        "    \"This is an awesome book to learn NLP.\",\n",
        "    \"DistilBERT is an amazing NLP model.\",\n",
        "    \"We can interchangeably use embedding, encoding, or vectorizing.\",\n",
        "]\n",
        "\n",
        "vectorizer = Vectorizer()\n",
        "vectorizer.bert(sentences)\n",
        "vectors_bert = vectorizer.vectors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsAA2F1kDu0P"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## **2. Ordonnancement**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-rv-pd-D6IM"
      },
      "source": [
        "\n",
        "\n",
        "> #### cosine similarity\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AS6psQWBIZpE"
      },
      "source": [
        "def voisins(word, df, n, distfunc=cosine):\n",
        "    assert distfunc.__name__ == 'cosine' or distfunc.__name__ == 'euclidean', \"distance metric not supported\"\n",
        "    order = True if distfunc.__name__ == 'euclidean' else False\n",
        "\n",
        "    closest = {}\n",
        "    for w in df:\n",
        "        distance = distfunc(word, df[w])\n",
        "        closest[w] = distance\n",
        "\n",
        "    closest = {k: v for k, v in sorted(closest.items(), key=lambda item: item[1], reverse=order)}\n",
        "\n",
        "    return list(closest.keys())[:n], list(closest.values())[:n]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5GbmL_aSYwD"
      },
      "source": [
        "# paragraphs_vocab = buildVocab(paragraphs_text)\n",
        "questions_vocab = buildVocab(questions_text)\n",
        "\n",
        "questions_ids = train_ids[0]\n",
        "vectorizer = TfidfVectorizer(max_features=500) # vocabulary=questions_vocab\n",
        "questions_tfidf = getTfIdfReprentation(questions_text, vectorizer)\n",
        "\n",
        "dic_questions = {}\n",
        "for i, ids in enumerate(questions) :\n",
        "    dic_questions[questions_ids[i]] = questions_tfidf[i]\n",
        "\n",
        "#questions_tfidf_r = sklearn_svd(questions_tfidf, len(embeddings))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxrCn0Nh_FFh"
      },
      "source": [
        "start = time.time()\n",
        "\n",
        "ranking_list = {}\n",
        "for i in range(len(test_questions_text)):\n",
        "    new_question = test_questions_text[i]\n",
        "    new_question_tokenized = pre.preprocess_pipeline([new_question])\n",
        "    new_question_text = [\" \".join(sentence) for sentence in new_question_tokenized]\n",
        "    \n",
        "    new_question_tfidf = vectorizer.transform(new_question_text).todense()\n",
        "\n",
        "    topk_ids, topk_questions = voisins(new_question_tfidf, dic_questions, 3, distfunc=cosine)\n",
        "    print(topk_ids, topk_questions)\n",
        "\n",
        "    ranking_list[i] = topk_ids\n",
        "\n",
        "print(time.time() - start)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOd9JHBwR_td"
      },
      "source": [
        "# save to file\n",
        "save_to_csv(\"test_cosine_sim_questions.csv\", ranking_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYzHFoo0d8aI"
      },
      "source": [
        "\n",
        "\n",
        "> #### LSTM Siamese text similarity\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4v49V-JIeAb_"
      },
      "source": [
        "# !git clone https://github.com/amansrivastava17/lstm-siamese-text-similarity.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hus1J2v8U0_D"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/brmson/dataset-sts/master/data/sts/sick2014/SICK_train.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPuQlVrweGQD"
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/Colab Notebooks/INF8460/Project/LSTMSiameseTextSimilarity/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "foJhzzO_Xqfm",
        "outputId": "29791c39-375f-49ad-f5fa-9d104a78bd64"
      },
      "source": [
        "from model import SiameseBiLSTM\n",
        "from inputHandler import word_embed_meta_data, create_test_data\n",
        "from config import siamese_config\n",
        "import pandas as pd\n",
        "\n",
        "############ Data Preperation ##########\n",
        "\n",
        "# df = pd.read_csv('lstm-siamese-text-similarity/sample_data.csv')\n",
        "df = pd.read_csv('SICK_train.txt', names=['pair_ID', 'sentence_A', 'sentence_B', 'relatedness_score', 'entailment_judgment'], \n",
        "                 skiprows=1, sep='\\t')\n",
        "df.drop('pair_ID', axis=1, inplace=True)\n",
        "df.drop(df[df.entailment_judgment == 'CONTRADICTION'].index, inplace=True)\n",
        "print(df.head())\n",
        "\n",
        "sentences1 = list(df['sentence_A'])\n",
        "sentences2 = list(df['sentence_B'])\n",
        "is_similar = list(df['relatedness_score'])\n",
        "del df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                          sentence_A  ... entailment_judgment\n",
            "0  A group of kids is playing in a yard and an ol...  ...             NEUTRAL\n",
            "1  A group of children is playing in the house an...  ...             NEUTRAL\n",
            "2  The young boys are playing outdoors and the ma...  ...          ENTAILMENT\n",
            "3  The kids are playing outdoors near a man with ...  ...             NEUTRAL\n",
            "4  The young boys are playing outdoors and the ma...  ...             NEUTRAL\n",
            "\n",
            "[5 rows x 4 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Cdba8HkeeJ2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91634da4-08a2-4f18-d528-bc7dc3aa3cd7"
      },
      "source": [
        "######## Word Embedding ############\n",
        "\n",
        "tokenizer, embedding_matrix = word_embed_meta_data(sentences1 + sentences2,  siamese_config['EMBEDDING_DIM'], glove_dict)\n",
        "\n",
        "embedding_meta_data = {\n",
        "\t'tokenizer': tokenizer,\n",
        "\t'embedding_matrix': embedding_matrix\n",
        "}\n",
        "\n",
        "## creating sentence pairs\n",
        "sentences_pair = [(x1, x2) for x1, x2 in zip(sentences1, sentences2)]\n",
        "del sentences1\n",
        "del sentences2\n",
        "\n",
        "######## Training ########\n",
        "\n",
        "class Configuration(object):\n",
        "    \"\"\"Dump stuff here\"\"\"\n",
        "\n",
        "CONFIG = Configuration()\n",
        "\n",
        "CONFIG.embedding_dim = siamese_config['EMBEDDING_DIM']\n",
        "CONFIG.max_sequence_length = siamese_config['MAX_SEQUENCE_LENGTH']\n",
        "CONFIG.number_lstm_units = siamese_config['NUMBER_LSTM']\n",
        "CONFIG.rate_drop_lstm = siamese_config['RATE_DROP_LSTM']\n",
        "CONFIG.number_dense_units = siamese_config['NUMBER_DENSE_UNITS']\n",
        "CONFIG.activation_function = siamese_config['ACTIVATION_FUNCTION']\n",
        "CONFIG.rate_drop_dense = siamese_config['RATE_DROP_DENSE']\n",
        "CONFIG.validation_split_ratio = siamese_config['VALIDATION_SPLIT']\n",
        "\n",
        "siamese = SiameseBiLSTM(CONFIG.embedding_dim , CONFIG.max_sequence_length, CONFIG.number_lstm_units , CONFIG.number_dense_units, CONFIG.rate_drop_lstm, CONFIG.rate_drop_dense, CONFIG.activation_function, CONFIG.validation_split_ratio)\n",
        "\n",
        "best_model_path = siamese.train_model(sentences_pair, is_similar, embedding_meta_data, model_save_directory='./')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Embedding matrix shape: (2277, 300)\n",
            "vector not found for word - dog,\n",
            "vector not found for word - isn't\n",
            "vector not found for word - man's\n",
            "vector not found for word - black,\n",
            "vector not found for word - corndogs\n",
            "vector not found for word - little,\n",
            "vector not found for word - young,\n",
            "vector not found for word - white,\n",
            "vector not found for word - small,\n",
            "vector not found for word - girl,\n",
            "vector not found for word - red,\n",
            "vector not found for word - woman,\n",
            "vector not found for word - uniform,\n",
            "vector not found for word - child,\n",
            "vector not found for word - rope,\n",
            "vector not found for word - wagon,\n",
            "vector not found for word - field,\n",
            "vector not found for word - boy,\n",
            "vector not found for word - backbends\n",
            "vector not found for word - necklace,\n",
            "vector not found for word - tree,\n",
            "vector not found for word - pine,\n",
            "vector not found for word - snow,\n",
            "vector not found for word - blue,\n",
            "vector not found for word - gray,\n",
            "vector not found for word - big,\n",
            "vector not found for word - dog's\n",
            "vector not found for word - large,\n",
            "vector not found for word - hair,\n",
            "vector not found for word - one,\n",
            "vector not found for word - stairs,\n",
            "vector not found for word - guitar,\n",
            "vector not found for word - acoustic,\n",
            "vector not found for word - brown,\n",
            "vector not found for word - rock.\n",
            "vector not found for word - arm.\n",
            "vector not found for word - person's\n",
            "vector not found for word - tail.\n",
            "vector not found for word - somebody's\n",
            "vector not found for word - seadoo\n",
            "vector not found for word - suit,\n",
            "vector not found for word - climbing,\n",
            "vector not found for word - t-ball\n",
            "vector not found for word - artist's\n",
            "vector not found for word - bread,\n",
            "vector not found for word - tan,\n",
            "vector not found for word - man,\n",
            "vector not found for word - mouth.\n",
            "vector not found for word - old,\n",
            "vector not found for word - asia,\n",
            "vector not found for word - shirt,\n",
            "vector not found for word - stenograph\n",
            "vector not found for word - coat,\n",
            "vector not found for word - men,\n",
            "vector not found for word - mittened\n",
            "vector not found for word - dogs,\n",
            "vector not found for word - pristine,\n",
            "vector not found for word - road,\n",
            "vector not found for word - graphitized\n",
            "vector not found for word - pants,\n",
            "vector not found for word - someone's\n",
            "vector not found for word - room,\n",
            "vector not found for word - child's\n",
            "vector not found for word - daschunds\n",
            "vector not found for word - aren't\n",
            "vector not found for word - cloak,\n",
            "vector not found for word - age,\n",
            "vector not found for word - pool,\n",
            "vector not found for word - wall.\n",
            "vector not found for word - window,\n",
            "vector not found for word - doesn't\n",
            "vector not found for word - amusedly\n",
            "vector not found for word - running,\n",
            "vector not found for word - badger,\n",
            "vector not found for word - shrewd,\n",
            "vector not found for word - prey.\n",
            "vector not found for word - kid,\n",
            "vector not found for word - magic,\n",
            "vector not found for word - cat's\n",
            "vector not found for word - ball,\n",
            "vector not found for word - car,\n",
            "vector not found for word - monkey's\n",
            "vector not found for word - woman's\n",
            "vector not found for word - shoeless,\n",
            "vector not found for word - male,\n",
            "vector not found for word - skinned,\n",
            "vector not found for word - bikes,\n",
            "vector not found for word - children,\n",
            "vector not found for word - jacket,\n",
            "vector not found for word - midspeech\n",
            "vector not found for word - skit,\n",
            "vector not found for word - funny,\n",
            "vector not found for word - open.\n",
            "vector not found for word - crane,\n",
            "vector not found for word - rollerblader\n",
            "vector not found for word - water's\n",
            "vector not found for word - person,\n",
            "vector not found for word - bike,\n",
            "vector not found for word - bricks,\n",
            "vector not found for word - unprotective\n",
            "vector not found for word - blew,\n",
            "vector not found for word - girl's\n",
            "vector not found for word - red.\n",
            "vector not found for word - riskily\n",
            "vector not found for word - boat's\n",
            "vector not found for word - field.\n",
            "vector not found for word - pony's\n",
            "vector not found for word - bowl,\n",
            "vector not found for word - unstitching\n",
            "vector not found for word - cabinet,\n",
            "vector not found for word - pack,\n",
            "vector not found for word - topless,\n",
            "vector not found for word - swimsuit,\n",
            "vector not found for word - yellow,\n",
            "vector not found for word - arms,\n",
            "vector not found for word - uninterestedly\n",
            "vector not found for word - crowd,\n",
            "vector not found for word - motionlessly\n",
            "vector not found for word - t-shirt,\n",
            "vector not found for word - sand,\n",
            "vector not found for word - air,\n",
            "vector not found for word - vertical,\n",
            "vector not found for word - barren,\n",
            "vector not found for word - motorcycle.\n",
            "vector not found for word - it.\n",
            "vector not found for word - eyebrow,\n",
            "vector not found for word - rhino,\n",
            "vector not found for word - seated,\n",
            "vector not found for word - kangaroo,\n",
            "vector not found for word - baby,\n",
            "vector not found for word - boy's\n",
            "vector not found for word - animal,\n",
            "vector not found for word - mountain,\n",
            "vector not found for word - rocky,\n",
            "vector not found for word - sunset,\n",
            "vector not found for word - waterfall,\n",
            "vector not found for word - challengingly\n",
            "vector not found for word - rocks,\n",
            "vector not found for word - bmxs\n",
            "vector not found for word - family,\n",
            "vector not found for word - happy,\n",
            "vector not found for word - weather,\n",
            "vector not found for word - skirt,\n",
            "vector not found for word - amazedly\n",
            "vector not found for word - eyebrows,\n",
            "Null word embeddings: 146\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "Epoch 1/200\n",
            " 1/54 [..............................] - ETA: 0s - loss: 18.3893 - acc: 0.0000e+00WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
            "Instructions for updating:\n",
            "use `tf.profiler.experimental.stop` instead.\n",
            " 2/54 [>.............................] - ETA: 29s - loss: 18.1654 - acc: 0.0078   WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.2532s vs `on_train_batch_end` time: 0.8731s). Check your callbacks.\n",
            "54/54 [==============================] - 12s 229ms/step - loss: 13.5467 - acc: 0.0133 - val_loss: 6.1147 - val_acc: 0.0261\n",
            "Epoch 2/200\n",
            "54/54 [==============================] - 10s 179ms/step - loss: 9.6653 - acc: 0.0125 - val_loss: 6.6070 - val_acc: 0.0131\n",
            "Epoch 3/200\n",
            "54/54 [==============================] - 10s 194ms/step - loss: 6.4448 - acc: 0.0174 - val_loss: 4.7807 - val_acc: 0.0287\n",
            "Epoch 4/200\n",
            "54/54 [==============================] - 10s 189ms/step - loss: 4.1387 - acc: 0.0223 - val_loss: 2.8861 - val_acc: 0.0392\n",
            "Epoch 5/200\n",
            "54/54 [==============================] - 10s 186ms/step - loss: 2.7174 - acc: 0.0272 - val_loss: 1.4785 - val_acc: 0.0392\n",
            "Epoch 6/200\n",
            "54/54 [==============================] - 10s 187ms/step - loss: 2.0804 - acc: 0.0333 - val_loss: 0.9469 - val_acc: 0.0392\n",
            "Epoch 7/200\n",
            "54/54 [==============================] - 10s 181ms/step - loss: 1.7875 - acc: 0.0345 - val_loss: 0.8405 - val_acc: 0.0392\n",
            "Epoch 8/200\n",
            "54/54 [==============================] - 10s 183ms/step - loss: 1.6710 - acc: 0.0353 - val_loss: 0.6838 - val_acc: 0.0392\n",
            "Epoch 9/200\n",
            "54/54 [==============================] - 10s 193ms/step - loss: 1.5008 - acc: 0.0356 - val_loss: 0.6380 - val_acc: 0.0392\n",
            "Epoch 10/200\n",
            "54/54 [==============================] - 10s 182ms/step - loss: 1.3313 - acc: 0.0359 - val_loss: 0.6917 - val_acc: 0.0392\n",
            "Epoch 11/200\n",
            "54/54 [==============================] - 10s 184ms/step - loss: 1.3035 - acc: 0.0353 - val_loss: 0.7368 - val_acc: 0.0392\n",
            "Epoch 12/200\n",
            "54/54 [==============================] - 10s 181ms/step - loss: 1.2453 - acc: 0.0385 - val_loss: 0.6041 - val_acc: 0.0392\n",
            "Epoch 13/200\n",
            "54/54 [==============================] - 10s 179ms/step - loss: 1.1452 - acc: 0.0339 - val_loss: 0.6189 - val_acc: 0.0366\n",
            "Epoch 14/200\n",
            "54/54 [==============================] - 10s 187ms/step - loss: 1.1377 - acc: 0.0362 - val_loss: 0.6459 - val_acc: 0.0366\n",
            "Epoch 15/200\n",
            "54/54 [==============================] - 10s 186ms/step - loss: 1.0019 - acc: 0.0362 - val_loss: 0.6411 - val_acc: 0.0392\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDwJAlO1J_cw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4b36ac4-ddaa-45f8-9c9a-a453660356e2"
      },
      "source": [
        "######## Testing Q/Q pairs ########\n",
        "\n",
        "from operator import itemgetter\n",
        "from keras.models import load_model\n",
        "\n",
        "model = load_model(best_model_path)\n",
        "\n",
        "test_sentence_pairs = [('A man with a jersey is dunking the ball at a basketball game', 'The ball is being dunked by a man with a jersey at a basketball game'),\n",
        "                       ('Two dogs are fighting', 'Two dogs are wrestling and hugging'), \n",
        "                       ('A group of kids is playing in a yard and an old man is standing in the background', 'A group of boys in a yard is playing and a man is standing in the background'), \n",
        "                       ('What can make Physics easy to learn?','How can you make physics easy to learn?'),\n",
        "                       ('How many times a day do a clocks hands overlap?','What does it mean that every time I look at the clock the numbers are the same?')]\n",
        "\n",
        "test_data_x1, test_data_x2, leaks_test = create_test_data(tokenizer,test_sentence_pairs,  siamese_config['MAX_SEQUENCE_LENGTH'])\n",
        "\n",
        "preds = list(model.predict([test_data_x1, test_data_x2, leaks_test], verbose=1).ravel())\n",
        "results = [(x, y, z) for (x, y), z in zip(test_sentence_pairs, preds)]\n",
        "results.sort(key=itemgetter(2), reverse=True)\n",
        "print(results)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f971163a378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 [==============================] - 0s 7ms/step\n",
            "[('A man with a jersey is dunking the ball at a basketball game', 'The ball is being dunked by a man with a jersey at a basketball game', 4.673317), ('A group of kids is playing in a yard and an old man is standing in the background', 'A group of boys in a yard is playing and a man is standing in the background', 3.9694657), ('Two dogs are fighting', 'Two dogs are wrestling and hugging', 3.3925214), ('What can make Physics easy to learn?', 'How can you make physics easy to learn?', 3.1973097), ('How many times a day do a clocks hands overlap?', 'What does it mean that every time I look at the clock the numbers are the same?', 1.8688544)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKcvBR4l3bZd",
        "outputId": "5421d86e-97d3-4e6e-c411-05c0db6d8e3b"
      },
      "source": [
        "for i, doc in enumerate(paragraphs_text):\n",
        "    test_sentence_pairs = [(doc, passage) for passage in set(paragraphs_text[:i]+paragraphs_text[i+1:])]\n",
        "\n",
        "    test_data_x1, test_data_x2, leaks_test = create_test_data(tokenizer, test_sentence_pairs, siamese_config['MAX_SEQUENCE_LENGTH'])\n",
        "\n",
        "    preds = list(model.predict([test_data_x1, test_data_x2, leaks_test], verbose=1).ravel())\n",
        "    results = [(x, y, z) for (x, y), z in zip(test_sentence_pairs, preds)]\n",
        "    results.sort(key=itemgetter(2), reverse=True)\n",
        "    \n",
        "    print(doc)\n",
        "    print('\\n')\n",
        "    for line in results[:10]:\n",
        "        print(line[1], line[2])\n",
        "\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2096/2096 [==============================] - 61s 29ms/step\n",
            "the norman norman nourmands french normands latin normanni were the people who in the and century gave their name to normandy region in france they were descended from norse norman come from norseman raider and pirate from denmark iceland and norway who under their leader rollo agreed to swear fealty to king charles iii of west francia through generation of assimilation and mixing with the native frankish and roman gaulish population their descendant would gradually merge with the carolingian based culture of west francia the distinct cultural and ethnic identity of the norman emerged initially in the first half of the century and it continued to evolve over the succeeding century\n",
            "\n",
            "\n",
            "both the ford foundation and the rockefeller foundation were heavily involved one key leader wa norman borlaug the father of the green revolution who received the nobel peace prize in he is credited with saving over billion people from starvation the basic approach wa the development of high yielding variety of cereal grain expansion of irrigation infrastructure modernization of management technique distribution of hybridized seed synthetic fertilizer and pesticide to farmer 3.660336\n",
            "the seventeenth amendment amendment xvii to the united state constitution established the popular election of united state senator by the people of the state the amendment supersedes article clause and of the constitution under which senator were elected by state legislature it also alters the procedure for filling vacancy in the senate allowing for state legislature to permit their governor to make temporary appointment until special election can be held 3.6422997\n",
            "geisel received the laura ingalls wilder medal from the professional child librarian in recognizing his substantial and lasting contribution to child literature at the time it wa awarded every five year he won special pulitzer prize in citing his contribution over nearly half century to the education and enjoyment of america child and their parent 3.6043751\n",
            "valencia vəˈlɛnsiə spanish baˈlenθja or valència valencian vaˈlensia is the capital of the autonomous community of valencia and the third largest city in spain after madrid and barcelona with around inhabitant in the administrative centre it urban area extends beyond the administrative city limit with population of around million people valencia is spain third largest metropolitan area with population ranging from to million the city ha global city status the port of valencia is the busiest container port in europe and the busiest container port on the mediterranean sea 3.603384\n",
            "the third season of the animated television series rick and morty originally aired in the united state on cartoon network late night programming block adult swim it premiered with the rickshank rickdemption which aired unannounced on april and wa replayed every half hour from to et a part of adult swim annual april fool prank the episode wa also simulcast a looping live stream on adult swim site 3.603122\n",
            "although the tory were dismissed from office for half century for most of this period at first under the leadership of sir william wyndham the tory retained party cohesion with occasional hope of regaining office particularly at the accession of george ii and the downfall of the ministry of sir robert walpole in they acted a united though unavailing opposition to whig corruption and scandal at time they cooperated with the opposition whig whig who were in opposition to the whig government however the ideological gap between the tory and the opposition whig prevented them from coalescing a single party they finally regained power with the accession of george iii in under lord bute 3.5911543\n",
            "nasser also attempted to maintain oversight of the country civil service to prevent it from inflating and consequently becoming burden to the state new law provided worker with minimum wage profit share free education free health care reduced working hour and encouragement to participate in management land reform guaranteed the security of tenant farmer promoted agricultural growth and reduced rural poverty a result of the measure government ownership of egyptian business reached percent and the national union wa renamed the arab socialist union asu with these measure came more domestic repression a thousand of islamist were imprisoned including dozen of military officer nasser tilt toward soviet style system led his aide boghdadi and hussein el shafei to submit their resignation in protest 3.5801783\n",
            "the article of confederation and perpetual union wa the first constitution of the united state it wa drafted by the second continental congress from mid through late and ratification by all state wa completed by early under the article of confederation the central government power wa quite limited the confederation congress could make decision but lacked enforcement power implementation of most decision including modification to the article required unanimous approval of all thirteen state legislature 3.5801382\n",
            "dominion were semi independent polity under the british crown constituting the british empire beginning with canadian confederation in they included canada australia new zealand newfoundland south africa and the irish free state and then from the late also india pakistan and ceylon now sri lanka the balfour declaration of recognised the dominion a autonomous community within the british empire and the statute of westminster confirmed their full legislative independence 3.5747845\n",
            "table tr th window version th th release date th th release version th th edition th th latest build th tr tr td window td td july td td nt td td ul li window home li li window pro li li window pro for workstation li li window pro education li li window enterprise li li window enterprise ltsb li li window education li li window iot core li li window iot enterprise li li window li ul see window edition td td april update td tr tr td window td td october td td nt td td ul li window li li window pro li li window enterprise li li window oem li li window with bing li ul see window edition td td april update td tr tr td window td td october td td nt td td ul li window li li window pro li li window enterprise li li window oem li ul see window edition td td td tr tr td window td td july td td nt td td ul li window starter li li window home basic li li window home premium li li window pro li li window enterprise business edition li li window ultimate li li window thin pc li ul see window edition td td service pack td tr tr td window vista td td january td td nt td td ul li window vista starter li li window vista home basic li li window vista home premium li li window vista business li li window vista enterprise li li window vista ultimate li ul see window vista edition td td service pack td tr tr td window xp professional td td april td td nt td td td td service pack td tr tr td window xp td td october td td nt td td ul li window xp starter li li window xp home li li window xp professional li li window xp bit edition li li window fundamental for legacy pc july li ul see window xp edition td td service pack td tr tr td window me td td september td td td td td td td tr tr td window td td february td td nt td td professional td td td tr tr td window td td june td td td td ul li window li li window second edition april li ul td td td tr tr td window nt td td august td td nt td td window nt workstation td td service pack td tr tr td window td td august td td td td ul li window li li window december li li window february li li window august li li window usb supplement to august li li window august li li window november li ul td td td tr tr td window nt td td may td td nt td td window nt workstation td td td tr tr td window nt td td september td td nt td td window nt workstation td td td tr tr td window td td november td td td td td td td tr tr td window for workgroups td td november td td td td td td td tr tr td window nt td td july td td nt td td window nt td td td tr tr td window td td april td td td td ul li window li li window for workgroups october li ul td td td tr tr td window td td may td td td td td td td tr tr td window td td march td td td td ul li window li li window li ul td td td tr tr td window td td may td td td td ul li window li li window li ul td td td tr tr td window td td december td td td td td td td tr tr td window td td april td td td td td td td tr tr td window td td august td td td td td td td tr tr td window td td may td td td td td td td tr tr td window td td november td td td td td td td tr table 3.550599\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8T1EyGDfjey4"
      },
      "source": [
        "\n",
        "\n",
        "> #### MLP regression model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqDuMAqPXpEZ",
        "outputId": "71e96210-9a74-4a79-ba16-1cd1e8dd0755"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/INF8460/Project/MSMARCO/msmarco-docdev-100k.csv')\n",
        "df.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
        "df = df.iloc[:25000,:] \n",
        "print(len(df))\n",
        "\n",
        "df_score = df['score']\n",
        "normalized_df = (df_score - df_score.min())/ (df_score.max()-df_score.min()) #df_score.std()\n",
        "del df_score\n",
        "print(normalized_df.head())\n",
        "\n",
        "y = normalized_df#.multiply(1.0/df[\"rank\"], axis=\"index\")\n",
        "# print(y.head())\n",
        "del normalized_df\n",
        "\n",
        "df.drop(['rank', 'score'], axis=1, inplace=True)\n",
        "\n",
        "# Concatenate queries and docs in pairs\n",
        "\n",
        "# X = df[['query', 'doc']].agg(' [sep] '.join, axis=1)\n",
        "# del df"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25000\n",
            "0    0.761969\n",
            "1    0.759416\n",
            "2    0.755649\n",
            "3    0.748219\n",
            "4    0.746034\n",
            "Name: score, dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nSk8IjOd_FY"
      },
      "source": [
        "from data_handling import *\n",
        "pre = Preprocess()\n",
        "\n",
        "paragraphs_tokenized = pre.preprocess_pipeline(df[\"doc\"])\n",
        "questions_tokenized = pre.preprocess_pipeline(df[\"query\"])\n",
        "\n",
        "paragraphs_text = [\" \".join(sentence) for sentence in paragraphs_tokenized]\n",
        "questions_text = [\" \".join(sentence) for sentence in questions_tokenized]\n",
        "\n",
        "del paragraphs_tokenized\n",
        "del questions_tokenized\n",
        "del df"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsuAHj0bVvM7"
      },
      "source": [
        "# tfidf embeddings\n",
        "\n",
        "from create_embeddings import buildVocab, getTfIdfReprentation, get_doc_embedded, sklearn_svd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# paragraphs_vocab = buildVocab(paragraphs_text) # taille du vocab: 495737\n",
        "# max_feat = int(0.01 * len(paragraphs_vocab))\n",
        "# vectorizer = TfidfVectorizer(max_features=max_feat) # vocabulary=paragraphs_vocab\n",
        "msmarco_paragraphs_tfidf = getTfIdfReprentation(paragraphs_text, vectorizer) # meme vectorizer utilisé pour notre corpus\n",
        "\n",
        "# new_question = vals_questions_text[i]\n",
        "# new_question_tokenized = pre.preprocess_pipeline([new_question])\n",
        "# new_question_text = [\" \".join(sentence) for sentence in new_question_tokenized]\n",
        "\n",
        "msmarco_questions_tfidf = vectorizer.transform(questions_text).todense()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2vHlZXReiVpe",
        "outputId": "950e620c-628d-4591-ae5c-38770b216eb9"
      },
      "source": [
        "# glove embeddings\n",
        "\n",
        "import pickle\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from create_embeddings import get_gloves_dict, get_lines_gloves, get_plong_corpus\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "vocab_queries = vectorizer.fit(questions_text).vocabulary_\n",
        "\n",
        "glove_dict = get_gloves_dict()\n",
        "key_set = set(vocab_queries.keys()) & set(glove_dict.keys())\n",
        "glove_dict_vocab_corpus = {key: glove_dict[key] for key in key_set}\n",
        "del vocab_queries\n",
        "plongement_queries = get_plong_corpus(questions_text, glove_dict_vocab_corpus)\n",
        "\n",
        "with open('glove_query_emb_msmarco.txt', 'wb') as fp:\n",
        "    pickle.dump(plongement_queries, fp)\n",
        "# del plongement_queries\n",
        "# df.drop(['query'], axis=1, inplace=True)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/create_embeddings.py:123: RuntimeWarning: invalid value encountered in true_divide\n",
            "  return vec / sum(temp_.values())\n",
            "/content/create_embeddings.py:123: RuntimeWarning: invalid value encountered in true_divide\n",
            "  return vec / sum(temp_.values())\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fh5cDKTfb5xz"
      },
      "source": [
        "import pickle\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from create_embeddings import get_gloves_dict, get_lines_gloves, get_plong_corpus\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "vocab_docs = vectorizer.fit(paragraphs_text).vocabulary_\n",
        "\n",
        "glove_dict = get_gloves_dict()\n",
        "key_set = set(vocab_docs.keys()) & set(glove_dict.keys())\n",
        "glove_dict_vocab_corpus = {key: glove_dict[key] for key in key_set}\n",
        "del vocab_docs\n",
        "plongement_doc = get_plong_corpus(paragraphs_text, glove_dict_vocab_corpus)\n",
        "\n",
        "with open('glove_doc_emb_msmarco.txt', 'wb') as fp:\n",
        "    pickle.dump(plongement_doc, fp)\n",
        "# del plongement_doc"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OmraS4ZYS81"
      },
      "source": [
        "import pickle\n",
        "\n",
        "with open('glove_query_emb_msmarco.txt', 'rb') as fp:\n",
        "    plongement_queries = pickle.load(fp)\n",
        "\n",
        "with open('glove_doc_emb_msmarco.txt', 'rb') as fp:\n",
        "    plongement_doc = pickle.load(fp)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fZElGzP7yig",
        "outputId": "65652a41-6372-4f98-e591-74e2a2df4bc7"
      },
      "source": [
        "# bert embeddings\n",
        "\n",
        "from scipy import spatial\n",
        "from sent2vec.vectorizer import Vectorizer\n",
        "\n",
        "vectorizer = Vectorizer()\n",
        "vectorizer.bert(paragraphs_text)\n",
        "docs_bert = vectorizer.vectors\n",
        "\n",
        "vectorizer = Vectorizer()\n",
        "vectorizer.bert(questions_text)\n",
        "queries_bert = vectorizer.vectors"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1617 > 512). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6QMUpaDBQCj"
      },
      "source": [
        "import pickle\n",
        "\n",
        "with open('bert_query_emb_msmarco.txt', 'wb') as fp:\n",
        "    pickle.dump(queries_bert, fp)\n",
        "\n",
        "with open('bert_doc_emb_msmarco.txt', 'wb') as fp:\n",
        "    pickle.dump(docs_bert, fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKNbPlimXqw0"
      },
      "source": [
        "import tensorflow as tf\n",
        "from keras.layers import Dense, Embedding, Input\n",
        "from keras.models import Sequential, Model\n",
        "from keras.callbacks import EarlyStopping\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(np.concatenate((plongement_queries, plongement_doc), axis=1), y, test_size=0.10, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(np.concatenate((msmarco_questions_tfidf, msmarco_paragraphs_tfidf), axis=1), y, test_size=0.10, random_state=42)\n",
        "# X_train, X_test, y_train, y_test = train_test_split(np.concatenate((queries_bert, docs_bert), axis=1), y, test_size=0.20, random_state=42)\n",
        "\n",
        "def MLPregression(X, y, ndims):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(100, input_dim=ndims, activation='relu'))\n",
        "    model.add(Dense(50, activation='relu'))\n",
        "    model.add(Dense(10, activation='relu'))\n",
        "    model.add(Dense(1, activation='linear'))\n",
        "    model.compile(optimizer=\"adam\", loss=\"mean_squared_error\", metrics=['mse'])\n",
        "\n",
        "    # stop = EarlyStopping(monitor='val_accuracy', patience=2, restore_best_weights=True)\n",
        "    model.fit(X, y, epochs=25, batch_size=100, validation_split=0.15) #, callbacks=[stop])\n",
        "    return model\n",
        "    \n",
        "MLPmodel = MLPregression(X_train, np.array(y_train), max_feat*2)\n",
        "preds = MLPmodel.predict(X_test)\n",
        "\n",
        "MLPmodel.save('/content/drive/MyDrive/Colab Notebooks/INF8460/Project/mlp_regression_msmarco_tfidf_8000*2.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_U_0u6QeKOlM",
        "outputId": "37e3c074-add1-42c3-a435-1e24a28262e9"
      },
      "source": [
        "print(np.array(y_test)[:5])\n",
        "print(preds[:5])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.74325098 0.78950238 0.78060363 0.78547908 0.74975728]\n",
            "[[0.7506879 ]\n",
            " [0.7999193 ]\n",
            " [0.7503954 ]\n",
            " [0.78310984]\n",
            " [0.7624861 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-4W_BjDa9V9"
      },
      "source": [
        "import keras\n",
        "\n",
        "reconstructed_MLPmodel = keras.models.load_model('/content/drive/MyDrive/Colab Notebooks/INF8460/Project/mlp_regression_msmarco_tfidf_4957*2.h5')\n",
        "\n",
        "for i, query in enumerate(val_questions_tfidf):\n",
        "    test_sentence_pairs = []\n",
        "    for doc in paragraphs_tfidf:\n",
        "        test_sentence_pairs.append(query + doc)\n",
        "    # np.concatenate([query for i in range(len(paragraphs_tfidf))], paragraphs_tfidf)\n",
        "\n",
        "    preds = reconstructed_MLPmodel.predict(test_sentence_pairs)\n",
        "\n",
        "    results = [(i, x, y) for (x, y), z in zip(paragraphs_ids, preds)]\n",
        "    results.sort(key=itemgetter(2), reverse=True)\n",
        "    \n",
        "    print(query)\n",
        "    print('\\n')\n",
        "    for line in results[:20]:\n",
        "        print(line[1], line[2])\n",
        "\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f02FORmkLEzj"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## **3. Extraction de reponse**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EN46aA0ALL-a"
      },
      "source": [
        "\n",
        "\n",
        "> #### Pytorch BERT squad 2.0\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "woIK3xzCLLSR",
        "outputId": "8d723460-1673-4efe-9566-5716272386ad"
      },
      "source": [
        "!git clone https://github.com/surbhardwaj/BERT-QnA-Squad_2.0_Finetuned_Model.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'BERT-QnA-Squad_2.0_Finetuned_Model'...\n",
            "remote: Enumerating objects: 60, done.\u001b[K\n",
            "remote: Total 60 (delta 0), reused 0 (delta 0), pack-reused 60\u001b[K\n",
            "Unpacking objects: 100% (60/60), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjYXIkGxPzdz"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import argparse\n",
        "from pytorch_pretrained_bert.tokenization import (BasicTokenizer,\n",
        "                                                  BertTokenizer,whitespace_tokenize)\n",
        "import collections\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset\n",
        "from pytorch_pretrained_bert.modeling import BertForQuestionAnswering, BertConfig\n",
        "import math\n",
        "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
        "                              TensorDataset)\n",
        "from tqdm import tqdm\n",
        "from termcolor import colored, cprint\n",
        "\n",
        "\n",
        "class SquadExample(object):\n",
        "    \"\"\"\n",
        "    A single training/test example for the Squad dataset.\n",
        "    For examples without an answer, the start and end position are -1.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 example_id,\n",
        "                 para_text,\n",
        "                 qas_id,\n",
        "                 question_text,\n",
        "                 doc_tokens,\n",
        "                unique_id):\n",
        "        self.qas_id = qas_id\n",
        "        self.question_text = question_text\n",
        "        self.doc_tokens = doc_tokens\n",
        "        self.example_id = example_id\n",
        "        self.para_text = para_text\n",
        "        self.unique_id = unique_id\n",
        "        \n",
        "\n",
        "    def __str__(self):\n",
        "        return self.__repr__()\n",
        "\n",
        "    def __repr__(self):\n",
        "        s = \"\"\n",
        "        s += \"qas_id: %s\" % (self.qas_id)\n",
        "        s += \", question_text: %s\" % (\n",
        "            self.question_text)\n",
        "        s += \", doc_tokens: [%s]\" % (\" \".join(self.doc_tokens))\n",
        "        \n",
        "        return s\n",
        "\n",
        "\n",
        "\n",
        "### Convert paragraph to tokens and returns question_text\n",
        "def read_squad_examples(input_data):\n",
        "    \"\"\"Read a SQuAD json file into a list of SquadExample.\"\"\"\n",
        "    def is_whitespace(c):\n",
        "        if c == \" \" or c == \"\\t\" or c == \"\\r\" or c == \"\\n\" or ord(c) == 0x202F:\n",
        "            return True\n",
        "        return False\n",
        "    i = 0\n",
        "    examples = []\n",
        "    for entry in input_data:\n",
        "        example_id = entry['id']\n",
        "        paragraph_text = entry['text']\n",
        "        doc_tokens = []\n",
        "        prev_is_whitespace = True\n",
        "        for c in paragraph_text:\n",
        "            if is_whitespace(c):\n",
        "                prev_is_whitespace = True\n",
        "            else:\n",
        "                if prev_is_whitespace:\n",
        "                    doc_tokens.append(c)\n",
        "                else:\n",
        "                    doc_tokens[-1] += c\n",
        "                prev_is_whitespace = False\n",
        "            \n",
        "        for qa in entry['ques']:\n",
        "            qas_id = i\n",
        "            question_text = qa\n",
        "            \n",
        "            \n",
        "            example = SquadExample(example_id = example_id,\n",
        "                    qas_id=qas_id,\n",
        "                    para_text = paragraph_text,               \n",
        "                    question_text=question_text,\n",
        "                    doc_tokens=doc_tokens,\n",
        "                    unique_id = i)\n",
        "            i+=1\n",
        "            examples.append(example)\n",
        "\n",
        "    return examples\n",
        "\n",
        "\n",
        "\n",
        "def _check_is_max_context(doc_spans, cur_span_index, position):\n",
        "    \"\"\"Check if this is the 'max context' doc span for the token.\"\"\"\n",
        "    best_score = None\n",
        "    best_span_index = None\n",
        "    for (span_index, doc_span) in enumerate(doc_spans):\n",
        "        end = doc_span.start + doc_span.length - 1\n",
        "        if position < doc_span.start:\n",
        "            continue\n",
        "        if position > end:\n",
        "            continue\n",
        "        num_left_context = position - doc_span.start\n",
        "        num_right_context = end - position\n",
        "        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n",
        "        if best_score is None or score > best_score:\n",
        "            best_score = score\n",
        "            best_span_index = span_index\n",
        "\n",
        "    return cur_span_index == best_span_index\n",
        "\n",
        "class InputFeatures(object):\n",
        "    \"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 unique_id,\n",
        "                 example_index,\n",
        "                 doc_span_index,\n",
        "                 tokens,\n",
        "                 token_is_max_context,\n",
        "                 token_to_orig_map,\n",
        "                 input_ids,\n",
        "                 input_mask,\n",
        "                 segment_ids):\n",
        " \n",
        "        self.doc_span_index = doc_span_index\n",
        "        self.unique_id = unique_id\n",
        "        self.example_index = example_index\n",
        "        self.tokens = tokens\n",
        "        self.token_is_max_context = token_is_max_context\n",
        "        self.token_to_orig_map = token_to_orig_map\n",
        "        self.input_ids = input_ids\n",
        "        self.input_mask = input_mask\n",
        "        self.segment_ids = segment_ids\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def convert_examples_to_features(examples, tokenizer, max_seq_length,\n",
        "                                 doc_stride, max_query_length):\n",
        "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
        "\n",
        "\n",
        "    features = []\n",
        "    unique_id = 1\n",
        "    for (example_index, example) in enumerate(examples):\n",
        "        query_tokens = tokenizer.tokenize(example.question_text)\n",
        "        ### Truncate the query if query length > max_query_length..\n",
        "        if len(query_tokens) > max_query_length:\n",
        "            query_tokens = query_tokens[0:max_query_length]\n",
        "\n",
        "        tok_to_orig_index = []\n",
        "        orig_to_tok_index = []\n",
        "        all_doc_tokens = []\n",
        "        for (i, token) in enumerate(example.doc_tokens):\n",
        "            orig_to_tok_index.append(len(all_doc_tokens))\n",
        "            sub_tokens = tokenizer.tokenize(token)\n",
        "            for sub_token in sub_tokens:\n",
        "                tok_to_orig_index.append(i)\n",
        "                all_doc_tokens.append(sub_token)\n",
        "\n",
        "        tok_start_position = None\n",
        "        tok_end_position = None\n",
        "\n",
        "        max_tokens_for_doc = max_seq_length - len(query_tokens) - 3\n",
        "    \n",
        "\n",
        "        # We can have documents that are longer than the maximum sequence length.\n",
        "        # To deal with this we do a sliding window approach, where we take chunks\n",
        "        # of the up to our max length with a stride of `doc_stride`.\n",
        "        _DocSpan = collections.namedtuple(  # pylint: disable=invalid-name\n",
        "            \"DocSpan\", [\"start\", \"length\"])\n",
        "        doc_spans = []\n",
        "        start_offset = 0\n",
        "        while start_offset < len(all_doc_tokens):\n",
        "            length = len(all_doc_tokens) - start_offset\n",
        "            if length > max_tokens_for_doc:\n",
        "                length = max_tokens_for_doc\n",
        "            doc_spans.append(_DocSpan(start=start_offset, length=length))\n",
        "            if start_offset + length == len(all_doc_tokens):\n",
        "                break\n",
        "            start_offset += min(length, doc_stride)\n",
        "\n",
        "        for (doc_span_index, doc_span) in enumerate(doc_spans):\n",
        "            tokens = []\n",
        "            token_to_orig_map = {}\n",
        "            token_is_max_context = {}\n",
        "            segment_ids = []\n",
        "            tokens.append(\"[CLS]\")\n",
        "            segment_ids.append(0)\n",
        "            for token in query_tokens:\n",
        "                tokens.append(token)\n",
        "                segment_ids.append(0)\n",
        "            tokens.append(\"[SEP]\")\n",
        "            segment_ids.append(0)\n",
        "\n",
        "            for i in range(doc_span.length):\n",
        "                split_token_index = doc_span.start + i\n",
        "                token_to_orig_map[len(tokens)] = tok_to_orig_index[split_token_index]\n",
        "\n",
        "                is_max_context = _check_is_max_context(doc_spans, doc_span_index,\n",
        "                                                       split_token_index)\n",
        "                token_is_max_context[len(tokens)] = is_max_context\n",
        "                tokens.append(all_doc_tokens[split_token_index])\n",
        "                segment_ids.append(1)\n",
        "            tokens.append(\"[SEP]\")\n",
        "            segment_ids.append(1)\n",
        "\n",
        "\n",
        "            input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "                # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "                # tokens are attended to.\n",
        "            input_mask = [1] * len(input_ids)\n",
        "\n",
        "            # Zero-pad up to the sequence length.\n",
        "            while len(input_ids) < max_seq_length:\n",
        "                input_ids.append(0)\n",
        "                input_mask.append(0)\n",
        "                segment_ids.append(0)\n",
        "\n",
        "            assert len(input_ids) == max_seq_length\n",
        "            assert len(input_mask) == max_seq_length\n",
        "            assert len(segment_ids) == max_seq_length\n",
        "\n",
        "            \n",
        "            features.append(InputFeatures(unique_id = unique_id,\n",
        "                            example_index = example_index,\n",
        "                            doc_span_index=doc_span_index,\n",
        "                            tokens=tokens,   \n",
        "                            token_is_max_context=token_is_max_context,\n",
        "                            token_to_orig_map=token_to_orig_map,\n",
        "                            input_ids=input_ids,\n",
        "                            input_mask=input_mask,\n",
        "                            segment_ids=segment_ids))\n",
        "            unique_id += 1\n",
        "\n",
        "            \n",
        "    \n",
        "    return features\n",
        "\n",
        "\n",
        "def _get_best_indexes(logits, n_best_size):\n",
        "    \"\"\"Get the n-best logits from a list.\"\"\"\n",
        "    index_and_score = sorted(enumerate(logits), key=lambda x: x[1], reverse=True)\n",
        "    best_indexes = []\n",
        "    for i in range(len(index_and_score)):\n",
        "        if i >= n_best_size:\n",
        "            break\n",
        "        best_indexes.append(index_and_score[i][0])\n",
        "   \n",
        "    return best_indexes\n",
        "\n",
        "\n",
        "\n",
        "def get_final_text(pred_text, orig_text, do_lower_case, verbose_logging=False):\n",
        "    \"\"\"Project the tokenized prediction back to the original text.\"\"\"\n",
        "\n",
        "    def _strip_spaces(text):\n",
        "        ns_chars = []\n",
        "        ns_to_s_map = collections.OrderedDict()\n",
        "        for (i, c) in enumerate(text):\n",
        "            if c == \" \":\n",
        "                continue\n",
        "            ns_to_s_map[len(ns_chars)] = i\n",
        "            ns_chars.append(c)\n",
        "        ns_text = \"\".join(ns_chars)\n",
        "        return (ns_text, ns_to_s_map)\n",
        "\n",
        "    \n",
        "    tokenizer = BasicTokenizer(do_lower_case=do_lower_case)\n",
        "\n",
        "    tok_text = \" \".join(tokenizer.tokenize(orig_text))\n",
        "\n",
        "    start_position = tok_text.find(pred_text)\n",
        "    if start_position == -1:\n",
        "        if verbose_logging:\n",
        "            logger.info(\n",
        "                \"Unable to find text: '%s' in '%s'\" % (pred_text, orig_text))\n",
        "            print(\"no answer\")\n",
        "        return orig_text\n",
        "    end_position = start_position + len(pred_text) - 1\n",
        "\n",
        "    (orig_ns_text, orig_ns_to_s_map) = _strip_spaces(orig_text)\n",
        "    (tok_ns_text, tok_ns_to_s_map) = _strip_spaces(tok_text)\n",
        "\n",
        "    if len(orig_ns_text) != len(tok_ns_text):\n",
        "        if verbose_logging:\n",
        "            logger.info(\"Length not equal after stripping spaces: '%s' vs '%s'\",\n",
        "                        orig_ns_text, tok_ns_text)\n",
        "        return orig_text\n",
        "\n",
        "    # We then project the characters in `pred_text` back to `orig_text` using\n",
        "    # the character-to-character alignment.\n",
        "    tok_s_to_ns_map = {}\n",
        "    for (i, tok_index) in tok_ns_to_s_map.items():\n",
        "        tok_s_to_ns_map[tok_index] = i\n",
        "\n",
        "    orig_start_position = None\n",
        "    if start_position in tok_s_to_ns_map:\n",
        "        ns_start_position = tok_s_to_ns_map[start_position]\n",
        "        if ns_start_position in orig_ns_to_s_map:\n",
        "            orig_start_position = orig_ns_to_s_map[ns_start_position]\n",
        "\n",
        "    if orig_start_position is None:\n",
        "        if verbose_logging:\n",
        "            logger.info(\"Couldn't map start position\")\n",
        "        return orig_text\n",
        "\n",
        "    orig_end_position = None\n",
        "    if end_position in tok_s_to_ns_map:\n",
        "        ns_end_position = tok_s_to_ns_map[end_position]\n",
        "        if ns_end_position in orig_ns_to_s_map:\n",
        "            orig_end_position = orig_ns_to_s_map[ns_end_position]\n",
        "\n",
        "    if orig_end_position is None:\n",
        "        if verbose_logging:\n",
        "            logger.info(\"Couldn't map end position\")\n",
        "        return orig_text\n",
        "\n",
        "    output_text = orig_text[orig_start_position:(orig_end_position + 1)]\n",
        "    return output_text\n",
        "\n",
        "    \n",
        "    \n",
        "_PrelimPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n",
        "        \"PrelimPrediction\",\n",
        "[\"feature_index\", \"start_index\", \"end_index\", \"start_logit\", \"end_logit\"])\n",
        "\n",
        "\n",
        "def _compute_softmax(scores):\n",
        "    \"\"\"Compute softmax probability over raw logits.\"\"\"\n",
        "    if not scores:\n",
        "        return []\n",
        "\n",
        "    max_score = None\n",
        "    for score in scores:\n",
        "        if max_score is None or score > max_score:\n",
        "            max_score = score\n",
        "\n",
        "    exp_scores = []\n",
        "    total_sum = 0.0\n",
        "    for score in scores:\n",
        "        x = math.exp(score - max_score)\n",
        "        exp_scores.append(x)\n",
        "        total_sum += x\n",
        "\n",
        "    probs = []\n",
        "    for score in exp_scores:\n",
        "        probs.append(score / total_sum)\n",
        "    return probs\n",
        "\n",
        "\n",
        "_NbestPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n",
        "    \"NbestPrediction\", [\"text\", \"start_logit\", \"end_logit\"])\n",
        "\n",
        "def predict(examples, all_features, all_results, max_answer_length, thresh):\n",
        "\n",
        "    n_best_size = 10\n",
        "    \n",
        "    ### Adding index to feature ###\n",
        "    example_index_to_features = collections.defaultdict(list)\n",
        "    for feature in all_features:\n",
        "        example_index_to_features[feature.example_index].append(feature)\n",
        "     \n",
        "    unique_id_to_result = {}\n",
        "    for result in all_results:\n",
        "        unique_id_to_result[result.unique_id] = result\n",
        "        \n",
        "        \n",
        "    all_predictions = collections.OrderedDict()\n",
        "   \n",
        "    \n",
        "    \n",
        "    for example in examples:\n",
        "        index = 0\n",
        "        features = example_index_to_features[example.unique_id]\n",
        "        prelim_predictions = []\n",
        "       \n",
        "        for (feature_index, feature) in enumerate(features):\n",
        "            result = unique_id_to_result[feature.unique_id]\n",
        "            start_indexes = _get_best_indexes(result.start_logits, n_best_size)\n",
        "            end_indexes = _get_best_indexes(result.end_logits, n_best_size)\n",
        "            for start_index in start_indexes:\n",
        "                    for end_index in end_indexes:\n",
        "                     #### we remove the indexes which are invalid @\n",
        "                        if start_index >= len(feature.tokens):\n",
        "                            continue\n",
        "                        if end_index >= len(feature.tokens):\n",
        "\n",
        "                            continue\n",
        "                        if start_index not in feature.token_to_orig_map:\n",
        "                            continue\n",
        "                        if end_index not in feature.token_to_orig_map:\n",
        "                            continue\n",
        "                        if not feature.token_is_max_context.get(start_index, False):\n",
        "                            continue\n",
        "                        if end_index < start_index:\n",
        "                            continue\n",
        "                        length = end_index - start_index + 1\n",
        "                        if length > max_answer_length:\n",
        "                            continue\n",
        "\n",
        "                        prelim_predictions.append(\n",
        "                                        _PrelimPrediction(\n",
        "                                            feature_index=feature_index,\n",
        "                                            start_index=start_index,\n",
        "                                            end_index=end_index,\n",
        "                                            start_logit=result.start_logits[start_index],\n",
        "                                            end_logit=result.end_logits[end_index]))\n",
        "\n",
        "\n",
        "        prelim_predictions = sorted(\n",
        "            prelim_predictions,\n",
        "            key=lambda x: (x.start_logit + x.end_logit),\n",
        "            reverse=True) \n",
        "            \n",
        "    \n",
        "         \n",
        "        seen_predictions = {}\n",
        "        nbest = []\n",
        "        for pred in prelim_predictions:\n",
        "            if len(nbest) >= n_best_size:\n",
        "                break\n",
        "                \n",
        "            feature = features[pred.feature_index]\n",
        "            if pred.start_index > 0:  # this is a non-null prediction\n",
        "                tok_tokens = feature.tokens[pred.start_index:(pred.end_index + 1)]\n",
        "                orig_doc_start = feature.token_to_orig_map[pred.start_index]\n",
        "                orig_doc_end = feature.token_to_orig_map[pred.end_index]\n",
        "                orig_tokens = example.doc_tokens[orig_doc_start:(orig_doc_end + 1)]\n",
        "                tok_text = \" \".join(tok_tokens)\n",
        "\n",
        "                # De-tokenize WordPieces that have been split off.\n",
        "                tok_text = tok_text.replace(\" ##\", \"\")\n",
        "                tok_text = tok_text.replace(\"##\", \"\")\n",
        "\n",
        "                # Clean whitespace\n",
        "                tok_text = tok_text.strip()\n",
        "                tok_text = \" \".join(tok_text.split())\n",
        "                orig_text = \" \".join(orig_tokens)\n",
        "\n",
        "                final_text = get_final_text(tok_text, orig_text, True)\n",
        "                if final_text in seen_predictions:\n",
        "                    continue\n",
        "\n",
        "                seen_predictions[final_text] = True\n",
        "            else:\n",
        "                final_text = \"\"\n",
        "                seen_predictions[final_text] = True\n",
        "\n",
        "            nbest.append(\n",
        "                _NbestPrediction(\n",
        "                    text=final_text,\n",
        "                    start_logit=pred.start_logit,\n",
        "                    end_logit=pred.end_logit))\n",
        "        \n",
        "        \n",
        "    \n",
        "        if not nbest:\n",
        "                nbest.append(\n",
        "                    _NbestPrediction(text=\"empty\", start_logit=0.0, end_logit=0.0))\n",
        "\n",
        "        assert len(nbest) >= 1\n",
        "        \n",
        "        \n",
        "        total_scores = []\n",
        "        best_non_null_entry = None\n",
        "        for entry in nbest:\n",
        "            total_scores.append(entry.start_logit + entry.end_logit)\n",
        "            if not best_non_null_entry:\n",
        "                if entry.text:\n",
        "                    best_non_null_entry = entry\n",
        "\n",
        "    \n",
        "        probs = _compute_softmax(total_scores)\n",
        "        nbest_json = []\n",
        "        for (i, entry) in enumerate(nbest):\n",
        "\n",
        "            output = collections.OrderedDict()\n",
        "            output[\"text\"] = entry.text if probs[i] > thresh else \"<No Answer>\"\n",
        "            output[\"probability\"] = probs[i]\n",
        "            output[\"start_logit\"] = entry.start_logit if probs[i] > thresh else -1\n",
        "            output[\"end_logit\"] = entry.end_logit if probs[i] > thresh else -1\n",
        "            nbest_json.append(output)\n",
        "\n",
        "        assert len(nbest_json) >= 1\n",
        "        all_predictions[example] = nbest_json[0][\"text\"]\n",
        "        index=+1\n",
        "    return all_predictions\n",
        "        \n",
        "\n",
        "                \n",
        "\n",
        "RawResult = collections.namedtuple(\"RawResult\",\n",
        "                                   [\"unique_id\", \"start_logits\", \"end_logits\"])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def main(para_file=\"BERT-QnA-Squad_2.0_Finetuned_Model/Input_file.txt\"):\n",
        "    \n",
        "    # args = parser.parse_args()\n",
        "    model_path = \"/content/drive/My Drive/Colab Notebooks/INF8460/Project/pytorch_model.bin\"\n",
        "    \n",
        "    if torch.cuda.is_available:\n",
        "        print('GPU available')\n",
        "    else:\n",
        "        print('Please set GPU')\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available else \"cpu\")\n",
        "    n_gpu = torch.cuda.device_count()\n",
        "    \n",
        "    ### Raeding paragraph\n",
        "    f = open(para_file, 'r')\n",
        "    para = f.read()\n",
        "    f.close()\n",
        "    \n",
        "    ## Reading question\n",
        "#     f = open(ques_file, 'r')\n",
        "#     ques = f.read()\n",
        "#     f.close()\n",
        "    \n",
        "    para_list = para.split('\\n\\n')\n",
        "    \n",
        "    input_data = []\n",
        "    i = 1\n",
        "    for para in para_list :\n",
        "        paragraphs = {}\n",
        "        splits = para.split('\\nQuestions:')\n",
        "        paragraphs['id'] = i\n",
        "        paragraphs['text'] = splits[0].replace('Paragraph:', '').strip('\\n')\n",
        "        paragraphs['ques']=splits[1].lstrip('\\n').split('\\n')\n",
        "        input_data.append(paragraphs)\n",
        "        i+=1\n",
        "       \n",
        "    \n",
        "    ## input_data is a list of dictionary which has a paragraph and questions\n",
        "\n",
        "    \n",
        "    examples = read_squad_examples(input_data)\n",
        "    # print(examples)\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "    \n",
        "    \n",
        "    eval_features = convert_examples_to_features(\n",
        "            examples = examples,\n",
        "            tokenizer=tokenizer,\n",
        "            max_seq_length=384,\n",
        "            doc_stride=128,\n",
        "            max_query_length=64)\n",
        "    \n",
        "    # print(eval_features)\n",
        "    \n",
        "    \n",
        "    all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n",
        "    all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n",
        "    all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n",
        "    all_example_index = torch.arange(all_input_ids.size(0), dtype=torch.long)\n",
        "    \n",
        "    ### Loading Pretrained model for QnA \n",
        "    config = BertConfig(\"BERT-QnA-Squad_2.0_Finetuned_Model//Results/bert_config.json\")\n",
        "    model = BertForQuestionAnswering(config)\n",
        "    model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
        "    model.to(device)\n",
        "   \n",
        "\n",
        "    pred_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_example_index)\n",
        "    # Run prediction for full data\n",
        "    pred_sampler = SequentialSampler(pred_data)\n",
        "    pred_dataloader = DataLoader(pred_data, sampler=pred_sampler, batch_size=9)\n",
        "    \n",
        "    predictions = []\n",
        "    for input_ids, input_mask, segment_ids, example_indices in tqdm(pred_dataloader):\n",
        "        input_ids = input_ids.to(device)\n",
        "        input_mask = input_mask.to(device)\n",
        "        segment_ids = segment_ids.to(device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            batch_start_logits, batch_end_logits = model(input_ids, segment_ids, input_mask)\n",
        "            \n",
        "    \n",
        "        features=[]\n",
        "        example = []\n",
        "        all_results = []\n",
        "       \n",
        "        for i, example_index in enumerate(example_indices):\n",
        "                start_logits = batch_start_logits[i].detach().cpu().tolist()\n",
        "                end_logits =   batch_end_logits[i].detach().cpu().tolist()\n",
        "                feature = eval_features[example_index.item()]\n",
        "                unique_id = int(feature.unique_id)\n",
        "                features.append(feature)\n",
        "                all_results.append(RawResult(unique_id=unique_id,\n",
        "                                             start_logits=start_logits,\n",
        "                                             end_logits=end_logits))\n",
        "                \n",
        "       \n",
        "        output = predict(examples, features, all_results, 30, 0.25)\n",
        "        print('Output:\\n')\n",
        "        print(output)\n",
        "        print('\\n')\n",
        "        predictions.append(output)\n",
        " \n",
        "   \n",
        "    ### For printing the results ####\n",
        "    index = None\n",
        "    for example in examples:\n",
        "        if index!= example.example_id:\n",
        "            # print(example.para_text)\n",
        "            index = example.example_id\n",
        "            print('\\n')\n",
        "            print(colored('***********Question and Answers *************', 'red'))\n",
        "          \n",
        "        ques_text = colored(example.question_text, 'blue')\n",
        "        print(ques_text)\n",
        "        prediction = colored(predictions[math.floor(example.unique_id/12)][example], 'green', attrs=['reverse', 'blink'])\n",
        "        print(prediction)\n",
        "        print('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PaxFAIIMP4eS",
        "outputId": "81e9ae9c-31fe-4929-9c62-4129d4aceace"
      },
      "source": [
        "main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU available\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  5.38it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Output:\n",
            "\n",
            "OrderedDict([(qas_id: 0, question_text: What is the formula of alkyl group?, doc_tokens: [Ethers are a class of organic compounds that contain an ether group—an oxygen atom connected to two alkyl or aryl groups. They have the general formula R–O–R′, where R and R′ represent the alkyl or aryl groups. Ethers can again be classified into two varieties: if the alkyl groups are the same on both sides of the oxygen atom, then it is a simple or symmetrical ether, whereas if they are different, the ethers are called mixed or unsymmetrical ethers. A typical example of the first group is the solvent and anesthetic diethyl ether, commonly referred to simply as \"ether\" (CH3–CH2–O–CH2–CH3). Ethers are common in organic chemistry and even more prevalent in biochemistry, as they are common linkages in carbohydrates and lignin.], 'R–O–R′'), (qas_id: 1, question_text: What is symmetrical ether ?, doc_tokens: [Ethers are a class of organic compounds that contain an ether group—an oxygen atom connected to two alkyl or aryl groups. They have the general formula R–O–R′, where R and R′ represent the alkyl or aryl groups. Ethers can again be classified into two varieties: if the alkyl groups are the same on both sides of the oxygen atom, then it is a simple or symmetrical ether, whereas if they are different, the ethers are called mixed or unsymmetrical ethers. A typical example of the first group is the solvent and anesthetic diethyl ether, commonly referred to simply as \"ether\" (CH3–CH2–O–CH2–CH3). Ethers are common in organic chemistry and even more prevalent in biochemistry, as they are common linkages in carbohydrates and lignin.], 'if the alkyl groups are the same on both sides of the oxygen atom'), (qas_id: 2, question_text: What does BERT achieves ?, doc_tokens: [BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE benchmark to 80.4% (7.6% absolute improvement), MultiNLI accuracy to 86.7% (5.6% absolute improvement) and the SQuAD v1.1 question answering Test F1 to 93.2 (1.5 absolute improvement), outperforming human performance by 2.0.], 'new state-of-the-art results'), (qas_id: 3, question_text: What is the BERT's accuracy on MultiNLI ?, doc_tokens: [BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE benchmark to 80.4% (7.6% absolute improvement), MultiNLI accuracy to 86.7% (5.6% absolute improvement) and the SQuAD v1.1 question answering Test F1 to 93.2 (1.5 absolute improvement), outperforming human performance by 2.0.], '86.7%'), (qas_id: 4, question_text: What is Bert's F1 score on Squad v1.1 ?, doc_tokens: [BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE benchmark to 80.4% (7.6% absolute improvement), MultiNLI accuracy to 86.7% (5.6% absolute improvement) and the SQuAD v1.1 question answering Test F1 to 93.2 (1.5 absolute improvement), outperforming human performance by 2.0.], '93.2')])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[31m***********Question and Answers *************\u001b[0m\n",
            "\u001b[34mWhat is the formula of alkyl group?\u001b[0m\n",
            "\u001b[5m\u001b[7m\u001b[32mR–O–R′\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[34mWhat is symmetrical ether ?\u001b[0m\n",
            "\u001b[5m\u001b[7m\u001b[32mif the alkyl groups are the same on both sides of the oxygen atom\u001b[0m\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[31m***********Question and Answers *************\u001b[0m\n",
            "\u001b[34mWhat does BERT achieves ?\u001b[0m\n",
            "\u001b[5m\u001b[7m\u001b[32mnew state-of-the-art results\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[34mWhat is the BERT's accuracy on MultiNLI ?\u001b[0m\n",
            "\u001b[5m\u001b[7m\u001b[32m86.7%\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[34mWhat is Bert's F1 score on Squad v1.1 ?\u001b[0m\n",
            "\u001b[5m\u001b[7m\u001b[32m93.2\u001b[0m\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBW9bndfP9K2",
        "outputId": "70360b66-ec7d-473c-a5de-9a8bf75b1ef3"
      },
      "source": [
        "main(\"sample_project.txt\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU available\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  4.60it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Output:\n",
            "\n",
            "OrderedDict([(qas_id: 0, question_text: Who heads an office or division?, doc_tokens: [In 1982, Regis McKenna was brought in to shape the marketing and launch of the Macintosh. Later the Regis McKenna team grew to include Jane Anderson, Katie Cadigan and Andy Cunningham, who eventually led the Apple account for the agency. Cunningham and Anderson were the primary authors of the Macintosh launch plan. The launch of the Macintosh pioneered many different tactics that are used today in launching technology products, including the \"multiple exclusive,\" event marketing (credited to John Sculley, who brought the concept over from Pepsi), creating a mystique around a product and giving an inside look into a product's creation.], 'Regis McKenna'), (qas_id: 1, question_text: Who leaders the sub-divisions of offices or divisions?, doc_tokens: [In 1982, Regis McKenna was brought in to shape the marketing and launch of the Macintosh. Later the Regis McKenna team grew to include Jane Anderson, Katie Cadigan and Andy Cunningham, who eventually led the Apple account for the agency. Cunningham and Anderson were the primary authors of the Macintosh launch plan. The launch of the Macintosh pioneered many different tactics that are used today in launching technology products, including the \"multiple exclusive,\" event marketing (credited to John Sculley, who brought the concept over from Pepsi), creating a mystique around a product and giving an inside look into a product's creation.], 'Regis McKenna'), (qas_id: 2, question_text: What helped Avicenna forget the Metaphysics of Aristotle?, doc_tokens: [In 1982, Regis McKenna was brought in to shape the marketing and launch of the Macintosh. Later the Regis McKenna team grew to include Jane Anderson, Katie Cadigan and Andy Cunningham, who eventually led the Apple account for the agency. Cunningham and Anderson were the primary authors of the Macintosh launch plan. The launch of the Macintosh pioneered many different tactics that are used today in launching technology products, including the \"multiple exclusive,\" event marketing (credited to John Sculley, who brought the concept over from Pepsi), creating a mystique around a product and giving an inside look into a product's creation.], '<No Answer>'), (qas_id: 3, question_text: Besides using 3kV DC what other power type is used in the former Soviet Union countries?, doc_tokens: [3 kV DC is used in Belgium, Italy, Spain, Poland, the northern Czech Republic, Slovakia, Slovenia, South Africa, Chile, and former Soviet Union countries (also using 25 kV 50 Hz AC). It was formerly used by the Milwaukee Road from Harlowton, Montana to Seattle-Tacoma, across the Continental Divide and including extensive branch and loop lines in Montana, and by the Delaware, Lackawanna & Western Railroad (now New Jersey Transit, converted to 25 kV AC) in the United States, and the Kolkata suburban railway (Bardhaman Main Line) in India, before it was converted to 25 kV 50 Hz AC.], '25 kV 50 Hz AC'), (qas_id: 4, question_text: What does the railway system of US use DC or AC?, doc_tokens: [3 kV DC is used in Belgium, Italy, Spain, Poland, the northern Czech Republic, Slovakia, Slovenia, South Africa, Chile, and former Soviet Union countries (also using 25 kV 50 Hz AC). It was formerly used by the Milwaukee Road from Harlowton, Montana to Seattle-Tacoma, across the Continental Divide and including extensive branch and loop lines in Montana, and by the Delaware, Lackawanna & Western Railroad (now New Jersey Transit, converted to 25 kV AC) in the United States, and the Kolkata suburban railway (Bardhaman Main Line) in India, before it was converted to 25 kV 50 Hz AC.], '25 kV AC'), (qas_id: 5, question_text: What helped Avicenna forget the Metaphysics of Aristotle?, doc_tokens: [3 kV DC is used in Belgium, Italy, Spain, Poland, the northern Czech Republic, Slovakia, Slovenia, South Africa, Chile, and former Soviet Union countries (also using 25 kV 50 Hz AC). It was formerly used by the Milwaukee Road from Harlowton, Montana to Seattle-Tacoma, across the Continental Divide and including extensive branch and loop lines in Montana, and by the Delaware, Lackawanna & Western Railroad (now New Jersey Transit, converted to 25 kV AC) in the United States, and the Kolkata suburban railway (Bardhaman Main Line) in India, before it was converted to 25 kV 50 Hz AC.], '25 kV 50 Hz AC.')])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[31m***********Question and Answers *************\u001b[0m\n",
            "\u001b[34mWho heads an office or division?\u001b[0m\n",
            "\u001b[5m\u001b[7m\u001b[32mRegis McKenna\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[34mWho leaders the sub-divisions of offices or divisions?\u001b[0m\n",
            "\u001b[5m\u001b[7m\u001b[32mRegis McKenna\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[34mWhat helped Avicenna forget the Metaphysics of Aristotle?\u001b[0m\n",
            "\u001b[5m\u001b[7m\u001b[32m<No Answer>\u001b[0m\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[31m***********Question and Answers *************\u001b[0m\n",
            "\u001b[34mBesides using 3kV DC what other power type is used in the former Soviet Union countries?\u001b[0m\n",
            "\u001b[5m\u001b[7m\u001b[32m25 kV 50 Hz AC\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[34mWhat does the railway system of US use DC or AC?\u001b[0m\n",
            "\u001b[5m\u001b[7m\u001b[32m25 kV AC\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[34mWhat helped Avicenna forget the Metaphysics of Aristotle?\u001b[0m\n",
            "\u001b[5m\u001b[7m\u001b[32m25 kV 50 Hz AC.\u001b[0m\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZ4O-vMtqtXE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}