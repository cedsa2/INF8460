{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6-final"
    },
    "colab": {
      "name": "inf8460_tp3_A20_equipe8.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ho-kYf56xD5x"
      },
      "source": [
        "# École Polytechnique de Montréal\n",
        "# Département Génie Informatique et Génie Logiciel\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "KPmpq04SxD5y"
      },
      "source": [
        "## Équipe et contributions \n",
        "Veuillez indiquer la contribution effective de chaque membre de l'équipe en pourcentage et en indiquant les modules ou questions sur lesquelles chaque membre a travaillé"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "0MLQXYV3xD5y"
      },
      "source": [
        "Cedric Sadeu (1869737): 1/3\n",
        "\n",
        "Mamoudou Sacko (1924187): 1/3\n",
        "\n",
        "Oumayma Messoussi (2016797): 1/3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEfTf4frxD5z"
      },
      "source": [
        "# Librairies externes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-24T13:30:14.696418Z",
          "start_time": "2020-09-24T13:30:14.651596Z"
        },
        "id": "HYumfJijxD5z",
        "outputId": "a47d1880-e1d5-4a42-f8a1-ec9b27acc914",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "import io\n",
        "import os\n",
        "import nltk\n",
        "import time\n",
        "import sklearn\n",
        "import zipfile\n",
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import Dict, List, Tuple\n",
        "from collections import Counter, defaultdict\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from scipy.spatial.distance import euclidean, cosine\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import f_classif\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\mamoudou\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     C:\\Users\\mamoudou\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAv5A1p5ssmR"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning) \n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning) "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "Zvjf3dcrxD5_"
      },
      "source": [
        "## Lecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-24T13:06:48.847418Z",
          "start_time": "2020-09-24T13:06:48.818869Z"
        },
        "hidden": true,
        "id": "AQ8mPGILxD5_"
      },
      "source": [
        "def read_data(path: str) -> Tuple[List[int], List[str]]:\n",
        "    data = pd.read_csv(path)\n",
        "    ids = data[\"id\"].tolist()\n",
        "    paragraphs = data[\"paragraph\"].tolist()\n",
        "    return ids, paragraphs\n",
        "\n",
        "def read_questions(path: str) -> Tuple[List[int], List[str], List[int], List[str]]:\n",
        "    data = pd.read_csv(path)\n",
        "    ids = data[\"id\"].tolist()\n",
        "    questions = data[\"question\"].tolist()\n",
        "    paragraph_ids = data[\"paragraph_id\"].tolist()\n",
        "    answers = data[\"answer\"].tolist()\n",
        "    return ids, questions, paragraph_ids, answers\n",
        "\n",
        "def read_questions_vectors(path: str) -> Tuple[List[int], List[str], List[int], List[str]]:\n",
        "    data = pd.read_csv(path)\n",
        "    ids = data[\"id\"].tolist()\n",
        "    questions = data[\"question\"].tolist()\n",
        "    questions_vectors = data[\"question_vector\"].tolist()\n",
        "    return ids, questions, questions_vectors\n",
        "\n",
        "def save_to_csv(path: str, corpus):\n",
        "    df = pd.DataFrame(corpus, columns= list(corpus.keys())).head()\n",
        "    df.to_csv (os.path.join(output_path, path), index = False, header=True)\n"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-24T13:06:51.435025Z",
          "start_time": "2020-09-24T13:06:50.020587Z"
        },
        "hidden": true,
        "id": "iNC--JSdxD6B"
      },
      "source": [
        "data_path = \"data\"\n",
        "output_path = \"output\"\n",
        "\n",
        "train_data = read_data(os.path.join(data_path, \"corpus.csv\"))\n",
        "train_ids = read_questions(os.path.join(data_path, \"train_ids.csv\"))\n",
        "\n",
        "\n",
        "paragraphs = [\" \".join(sentence.split()).lower() for sentence in train_data[1]]\n",
        "questions = [\" \".join(sentence.split()).lower() for sentence in train_ids[1]]\n"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "sZs15T7UxD6G"
      },
      "source": [
        "## Prétraitement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-24T13:06:54.237924Z",
          "start_time": "2020-09-24T13:06:54.204609Z"
        },
        "hidden": true,
        "id": "aWYoZJPJxD6G"
      },
      "source": [
        "class Preprocess(object):\n",
        "    def __init__(self, lemmatize=True):\n",
        "        self.stopwords = set(nltk.corpus.stopwords.words(\"english\"))\n",
        "        self.lemmatize = lemmatize\n",
        "\n",
        "    def preprocess_pipeline(self, data):\n",
        "        clean_tokenized_data = self._clean_doc(data)\n",
        "        if self.lemmatize:\n",
        "            clean_tokenized_data = self._lemmatize(clean_tokenized_data)\n",
        "\n",
        "        return clean_tokenized_data\n",
        "\n",
        "    def _clean_doc(self, data):\n",
        "        tokenizer = nltk.tokenize.RegexpTokenizer(r\"\\w+\")\n",
        "        return [\n",
        "            [\n",
        "                token.lower()\n",
        "                for token in tokenizer.tokenize(review)\n",
        "                if token.lower() not in self.stopwords\n",
        "                and len(token) > 1\n",
        "                and token.isalpha()\n",
        "            ]\n",
        "            for review in data\n",
        "        ]\n",
        "\n",
        "    def _lemmatize(self, data):\n",
        "        lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "        return [[lemmatizer.lemmatize(word) for word in review] for review in data]\n",
        "\n",
        "    def convert_to_reviews(self, tokenized_reviews):\n",
        "        reviews = []\n",
        "        for tokens in tokenized_reviews:\n",
        "            reviews.append(\" \".join(tokens))\n",
        "\n",
        "        return reviews"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-24T13:07:29.745222Z",
          "start_time": "2020-09-24T13:06:55.097985Z"
        },
        "hidden": true,
        "id": "ygebO3D_xD6J",
        "outputId": "6f179fb8-b87a-4ad9-c426-00ff0dcb554e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "pre = Preprocess()\n",
        "\n",
        "paragraphs_tokenized = pre.preprocess_pipeline(paragraphs)\n",
        "questions_tokenized = pre.preprocess_pipeline(questions)\n",
        "\n",
        "paragraphs_text = [\" \".join(sentence) for sentence in paragraphs_tokenized]\n",
        "questions_text = [\" \".join(sentence) for sentence in questions_tokenized]\n",
        "\n"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5e6C42v09qW"
      },
      "source": [
        "def buildVocab(X) -> object:\n",
        "  vectorizer = CountVectorizer(min_df=0, lowercase=False)\n",
        "  vectorizer.fit(X)\n",
        "  return vectorizer.vocabulary_\n",
        "\n",
        "def getTfIdfReprentation(vocab, data, feature = 5) -> object:\n",
        "  vectorizer = TfidfVectorizer(vocabulary=vocab) \n",
        "  data_tfidf = vectorizer.fit_transform(data)\n",
        "  features = vectorizer.get_feature_names()\n",
        "  dense = data_tfidf.todense()\n",
        "  return dense\n",
        "\n",
        "def getTfIdfEmbedded(vocab, data, feature = 5) -> object:\n",
        "  vectorizer = TfidfVectorizer(vocabulary=vocab) \n",
        "  data_tfidf = vectorizer.fit_transform(data)\n",
        "  features = vectorizer.get_feature_names()\n",
        "  dense = data_tfidf.todense()\n",
        "  denselist = dense.tolist()\n",
        "  df = pd.DataFrame(\n",
        "    denselist,columns=features)\n",
        "  return df\n",
        "\n",
        "\n",
        "def get_doc_embedded(X, vocab, embeddings) -> object:\n",
        "  X_embedded = np.zeros((len(X), len(embeddings)), dtype=float)\n",
        "\n",
        "  for i, doc in enumerate(X):\n",
        "    vec = np.zeros((1, len(embeddings)), dtype=float)\n",
        "    tokens = doc.split()new_question_tfidf\n",
        "    cpt = 0\n",
        "    for word in tokens:\n",
        "      if(word in vocab):\n",
        "        cpt += 1\n",
        "        vec += embeddings[word]\n",
        "    vec /= cpt\n",
        "    X_embedded[i] = vec\n",
        "  return X_embedded\n",
        "\n",
        "  def getMedian(corpus):\n",
        "    total_lenght = sorted([len(doc) for doc in corpus])\n",
        "    return total_lenght[int(len(total_lenght) * 2/3)]\n",
        "\n",
        "  \n",
        "def sklearn_svd(df, k):\n",
        "    svd_model = TruncatedSVD(n_components=k)\n",
        "    df_r = svd_model.fit_transform(df)\n",
        "    return  df_r"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "def voisins(word, df, n, distfunc=cosine):\n",
        "    assert distfunc.__name__ == 'cosine' or distfunc.__name__ == 'euclidean', \"distance metric not supported\"\n",
        "    order = True if distfunc.__name__ == 'euclidean' else False\n",
        "\n",
        "    closest = {}\n",
        "    for w in df:\n",
        "        distance = distfunc(word, df[w])\n",
        "        closest[w] = distance\n",
        "\n",
        "    closest = {k: v for k, v in sorted(closest.items(), key=lambda item: item[1], reverse=order)}\n",
        "\n",
        "    return list(closest.keys())[:n], list(closest.values())[:n]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.52915026 0.         0.         0.         0.        ]\n [0.         0.31622777 0.         0.         0.        ]\n [0.         0.         0.5        0.         0.        ]\n [0.         0.         0.         0.40824829 0.        ]\n [0.         0.         0.         0.         0.4472136 ]]\n[[0.         0.         0.         0.40824829 0.        ]]\n(['Did von Neumann rule hidden variable theories?', 'Who leaders the sub-divisions of offices or divisions?', 'Besides using 3kV DC what other power type is used in the former Soviet Union countries?'], [0.0, 1.0, 1.0])\n"
          ]
        }
      ],
      "source": [
        "new_question = [\"Did von Neumann rule hidden \"]\n",
        "new_question_tokenized = pre.preprocess_pipeline(new_question)\n",
        "new_question_text = [\" \".join(sentence) for sentence in new_question_tokenized]\n",
        "\n",
        "embeddings = getTfIdfEmbedded(questions_vocab, questions_text) \n",
        "new_question_tfidf = get_doc_embedded(new_question_text, questions_vocab, embeddings)\n",
        "\n",
        "\n",
        "questions = [\"Who leaders the sub-divisions of offices or divisions?\", \"Besides using 3kV DC what other power type is used in the former Soviet Union countries?\", \"\tHow many other cities had populations larger than 40,000 by 1500?\", \"Did von Neumann rule hidden variable theories?\", \"\tThe OSHA claimed that the Tajik government censored what?\"]\n",
        "questions_tfidf = get_doc_embedded(questions_text, questions_vocab, embeddings)\n",
        "\n",
        "\n",
        "#questions_tfidf_r = sklearn_svd(questions_tfidf, len(embeddings))\n",
        "dic_questions = {}\n",
        "for i, ids in enumerate(questions) :\n",
        "    dic_questions[questions[i]] = questions_tfidf[i]\n",
        "\n",
        "\n",
        "print(questions_tfidf)\n",
        "print(new_question_tfidf)\n",
        "\n",
        "print(voisins(new_question_tfidf, dic_questions, 3, distfunc=cosine))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ]
}