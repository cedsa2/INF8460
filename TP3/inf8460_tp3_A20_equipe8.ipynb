{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "inf8460_tp3_A20_equipe8.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ho-kYf56xD5x"
      },
      "source": [
        "# École Polytechnique de Montréal\n",
        "# Département Génie Informatique et Génie Logiciel\n",
        "\n",
        "# INF8460 – Traitement automatique de la langue naturelle - TP3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ao44MnEJxD5x"
      },
      "source": [
        "# Objectifs d’apprentissage\n",
        " • Utiliser des plongements lexicaux pré-entrainés pour de la classification\n",
        " \n",
        " • Entrainer des plongements lexicaux de type word2vec\n",
        " \n",
        " • Implanter des modèles de classification neuronaux"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "KPmpq04SxD5y"
      },
      "source": [
        "## Équipe et contributions \n",
        "Veuillez indiquer la contribution effective de chaque membre de l'équipe en pourcentage et en indiquant les modules ou questions sur lesquelles chaque membre a travaillé"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "0MLQXYV3xD5y"
      },
      "source": [
        "Cedric Sadeu (1869737): 1/3\n",
        "\n",
        "Mamoudou Sacko (1924187): 1/3\n",
        "\n",
        "Oumayma Messoussi (2016797): 1/3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEfTf4frxD5z"
      },
      "source": [
        "# Librairies externes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-24T13:30:14.696418Z",
          "start_time": "2020-09-24T13:30:14.651596Z"
        },
        "id": "HYumfJijxD5z",
        "outputId": "adc3ea8b-36ff-40ca-e6bf-719c752ccdd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "import io\n",
        "import os\n",
        "import nltk\n",
        "import time\n",
        "import gensim\n",
        "import sklearn\n",
        "import zipfile\n",
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from typing import Dict\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.test.utils import datapath\n",
        "from collections import Counter, defaultdict\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import classification_report, accuracy_score #, confusion_matrix, plot_confusion_matrix\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAv5A1p5ssmR"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning) \n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning) "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o40G7M7NxD52"
      },
      "source": [
        "# Téléchargement et lecture des données"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-24T13:06:47.074618Z",
          "start_time": "2020-09-24T13:06:47.026757Z"
        },
        "id": "-i-7P3oKxD53"
      },
      "source": [
        "DATA_PATH = os.path.join(os.getcwd(), \"aclImdb\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "nQ9UtPuaxD55"
      },
      "source": [
        "## Téléchargement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-22T15:19:34.239196Z",
          "start_time": "2020-09-22T15:16:55.591044Z"
        },
        "hidden": true,
        "id": "6_ovchLOxD56",
        "outputId": "2841bdb3-38df-4014-9874-1221e78f3664",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xzf aclImdb_v1.tar.gz\n",
        "!rm aclImdb_v1.tar.gz\n",
        "!echo Done!"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-21 17:40:32--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
            "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84125825 (80M) [application/x-gzip]\n",
            "Saving to: ‘aclImdb_v1.tar.gz’\n",
            "\n",
            "aclImdb_v1.tar.gz   100%[===================>]  80.23M  3.12MB/s    in 37s     \n",
            "\n",
            "2020-10-21 17:41:09 (2.16 MB/s) - ‘aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
            "\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "hNL33fFzxD59"
      },
      "source": [
        "def download_wikipedia_embeddings() -> None:\n",
        "    if not os.path.exists(os.path.join(os.getcwd(), \"model.txt\")):\n",
        "        res = requests.get(\"http://vectors.nlpl.eu/repository/11/3.zip\")\n",
        "        with zipfile.ZipFile(io.BytesIO(res.content)) as z:\n",
        "            z.extractall(\"./\")\n",
        "        os.remove(os.path.join(os.getcwd(), \"3.zip\"))\n",
        "        os.remove(os.path.join(os.getcwd(), \"meta.json\"))\n",
        "        os.remove(os.path.join(os.getcwd(), \"model.bin\"))\n",
        "        os.remove(os.path.join(os.getcwd(), \"README\"))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "Zvjf3dcrxD5_"
      },
      "source": [
        "## Lecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-24T13:06:48.847418Z",
          "start_time": "2020-09-24T13:06:48.818869Z"
        },
        "hidden": true,
        "id": "AQ8mPGILxD5_"
      },
      "source": [
        "def read_data(path):\n",
        "    traintest = ['train', 'test']\n",
        "    classes = ['pos', 'neg']\n",
        "    corpus = {cls: [] for cls in classes}\n",
        "\n",
        "    # Each data is a list of strings(reviews)\n",
        "    reviews = []\n",
        "    labels = []\n",
        "    for cls in classes:\n",
        "        dir_path = os.path.join(path, cls)\n",
        "        \n",
        "        for filename in os.listdir(dir_path):\n",
        "            file = os.path.join(dir_path, filename)\n",
        "            with open(file, encoding = 'utf-8') as f:\n",
        "                corpus[cls].append(f.read().replace(\"\\n\", \" \"))\n",
        "        \n",
        "    return corpus"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-24T13:06:51.435025Z",
          "start_time": "2020-09-24T13:06:50.020587Z"
        },
        "hidden": true,
        "id": "iNC--JSdxD6B"
      },
      "source": [
        "train_data = read_data(os.path.join(DATA_PATH, 'train'))\n",
        "test_data = read_data(os.path.join(DATA_PATH, 'test'))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENDmfC79xR03",
        "outputId": "f4c075ad-ba3c-46fb-e112-2b4a145e389c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "train_data['pos'][0]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"I did not set very high expectations for this movie, which left me pleasantly surprised. The story is a little strange sometimes but overall I think it has an acceptable credibility. The action scenes are rather nice and the accompanying music is used to induce a a bit of patriotic feelings common to US movies. This may not be the best movie ever but it's uncommon for Sweden and I hope to see more similar ones in the future.\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-24T13:06:51.480512Z",
          "start_time": "2020-09-24T13:06:51.437150Z"
        },
        "hidden": true,
        "id": "bgGbDeXDxD6E"
      },
      "source": [
        "def create_wikipedia_embeddings(word_indices: Dict[str, int], vocab_len: int) -> np.ndarray:\n",
        "    with open(\"./model.txt\", \"r\", encoding=\"UTF-8\") as f:\n",
        "        shape_string = f.readline()\n",
        "        lines = f.readlines() \n",
        "        \n",
        "    embedding = np.zeros((vocab_len, 300), dtype=float)\n",
        "    for line in lines:\n",
        "        splitted_line = line.split(\" \")\n",
        "        word = splitted_line[0].split(\"_\")[0]\n",
        "        if word in word_indices and word_indices[word] < vocab_len:\n",
        "            embedding_line = splitted_line[1:]\n",
        "            embedding[word_indices[word]] = list(map(float, embedding_line))\n",
        "        \n",
        "    return embedding"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "sZs15T7UxD6G"
      },
      "source": [
        "## Prétraitement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-24T13:06:54.237924Z",
          "start_time": "2020-09-24T13:06:54.204609Z"
        },
        "hidden": true,
        "id": "aWYoZJPJxD6G"
      },
      "source": [
        "class Preprocess(object):\n",
        "    def __init__(self, lemmatize=True):\n",
        "        self.stopwords = set(nltk.corpus.stopwords.words(\"english\"))\n",
        "        self.lemmatize = lemmatize\n",
        "\n",
        "    def preprocess_pipeline(self, data):\n",
        "        clean_tokenized_data = self._clean_doc(data)\n",
        "        if self.lemmatize:\n",
        "            clean_tokenized_data = self._lemmatize(clean_tokenized_data)\n",
        "\n",
        "        return clean_tokenized_data\n",
        "\n",
        "    def _clean_doc(self, data):\n",
        "        tokenizer = nltk.tokenize.RegexpTokenizer(r\"\\w+\")\n",
        "        return [\n",
        "            [\n",
        "                token.lower()\n",
        "                for token in tokenizer.tokenize(review)\n",
        "                if token.lower() not in self.stopwords\n",
        "                and len(token) > 1\n",
        "                and token.isalpha()\n",
        "                and token != \"br]\"\n",
        "            ]\n",
        "            for review in data\n",
        "        ]\n",
        "\n",
        "    def _lemmatize(self, data):\n",
        "        lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "        return [[lemmatizer.lemmatize(word) for word in review] for review in data]\n",
        "\n",
        "    def convert_to_reviews(self, tokenized_reviews):\n",
        "        reviews = []\n",
        "        for tokens in tokenized_reviews:\n",
        "            reviews.append(\" \".join(tokens))\n",
        "\n",
        "        return reviews"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-24T13:07:29.745222Z",
          "start_time": "2020-09-24T13:06:55.097985Z"
        },
        "hidden": true,
        "id": "ygebO3D_xD6J",
        "outputId": "5e7d88da-5974-4220-d482-65b3b60d4e82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "pre = Preprocess()\n",
        "\n",
        "train_pos = pre.preprocess_pipeline(train_data[\"pos\"])\n",
        "train_neg = pre.preprocess_pipeline(train_data[\"neg\"])\n",
        "test_pos = pre.preprocess_pipeline(test_data[\"pos\"])\n",
        "test_neg = pre.preprocess_pipeline(test_data[\"neg\"])\n",
        "\n",
        "y_train = [1] * len(train_pos) + [0] * len(train_neg)\n",
        "y_test = [1] * len(test_pos) + [0] * len(test_neg)\n",
        "X_train = [\" \".join(sentence) for sentence in train_pos + train_neg]\n",
        "X_test = [\" \".join(sentence) for sentence in test_pos + test_neg]\n",
        "\n",
        "print(\"{} training sentences: {} pos and {} neg\".format(len(X_train), len(train_pos), len(train_neg)))\n",
        "print(\"{} test sentences: {} pos and {} neg\".format(len(X_test), len(test_pos), len(test_neg)))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25000 training sentences: 12500 pos and 12500 neg\n",
            "25000 test sentences: 12500 pos and 12500 neg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIzXS_LyxD6L"
      },
      "source": [
        "# 1. Entrainement de plongements lexicaux"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hc6b5mstxD6M"
      },
      "source": [
        "Vous devez réaliser les étapes suivantes:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyxls-lYxD6M"
      },
      "source": [
        "## a) Utiliser Gensim pour entrainer un modèle word2vec sur le corpus. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Xy8ZXzAxD6M"
      },
      "source": [
        "X_train_tokenized = [sentence for sentence in train_pos + train_neg]\n",
        "\n",
        "model = Word2Vec(min_count=1, window=5, size=256, alpha=1e-2, min_alpha=1e-4, \n",
        "                 workers=(os.cpu_count()*2 - 1), sample=0.01, negative=5)\n",
        "\n",
        "model.build_vocab(X_train_tokenized)\n",
        "\n",
        "start = time.time()\n",
        "model.train(X_train_tokenized, total_examples=model.corpus_count, epochs=10)\n",
        "end = time.time() - start"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwIr4uNUxD6O"
      },
      "source": [
        "## b) Décrire les paramètres du ou des modèles entraînés, leur taille sur disque, le nombre de mots encodés, le temps d'entraînement, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lm-YxjBZd2Su",
        "outputId": "5cd8290d-06de-42f2-90ed-42b0278a1c24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "source": [
        "print(\"- Temps d'entrainement (en secondes): %f\\n\" % end)\n",
        "print(\"- Taille du modele sur disque (en octets): \", model.estimate_memory())\n",
        "\n",
        "word_vectors = model.wv\n",
        "print(\"\\n- Nombre de mots encodés (= taille du vocab): %d\\n\" % len(word_vectors.vectors))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "- Temps d'entrainement (en secondes): 71.511453\n",
            "\n",
            "- Taille du modele sur disque (en octets):  {'vocab': 32849500, 'vectors': 67275776, 'syn1neg': 67275776, 'total': 167401052}\n",
            "\n",
            "- Nombre de mots encodés (= taille du vocab): 65699\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OPIbkr8xD6O"
      },
      "source": [
        "**Les parametres du modele word2vec**\n",
        "\n",
        "*   ***size*** = la taille/nombre de dimensions des vecteurs de plongements générés par le modele. (idealement entre quelques dizaines a quelques centaines). Pour notre modele, on a choisit une valeur multiple de 2 pour une meuilleure gestion de memoire. De plus, les vecteurs de plongements finaux seront de taille (N, 256) avec N la taille du vocabulaire. Ainsi, on a jugé que 256 est un bon compromis.\n",
        "\n",
        "*   ***min_count*** = la fréquence minimale des mots a considerer. Le modele ignore tous les mots du corpus dont la fréquence est inférieure a *min_count*. On a fixé cette valeur a 1 pour pouvoir construire le vocabulaire le plus large possible qui contient tous les types du corpus.\n",
        "\n",
        "*   ***window*** = la taille de la fenetre a considerer autour du mot en question (entre le mot cible et ces voisins). (en generale entre 2 et 10). on a utiliser la valeur 5 comme juste milieu de l'intervalle recommendé.\n",
        "\n",
        "*   ***sample*** =  le seuil de sous-echantillonnage aléatoire des mots les plus fréquents. (idealement entre 0, 1e-5). \n",
        "\n",
        "*   ***alpha*** = le taux d'apprentissage. Ce parametre doit etre assez petit pour pouvoir s'approcher le plus de l'optimum local, mais assez grand pour eviter le surapprentissage. Pour cela, on l'a fixé a 0.01.\n",
        "\n",
        "*   ***min_alpha*** = la valeur a laquelle le taux d'apprentissage *alpha* va diminuer lineairement lors de l'entrainement. Une bonne estimation: alpha - (min_alpha * epochs) ~ 0.00. Dans notre cas, nos valeurs choisies respectent bien cette equation: 0.01 - 0.0001 * 10 = 0.009.\n",
        "\n",
        "*   ***negative*** = si positive, la valeur indique le nombre de mots \"bruit\" a introduire. (generalement entre 5 et 20). Ce parametre permet, entre autre, d'eviter le surapprentissage. Apres plusieurs tests, on a gardé la valeur 5.\n",
        "\n",
        "*   ***workers*** = nombre de threads a utiliser pour l'entrainement. Puisqu'on a utilisé Google Colab, les ressources alloués par session varient, donc pour s'assurer qu'on utilise le maximum de threads disponibles, on recupere ce nombre a travers *os.cpu_count()*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suvwhRwfxD6P"
      },
      "source": [
        "## c) Décrire le cas échéant et de manière précise tout problème que vous avez eu à obtenir votre modèle et les façons de résoudre ces problèmes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfSNxCCyxD6P"
      },
      "source": [
        "*   le parametre *sample*: Ce parametre est par excellence le plus sensible. On a du experimenté avec plusieurs valeurs pour etudier son impact et aboutir a de bons resultats."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_t-VKtwVxD6Q"
      },
      "source": [
        "## d) Retrouvez les 5 mots voisins des mots suivants : excellent, terrible"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emmkYtRcf43a",
        "outputId": "ce158d01-1f8f-47ec-fd85-e47d6d80b42f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "print(word_vectors.most_similar('excellent')[:5])\n",
        "print(word_vectors.most_similar('terrible')[:5])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('outstanding', 0.9267053604125977), ('fine', 0.9202706217765808), ('superb', 0.9140844345092773), ('terrific', 0.9097084999084473), ('fantastic', 0.9078267216682434)]\n",
            "[('horrible', 0.9704045057296753), ('awful', 0.9541709423065186), ('suck', 0.8495052456855774), ('atrocious', 0.8381000757217407), ('baaaaaad', 0.8236089944839478)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApQGSboEEeps",
        "outputId": "9145dda2-cdf1-4bf1-f11b-3ff09f11cbc6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "source": [
        "# a couple more test cases\n",
        "\n",
        "print(word_vectors.most_similar('fun')[:5])\n",
        "print(word_vectors.most_similar('film')[:5])\n",
        "print(word_vectors.most_similar('acting')[:5])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('laugh', 0.8005730509757996), ('enjoy', 0.7648371458053589), ('enjoyable', 0.752558708190918), ('entertaining', 0.7490490674972534), ('scary', 0.7229304313659668)]\n",
            "[('movie', 0.7168675065040588), ('cinema', 0.7107754349708557), ('documentary', 0.7094526290893555), ('picture', 0.6874585151672363), ('flick', 0.6587791442871094)]\n",
            "[('writing', 0.7815835475921631), ('directing', 0.7600515484809875), ('casting', 0.7400418519973755), ('direction', 0.7393010854721069), ('script', 0.7318971157073975)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JbsgXb5mm5E"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3pEfCMFmmQj"
      },
      "source": [
        "del model # to free up space"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66Mys_mcxD6S"
      },
      "source": [
        "# 2. Classification avec des plongements lexicaux"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XNuMEjTxD6T"
      },
      "source": [
        "On vous demande d’effectuer de la classification avec les plongements lexicaux obtenus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E42QN7uExD6T"
      },
      "source": [
        "## a) En reprenant le code développé dans le TP1 avec Scikitlearn, on vous demande cette fois de tester un modèle Naïve Bayes et de régression logistique avec des n-grammes (n=1,2,3 ensemble). Essayez de voir si une réduction de dimension améliore la classification. Ne fournissez que votre meilleur modèle. Evaluez vos algorithmes selon les métriques d’accuracy générale et de F1 par classe sur l’ensemble de test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0SZcn5WaLkT",
        "outputId": "a7e80a01-2621-4412-8633-55a29865a8a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "def build_voc(X_train, vocab_size):\n",
        "    freqs = Counter(word for doc in X_train for word in doc.split(\" \"))\n",
        "    vocab = {word: freq for word, freq in freqs.most_common(vocab_size)}\n",
        "    return {k: v for k, v in sorted(vocab.items(), key=lambda item: item[1], reverse=True)}\n",
        "\n",
        "print(build_voc(X_train, 500))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'br': 101871, 'movie': 51715, 'film': 47048, 'one': 27748, 'like': 20747, 'time': 15966, 'good': 15204, 'character': 14183, 'story': 13171, 'even': 12655, 'get': 12515, 'would': 12436, 'make': 12229, 'see': 12016, 'really': 11738, 'well': 10788, 'scene': 10596, 'much': 9764, 'people': 9387, 'bad': 9309, 'also': 9159, 'great': 9090, 'first': 9065, 'way': 8830, 'show': 8603, 'made': 8364, 'thing': 8213, 'life': 8038, 'could': 7922, 'think': 7734, 'go': 7602, 'watch': 7096, 'know': 7068, 'two': 6908, 'actor': 6877, 'year': 6874, 'plot': 6874, 'love': 6856, 'seen': 6681, 'many': 6675, 'end': 6635, 'look': 6561, 'say': 6506, 'acting': 6495, 'never': 6485, 'little': 6438, 'best': 6420, 'man': 6016, 'ever': 5992, 'better': 5742, 'take': 5702, 'come': 5673, 'still': 5656, 'work': 5653, 'part': 5222, 'director': 5124, 'find': 5080, 'something': 5076, 'back': 5045, 'want': 4990, 'give': 4953, 'lot': 4779, 'real': 4737, 'performance': 4720, 'woman': 4625, 'watching': 4608, 'old': 4580, 'though': 4566, 'play': 4452, 'guy': 4340, 'another': 4330, 'new': 4311, 'role': 4302, 'nothing': 4293, 'funny': 4290, 'actually': 4240, 'going': 4147, 'girl': 4065, 'point': 4040, 'day': 4013, 'world': 3978, 'every': 3978, 'cast': 3895, 'u': 3793, 'star': 3785, 'feel': 3761, 'fact': 3747, 'quite': 3739, 'minute': 3737, 'horror': 3715, 'comedy': 3686, 'action': 3665, 'pretty': 3664, 'young': 3663, 'thought': 3656, 'seems': 3619, 'around': 3617, 'got': 3587, 'however': 3537, 'right': 3497, 'big': 3477, 'enough': 3453, 'long': 3452, 'family': 3442, 'original': 3429, 'line': 3421, 'series': 3417, 'may': 3393, 'bit': 3349, 'fan': 3331, 'set': 3308, 'without': 3267, 'must': 3251, 'always': 3240, 'friend': 3233, 'script': 3176, 'saw': 3172, 'almost': 3140, 'interesting': 3129, 'least': 3113, 'try': 3110, 'done': 3096, 'whole': 3079, 'music': 3062, 'kid': 3039, 'shot': 2999, 'last': 2994, 'making': 2979, 'far': 2979, 'kind': 2974, 'anything': 2948, 'book': 2926, 'reason': 2920, 'start': 2919, 'might': 2919, 'since': 2907, 'probably': 2842, 'effect': 2837, 'child': 2837, 'place': 2822, 'tv': 2786, 'moment': 2778, 'away': 2775, 'put': 2761, 'yet': 2754, 'rather': 2734, 'worst': 2731, 'fun': 2695, 'sure': 2686, 'let': 2678, 'audience': 2674, 'hard': 2667, 'need': 2648, 'idea': 2638, 'anyone': 2632, 'turn': 2610, 'tell': 2599, 'episode': 2598, 'american': 2593, 'played': 2588, 'found': 2577, 'screen': 2566, 'although': 2538, 'especially': 2536, 'course': 2517, 'believe': 2505, 'looking': 2483, 'trying': 2473, 'ending': 2463, 'job': 2463, 'mean': 2444, 'dvd': 2410, 'version': 2410, 'different': 2385, 'war': 2382, 'money': 2369, 'someone': 2363, 'maybe': 2342, 'problem': 2338, 'true': 2333, 'sense': 2326, 'everything': 2324, 'second': 2311, 'three': 2298, 'night': 2292, 'house': 2290, 'worth': 2280, 'main': 2265, 'help': 2256, 'wife': 2245, 'keep': 2243, 'together': 2243, 'watched': 2236, 'john': 2223, 'everyone': 2223, 'le': 2216, 'later': 2200, 'father': 2200, 'said': 2196, 'instead': 2191, 'mind': 2180, 'beautiful': 2178, 'boy': 2178, 'seem': 2175, 'high': 2174, 'hour': 2166, 'special': 2135, 'left': 2125, 'death': 2114, 'black': 2111, 'half': 2101, 'seeing': 2099, 'excellent': 2071, 'eye': 2068, 'classic': 2062, 'lead': 2060, 'read': 2057, 'viewer': 2048, 'laugh': 2033, 'short': 2015, 'nice': 2012, 'else': 2001, 'name': 1995, 'face': 1990, 'production': 1973, 'simply': 1966, 'sound': 1965, 'piece': 1961, 'home': 1940, 'picture': 1939, 'song': 1918, 'human': 1915, 'men': 1914, 'top': 1912, 'hollywood': 1911, 'poor': 1897, 'budget': 1895, 'hand': 1889, 'completely': 1889, 'camera': 1889, 'dead': 1881, 'used': 1879, 'video': 1867, 'either': 1866, 'given': 1851, 'rest': 1836, 'wrong': 1834, 'head': 1832, 'low': 1817, 'writer': 1815, 'couple': 1812, 'boring': 1812, 'enjoy': 1812, 'word': 1811, 'use': 1804, 'full': 1780, 'along': 1776, 'kill': 1764, 'truly': 1743, 'run': 1732, 'awful': 1725, 'school': 1725, 'hope': 1720, 'next': 1717, 'style': 1715, 'sex': 1714, 'mr': 1708, 'remember': 1703, 'killer': 1702, 'stupid': 1701, 'case': 1695, 'person': 1686, 'perhaps': 1684, 'came': 1673, 'title': 1670, 'brother': 1670, 'recommend': 1668, 'sort': 1662, 'wonderful': 1658, 'dialogue': 1652, 'small': 1647, 'act': 1645, 'understand': 1644, 'terrible': 1638, 'playing': 1633, 'attempt': 1633, 'getting': 1627, 'game': 1623, 'fall': 1621, 'written': 1616, 'flick': 1615, 'care': 1613, 'early': 1605, 'sequence': 1603, 'art': 1601, 'often': 1601, 'joke': 1601, 'perfect': 1600, 'others': 1595, 'mother': 1593, 'actress': 1588, 'definitely': 1580, 'review': 1569, 'cinema': 1556, 'example': 1555, 'drama': 1553, 'lost': 1553, 'live': 1552, 'white': 1546, 'become': 1542, 'feeling': 1540, 'finally': 1536, 'yes': 1535, 'felt': 1528, 'liked': 1516, 'supposed': 1516, 'waste': 1510, 'quality': 1509, 'son': 1506, 'car': 1504, 'absolutely': 1485, 'begin': 1474, 'worse': 1469, 'direction': 1467, 'evil': 1463, 'went': 1463, 'heart': 1463, 'certainly': 1462, 'entire': 1461, 'oh': 1452, 'entertaining': 1443, 'overall': 1441, 'side': 1434, 'feature': 1434, 'called': 1433, 'fight': 1433, 'comment': 1432, 'based': 1431, 'murder': 1430, 'loved': 1428, 'beginning': 1426, 'lack': 1423, 'several': 1420, 'favorite': 1419, 'number': 1409, 'hero': 1388, 'already': 1381, 'dark': 1381, 'becomes': 1380, 'voice': 1373, 'type': 1372, 'michael': 1370, 'genre': 1369, 'age': 1369, 'despite': 1365, 'seemed': 1363, 'throughout': 1361, 'hit': 1360, 'unfortunately': 1353, 'wanted': 1352, 'matter': 1351, 'change': 1347, 'meet': 1345, 'history': 1339, 'final': 1335, 'writing': 1332, 'relationship': 1327, 'fine': 1326, 'close': 1323, 'amazing': 1321, 'town': 1318, 'humor': 1317, 'guess': 1316, 'daughter': 1312, 'cut': 1309, 'totally': 1307, 'city': 1285, 'today': 1284, 'event': 1284, 'behind': 1280, 'past': 1277, 'god': 1273, 'power': 1265, 'experience': 1259, 'able': 1259, 'zombie': 1258, 'move': 1257, 'enjoyed': 1246, 'stop': 1242, 'theme': 1239, 'soon': 1223, 'sometimes': 1218, 'gave': 1217, 'stand': 1213, 'etc': 1212, 'late': 1211, 'directed': 1204, 'credit': 1204, 'self': 1202, 'chance': 1202, 'horrible': 1201, 'talent': 1201, 'level': 1198, 'brilliant': 1197, 'body': 1190, 'blood': 1187, 'call': 1183, 'stuff': 1181, 'thinking': 1179, 'expect': 1178, 'element': 1175, 'musical': 1167, 'save': 1166, 'question': 1164, 'wonder': 1163, 'obviously': 1162, 'decent': 1157, 'light': 1157, 'situation': 1156, 'highly': 1148, 'add': 1147, 'view': 1146, 'lady': 1144, 'group': 1142, 'slow': 1132, 'except': 1130, 'novel': 1126, 'score': 1121, 'anyway': 1118, 'interest': 1115, 'career': 1114, 'heard': 1111, 'wish': 1111, 'killed': 1111, 'leave': 1107, 'husband': 1106, 'took': 1100, 'dream': 1099, 'strong': 1098, 'police': 1098, 'cannot': 1097, 'rating': 1096, 'robert': 1093, 'violence': 1093, 'country': 1086, 'happens': 1081, 'known': 1080, 'particularly': 1079, 'involved': 1077, 'happened': 1076, 'documentary': 1074, 'james': 1069, 'extremely': 1068, 'obvious': 1066, 'coming': 1065, 'living': 1063, 'told': 1062, 'talk': 1062, 'alone': 1061, 'order': 1060, 'theater': 1057, 'including': 1052, 'crap': 1050, 'reality': 1050, 'opinion': 1048, 'thriller': 1048, 'please': 1047, 'effort': 1046, 'happen': 1044, 'room': 1044, 'twist': 1043, 'sequel': 1042, 'gore': 1042, 'king': 1036, 'complete': 1035, 'comic': 1034, 'ago': 1033, 'hell': 1032, 'none': 1032, 'david': 1026, 'sister': 1025, 'female': 1024, 'simple': 1023, 'ok': 1019, 'rock': 1014, 'looked': 1010, 'oscar': 1008, 'season': 1007, 'seriously': 1002, 'miss': 1001, 'possible': 999, 'class': 999, 'annoying': 998, 'sad': 996, 'spoiler': 996, 'exactly': 995, 'shown': 994, 'running': 992, 'value': 992, 'serious': 989}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mX1Rd3aBu5m8"
      },
      "source": [
        "**Sans reduction de dimensions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMRuM59wXLXQ",
        "outputId": "31880751-e1eb-42c7-aa5b-97ea657fd67a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "vectorizer = TfidfVectorizer(ngram_range=(1,3))\n",
        "\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "print('X_train_tfidf:', X_train_tfidf.shape)\n",
        "print('X_test_tfidf:', X_test_tfidf.shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train_tfidf: (25000, 4472874)\n",
            "X_test_tfidf: (25000, 4472874)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RtqgwbGnpcw"
      },
      "source": [
        "**Naive Bayes (sans reduction de dimensions)**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkaeiFiPnlmE",
        "outputId": "482a90c3-5990-47ef-8ebc-6317569b170a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        }
      },
      "source": [
        "model = MultinomialNB(alpha=0.6)\n",
        "\n",
        "model.fit(X_train_tfidf, y_train)\n",
        "y_pred = model.predict(X_test_tfidf)\n",
        "\n",
        "print(\"Classification report:\\n\")\n",
        "print(classification_report(y_test, y_pred, target_names=[\"N\", \"P\"]))\n",
        "print(\"\\nAccuracy generale: %f \\n\" % accuracy_score(y_test, y_pred))\n",
        "\n",
        "# plot_confusion_matrix(model, X_test_tfidf, y_test, display_labels=[\"N\", \"P\"], cmap=plt.cm.Blues)\n",
        "# plt.show()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classification report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           N       0.85      0.89      0.87     12500\n",
            "           P       0.88      0.84      0.86     12500\n",
            "\n",
            "    accuracy                           0.86     25000\n",
            "   macro avg       0.86      0.86      0.86     25000\n",
            "weighted avg       0.86      0.86      0.86     25000\n",
            "\n",
            "\n",
            "Accuracy generale: 0.863480 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mW1Xa51trgxR"
      },
      "source": [
        "**Regression logistique (sans reduction de dimensions)**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDBnyzU5rlev",
        "outputId": "40406740-f535-401e-9c2f-505021df72bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        }
      },
      "source": [
        "model = LogisticRegression(C=2.0)\n",
        "\n",
        "model.fit(X_train_tfidf, y_train)\n",
        "y_pred = model.predict(X_test_tfidf)\n",
        "\n",
        "print(\"Classification report:\\n\")\n",
        "print(classification_report(y_test, y_pred, target_names=[\"N\", \"P\"]))\n",
        "print(\"\\nAccuracy generale: %f \\n\" % accuracy_score(y_test, y_pred))\n",
        "\n",
        "# plot_confusion_matrix(model, X_test_tfidf, y_test, display_labels=[\"N\", \"P\"], cmap=plt.cm.Blues)\n",
        "# plt.show()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classification report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           N       0.88      0.87      0.88     12500\n",
            "           P       0.88      0.88      0.88     12500\n",
            "\n",
            "    accuracy                           0.88     25000\n",
            "   macro avg       0.88      0.88      0.88     25000\n",
            "weighted avg       0.88      0.88      0.88     25000\n",
            "\n",
            "\n",
            "Accuracy generale: 0.878480 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TkLNyGWu1KJ"
      },
      "source": [
        "**Avec reduction de dimensions**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXeOe9zPqU_6"
      },
      "source": [
        "def lsa(df, k):\n",
        "    # print(df)\n",
        "    U, s, V = np.linalg.svd(df, full_matrices=False)\n",
        "    S = np.diag(s[:k])\n",
        "    Ck = np.dot(U[:, :k], S)\n",
        "    df_r =  pd.DataFrame(Ck).T\n",
        "    df_r.columns = df.keys()\n",
        "    return  df_r"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eig6ERnBqXAO",
        "outputId": "592fae44-5a27-40bc-da76-c43e6a9be7e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "X_train_lsa = lsa(np.ndarray(X_train_tfidf), 100)\n",
        "# X_test_lsa = lsa(X_test_tfidf, 100)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-6d67cb3843d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train_lsa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlsa\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_tfidf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# X_test_lsa = lsa(X_test_tfidf, 100)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: expected sequence object with len >= 0 or a single integer"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAxNYPQxvDlo"
      },
      "source": [
        "svd = TruncatedSVD(n_components=300)\n",
        "# df_r =  pd.DataFrame(df_r).T\n",
        "# df_r.columns = df.keys()\n",
        "\n",
        "X_train_lsa = svd.fit_transform(X_train_tfidf)\n",
        "# X_test_lsa = svd.fit(X_test_tfidf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ho8DUJhavKQd"
      },
      "source": [
        "**Naive Bayes (avec reduction de dimensions)**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qV0HMQaPvLvX"
      },
      "source": [
        "model = MultinomialNB(alpha=0.6)\n",
        "\n",
        "model.fit(X_train_lsa, y_train)\n",
        "y_pred = model.predict(X_test_lsa)\n",
        "\n",
        "print(\"Classification report:\\n\")\n",
        "print(classification_report(y_test, y_pred, target_names=[\"N\", \"P\"]))\n",
        "print(\"\\nAccuracy generale: %f \\n\" % accuracy_score(y_test, y_pred))\n",
        "\n",
        "# plot_confusion_matrix(model, X_test_tfidf, y_test, display_labels=[\"N\", \"P\"], cmap=plt.cm.Blues)\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tj0m_t7wvMEe"
      },
      "source": [
        "**Regression logistique (avec reduction de dimensions)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKZ3dkvBvMd8"
      },
      "source": [
        "model = LogisticRegression(C=2.0)\n",
        "\n",
        "model.fit(X_train_lsa, y_train)\n",
        "y_pred = model.predict(X_test_lsa)\n",
        "\n",
        "print(\"Classification report:\\n\")\n",
        "print(classification_report(y_test, y_pred, target_names=[\"N\", \"P\"]))\n",
        "print(\"\\nAccuracy generale: %f \\n\" % accuracy_score(y_test, y_pred))\n",
        "\n",
        "# plot_confusion_matrix(model, X_test_tfidf, y_test, display_labels=[\"N\", \"P\"], cmap=plt.cm.Blues)\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMXMEiaZxD6Y"
      },
      "source": [
        "## b) En utilisant Tensorflow (ou Pytorch), on vous demande de développer un classificateur perceptron multicouches et un bi-LSTM avec les vecteurs d’un modèle word2vec pré-entrainé sur Wikipédia en Anglais (enwiki_upos_skipgram_300_3_2019) disponible à http://vectors.nlpl.eu/repository/11/3.zip. \n",
        "\n",
        "On s’attend à ce que vous effectuiez une moyenne des vecteurs de mots de chaque document pour obtenir un plongement du document.  \n",
        "\n",
        "Evaluez vos algorithmes selon les métriques d’accuracy générale et de F1 par classe sur l’ensemble de test. Pour chacun des modèles, indiquez ses performances et ses spécifications (nombre d’époques, régularisation, optimiseur, nombre de couches, etc.). N’hésitez pas à expérimenter avec différents paramètres. Vous ne devez reporter que votre meilleure expérimentation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-01zN44jxD6b"
      },
      "source": [
        "## c) Ré-entrainez les modèles en b) avec vos propres vecteurs. Comparez maintenant la performance obtenue en en b) avec celles que vous obtenez en utilisant vos propres vecteurs de mots entrainés sur le corpus. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AI1sNwfPxD6b"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZ9x_RbYxD6d"
      },
      "source": [
        "## d) Générez une table ou un graphique qui regroupe les performances des modèles, leurs spécifications, la durée d’entraînement et commentez ces résultats. Quelle est l’influence des word embeddings sur les performances?  Quel est votre meilleur modèle ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6Mog5T1xD6d"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}