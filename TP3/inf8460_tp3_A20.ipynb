{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "inf8460_tp3_A20.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ho-kYf56xD5x"
      },
      "source": [
        "# École Polytechnique de Montréal\n",
        "# Département Génie Informatique et Génie Logiciel\n",
        "\n",
        "# INF8460 – Traitement automatique de la langue naturelle - TP3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ao44MnEJxD5x"
      },
      "source": [
        "# Objectifs d’apprentissage\n",
        " • Utiliser des plongements lexicaux pré-entrainés pour de la classification\n",
        " \n",
        " • Entrainer des plongements lexicaux de type word2vec\n",
        " \n",
        " • Implanter des modèles de classification neuronaux"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "KPmpq04SxD5y"
      },
      "source": [
        "## Équipe et contributions \n",
        "Veuillez indiquer la contribution effective de chaque membre de l'équipe en pourcentage et en indiquant les modules ou questions sur lesquelles chaque membre a travaillé"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "0MLQXYV3xD5y"
      },
      "source": [
        "Cedric Sadeu (1869737): 1/3\n",
        "\n",
        "Mamoudou Sacko (1924187): 1/3\n",
        "\n",
        "Oumayma Messoussi (2016797): 1/3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEfTf4frxD5z"
      },
      "source": [
        "# Librairies externes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-24T13:30:14.696418Z",
          "start_time": "2020-09-24T13:30:14.651596Z"
        },
        "id": "HYumfJijxD5z",
        "outputId": "b233e8ce-0071-4e92-80b8-d881c6c93198",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "import io\n",
        "import nltk\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import requests\n",
        "import sklearn\n",
        "import sklearn.naive_bayes\n",
        "import tensorflow as tf\n",
        "import time\n",
        "from typing import Dict\n",
        "import zipfile\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o40G7M7NxD52"
      },
      "source": [
        "# Téléchargement et lecture des données"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-24T13:06:47.074618Z",
          "start_time": "2020-09-24T13:06:47.026757Z"
        },
        "id": "-i-7P3oKxD53"
      },
      "source": [
        "DATA_PATH = os.path.join(os.getcwd(), \"aclImdb\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "nQ9UtPuaxD55"
      },
      "source": [
        "## Téléchargement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-22T15:19:34.239196Z",
          "start_time": "2020-09-22T15:16:55.591044Z"
        },
        "hidden": true,
        "id": "6_ovchLOxD56",
        "outputId": "29b8dbef-598f-4668-bbc2-dd9caf46260a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xzf aclImdb_v1.tar.gz\n",
        "!rm aclImdb_v1.tar.gz\n",
        "!echo Done!"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-09 14:05:25--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
            "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84125825 (80M) [application/x-gzip]\n",
            "Saving to: ‘aclImdb_v1.tar.gz’\n",
            "\n",
            "aclImdb_v1.tar.gz   100%[===================>]  80.23M  17.6MB/s    in 7.7s    \n",
            "\n",
            "2020-10-09 14:05:33 (10.5 MB/s) - ‘aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
            "\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "hNL33fFzxD59"
      },
      "source": [
        "def download_wikipedia_embeddings() -> None:\n",
        "    if not os.path.exists(os.path.join(os.getcwd(), \"model.txt\")):\n",
        "        res = requests.get(\"http://vectors.nlpl.eu/repository/11/3.zip\")\n",
        "        with zipfile.ZipFile(io.BytesIO(res.content)) as z:\n",
        "            z.extractall(\"./\")\n",
        "        os.remove(os.path.join(os.getcwd(), \"3.zip\"))\n",
        "        os.remove(os.path.join(os.getcwd(), \"meta.json\"))\n",
        "        os.remove(os.path.join(os.getcwd(), \"model.bin\"))\n",
        "        os.remove(os.path.join(os.getcwd(), \"README\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "Zvjf3dcrxD5_"
      },
      "source": [
        "## Lecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-24T13:06:48.847418Z",
          "start_time": "2020-09-24T13:06:48.818869Z"
        },
        "hidden": true,
        "id": "AQ8mPGILxD5_"
      },
      "source": [
        "def read_data(path):\n",
        "    traintest = ['train', 'test']\n",
        "    classes = ['pos', 'neg']\n",
        "    corpus = {cls: [] for cls in classes}\n",
        "\n",
        "    # Each data is a list of strings(reviews)\n",
        "    reviews = []\n",
        "    labels = []\n",
        "    for cls in classes:\n",
        "        dir_path = os.path.join(path, cls)\n",
        "        \n",
        "        for filename in os.listdir(dir_path):\n",
        "            file = os.path.join(dir_path, filename)\n",
        "            with open(file, encoding = 'utf-8') as f:\n",
        "                corpus[cls].append(f.read().replace(\"\\n\", \" \"))\n",
        "        \n",
        "    return corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-24T13:06:51.435025Z",
          "start_time": "2020-09-24T13:06:50.020587Z"
        },
        "hidden": true,
        "id": "iNC--JSdxD6B"
      },
      "source": [
        "train_data = read_data(os.path.join(DATA_PATH, 'train'))\n",
        "test_data = read_data(os.path.join(DATA_PATH, 'test'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENDmfC79xR03",
        "outputId": "a0911954-92a4-40e0-8aa8-72a555881880",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "train_data['pos'][0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'This is an interesting true story of Archie Grey Owl, Who dreamed of being an Indiain when he was a child until the age of 17 he was born in England then moved to Canada where he was adotped by Indiains and he writes collums in magazines and he wrote a book that caugt the attention of millions the book was of his life. But at the end he told his wife that he was not a real Indiain and she was fine with it and he died at the age of 43 two years after he went back into the wildness.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-24T13:06:51.480512Z",
          "start_time": "2020-09-24T13:06:51.437150Z"
        },
        "hidden": true,
        "id": "bgGbDeXDxD6E"
      },
      "source": [
        "def create_wikipedia_embeddings(word_indices: Dict[str, int], vocab_len: int) -> np.ndarray:\n",
        "    with open(\"./model.txt\", \"r\", encoding=\"UTF-8\") as f:\n",
        "        shape_string = f.readline()\n",
        "        lines = f.readlines() \n",
        "        \n",
        "    embedding = np.zeros((vocab_len, 300), dtype=float)\n",
        "    for line in lines:\n",
        "        splitted_line = line.split(\" \")\n",
        "        word = splitted_line[0].split(\"_\")[0]\n",
        "        if word in word_indices and word_indices[word] < vocab_len:\n",
        "            embedding_line = splitted_line[1:]\n",
        "            embedding[word_indices[word]] = list(map(float, embedding_line))\n",
        "        \n",
        "    return embedding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "sZs15T7UxD6G"
      },
      "source": [
        "## Prétraitement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-24T13:06:54.237924Z",
          "start_time": "2020-09-24T13:06:54.204609Z"
        },
        "hidden": true,
        "id": "aWYoZJPJxD6G"
      },
      "source": [
        "class Preprocess(object):\n",
        "    def __init__(self, lemmatize=True):\n",
        "        self.stopwords = set(nltk.corpus.stopwords.words(\"english\"))\n",
        "        self.lemmatize = lemmatize\n",
        "\n",
        "    def preprocess_pipeline(self, data):\n",
        "        clean_tokenized_data = self._clean_doc(data)\n",
        "        if self.lemmatize:\n",
        "            clean_tokenized_data = self._lemmatize(clean_tokenized_data)\n",
        "\n",
        "        return clean_tokenized_data\n",
        "\n",
        "    def _clean_doc(self, data):\n",
        "        tokenizer = nltk.tokenize.RegexpTokenizer(r\"\\w+\")\n",
        "        return [\n",
        "            [\n",
        "                token.lower()\n",
        "                for token in tokenizer.tokenize(review)\n",
        "                if token.lower() not in self.stopwords\n",
        "                and len(token) > 1\n",
        "                and token.isalpha()\n",
        "                and token != \"br]\"\n",
        "            ]\n",
        "            for review in data\n",
        "        ]\n",
        "\n",
        "    def _lemmatize(self, data):\n",
        "        lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "        return [[lemmatizer.lemmatize(word) for word in review] for review in data]\n",
        "\n",
        "    def convert_to_reviews(self, tokenized_reviews):\n",
        "        reviews = []\n",
        "        for tokens in tokenized_reviews:\n",
        "            reviews.append(\" \".join(tokens))\n",
        "\n",
        "        return reviews"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-24T13:07:29.745222Z",
          "start_time": "2020-09-24T13:06:55.097985Z"
        },
        "hidden": true,
        "id": "ygebO3D_xD6J",
        "outputId": "8f9d46ab-e7de-42b2-832f-4ce5ebd66d12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "pre = Preprocess()\n",
        "\n",
        "train_pos = pre.preprocess_pipeline(train_data[\"pos\"])\n",
        "train_neg = pre.preprocess_pipeline(train_data[\"neg\"])\n",
        "test_pos = pre.preprocess_pipeline(test_data[\"pos\"])\n",
        "test_neg = pre.preprocess_pipeline(test_data[\"neg\"])\n",
        "\n",
        "y_train = [1] * len(train_pos) + [0] * len(train_neg)\n",
        "y_test = [1] * len(test_pos) + [0] * len(test_neg)\n",
        "X_train = [\" \".join(sentence) for sentence in train_pos + train_neg]\n",
        "X_test = [\" \".join(sentence) for sentence in test_pos + test_neg]\n",
        "\n",
        "print(\"{} training sentences: {} pos and {} neg\".format(len(X_train), len(train_pos), len(train_neg)))\n",
        "print(\"{} test sentences: {} pos and {} neg\".format(len(X_test), len(test_pos), len(test_neg)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25000 training sentences: 12500 pos and 12500 neg\n",
            "25000 test sentences: 12500 pos and 12500 neg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIzXS_LyxD6L"
      },
      "source": [
        "# 1. Entrainement de plongements lexicaux"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hc6b5mstxD6M"
      },
      "source": [
        "Vous devez réaliser les étapes suivantes:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyxls-lYxD6M"
      },
      "source": [
        "## a) Utiliser Gensim pour entrainer un modèle word2vec sur le corpus. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Xy8ZXzAxD6M",
        "outputId": "44a379be-b0ee-433e-faa2-9c0127252d3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "model = Word2Vec(min_count=5, window=5, size=256, sample=1e-2,\n",
        "alpha=1e-2, min_alpha=1e-4, negative=5, workers=4)\n",
        "\n",
        "model.build_vocab(X_train)\n",
        "\n",
        "start = time.time()\n",
        "model.train(X_train, total_examples=model.corpus_count, epochs=10)\n",
        "print(time.time() - start)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "179.6863615512848\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwIr4uNUxD6O"
      },
      "source": [
        "## b) Décrire les paramètres du ou des modèles entraînés, leur taille sur disque, le nombre de mots encodés, le temps d'entraînement, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OPIbkr8xD6O"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suvwhRwfxD6P"
      },
      "source": [
        "## c) Décrire le cas échéant et de manière précise tout problème que vous avez eu à obtenir votre modèle et les façons de résoudre ces problèmes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfSNxCCyxD6P"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_t-VKtwVxD6Q"
      },
      "source": [
        "## d) Retrouvez les 5 mots voisins des mots suivants : excellent, terrible"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpLaT9EoxD6Q"
      },
      "source": [
        "print(model.most_similar('excellent')[:5])\n",
        "print(model.most_similar('terrible')[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66Mys_mcxD6S"
      },
      "source": [
        "# 2. Classification avec des plongements lexicaux"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XNuMEjTxD6T"
      },
      "source": [
        "On vous demande d’effectuer de la classification avec les plongements lexicaux obtenus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E42QN7uExD6T"
      },
      "source": [
        "## a) En reprenant le code développé dans le TP1 avec Scikitlearn, on vous demande cette fois de tester un modèle Naïve Bayes et de régression logistique avec des n-grammes (n=1,2,3 ensemble). Essayez de voir si une réduction de dimension améliore la classification. Ne fournissez que votre meilleur modèle. Evaluez vos algorithmes selon les métriques d’accuracy générale et de F1 par classe sur l’ensemble de test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCykb7K-xD6T"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMXMEiaZxD6Y"
      },
      "source": [
        "## b) En utilisant Tensorflow (ou Pytorch), on vous demande de développer un classificateur perceptron multicouches et un bi-LSTM avec les vecteurs d’un modèle word2vec pré-entrainé sur Wikipédia en Anglais (enwiki_upos_skipgram_300_3_2019) disponible à http://vectors.nlpl.eu/repository/11/3.zip. \n",
        "\n",
        "On s’attend à ce que vous effectuiez une moyenne des vecteurs de mots de chaque document pour obtenir un plongement du document.  \n",
        "\n",
        "Evaluez vos algorithmes selon les métriques d’accuracy générale et de F1 par classe sur l’ensemble de test. Pour chacun des modèles, indiquez ses performances et ses spécifications (nombre d’époques, régularisation, optimiseur, nombre de couches, etc.). N’hésitez pas à expérimenter avec différents paramètres. Vous ne devez reporter que votre meilleure expérimentation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEEbFV0NxD6Z"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-01zN44jxD6b"
      },
      "source": [
        "## c) Ré-entrainez les modèles en b) avec vos propres vecteurs. Comparez maintenant la performance obtenue en en b) avec celles que vous obtenez en utilisant vos propres vecteurs de mots entrainés sur le corpus. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AI1sNwfPxD6b"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZ9x_RbYxD6d"
      },
      "source": [
        "## d) Générez une table ou un graphique qui regroupe les performances des modèles, leurs spécifications, la durée d’entraînement et commentez ces résultats. Quelle est l’influence des word embeddings sur les performances?  Quel est votre meilleur modèle ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6Mog5T1xD6d"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}